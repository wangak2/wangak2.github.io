<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>丹青两幻</title>
  
  <subtitle>欢迎~</subtitle>
  <link href="https://wangak.cc/atom.xml" rel="self"/>
  
  <link href="https://wangak.cc/"/>
  <updated>2024-05-28T02:18:03.822Z</updated>
  <id>https://wangak.cc/</id>
  
  <author>
    <name>丹青两幻</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>AGILEFORMER:SPATIALLY AGILE TRANSFORMER UNET FOR MEDICAL IMAGE SEGMENTATION</title>
    <link href="https://wangak.cc/posts/6b854769.html"/>
    <id>https://wangak.cc/posts/6b854769.html</id>
    <published>2024-05-13T16:00:00.000Z</published>
    <updated>2024-05-28T02:18:03.822Z</updated>
    
    <content type="html"><![CDATA[<h2 id="AGILEFORMER-SPATIALLY-AGILE-TRANSFORMER-UNET-FOR-MEDICAL-IMAGE-SEGMENTATION"><a href="#AGILEFORMER-SPATIALLY-AGILE-TRANSFORMER-UNET-FOR-MEDICAL-IMAGE-SEGMENTATION" class="headerlink" title="AGILEFORMER: SPATIALLY AGILE TRANSFORMER UNET FOR MEDICAL IMAGE SEGMENTATION"></a>AGILEFORMER: SPATIALLY AGILE TRANSFORMER UNET FOR MEDICAL IMAGE SEGMENTATION</h2><p>论文：《AGILEFORMER: SPATIALLY AGILE TRANSFORMER UNET FOR MEDICAL IMAGE SEGMENTATION》（arXiv 2024）</p><ul><li><p>作者使用了一种新的patch embedding取代了vit-unet中标准的patch embedding</p></li><li><p>采用空间动态自注意力来捕获空间变化特征</p></li><li><p>提出了一种新的多尺度可变形位置编码</p></li></ul><p><img src="https://typoraimg.wangak.cc/2023/img/202405141004754.png" alt="image-20240511095459528"></p><h4 id="可变形的patch-embedding"><a href="#可变形的patch-embedding" class="headerlink" title="可变形的patch embedding"></a><strong>可变形的patch embedding</strong></h4><p>通过可变形卷积卷积核的采样位置可以根据输入的特征图进行微小的偏移，从而可以实现更灵活的特征提取。通过引入可变形的采样位置，使得补丁嵌入可以更好地适应不规则的结构，从而提高了特征提取的灵活性和准确性。</p><ul><li><p>第一个patch embedding：使用两个连续的可变形卷积层，这两个连续重叠的可变形patch embedding可以更好地提取局部特征，弥补了自注意力中局部性的不足</p></li><li><p>下采样层：通过3×3卷积完成下采样</p></li></ul><h4 id="空间动态自注意力"><a href="#空间动态自注意力" class="headerlink" title="空间动态自注意力"></a><strong>空间动态自注意力</strong></h4><p><strong>可变形多头自注意力：</strong></p><p>第h个头的计算过程如下：</p><p><img src="https://typoraimg.wangak.cc/2023/img/202405141004778.png" alt="image-20240511100930114" style="zoom:67%;"></p><p>注：f是输入的特征图，ϕ是插值函数，用于生成偏移后的特征图<script type="math/tex">\hat{f}</script>,<script type="math/tex">∆p_h</script>是第h个头部生成的偏移量,其通过一个卷积层生成</p><p><strong>邻域多头自注意力：</strong></p><p>与标准自注意力不同，标准自注意力计算特征图f中每个位置p的元素与其他位置元素的相似度，而邻域注意力只利用位置p周围k个最近邻的信息来计算注意力权重，而不是与所有位置的元素计算相似度。减少了标准自注意力的计算复杂度，从二次降至近似于空间维度线性的复杂度。重新引入了局部操作到自注意力中，使得模型具有平移等变性，从而提高了保留局部信息的能力。</p><h4 id="多尺度可变形位置编码"><a href="#多尺度可变形位置编码" class="headerlink" title="多尺度可变形位置编码"></a>多尺度可变形位置编码</h4><p><img src="https://typoraimg.wangak.cc/2023/img/202405141004551.png" alt="image-20240511102232095" style="zoom:67%;"></p><p>通过在跨多个尺度对不规则采样的位置信息进行编码。</p><p>公式如下：</p><p><img src="https://typoraimg.wangak.cc/2023/img/202405141004922.png" alt="image-20240511102056849"></p><p><em>注：f是输入特征图，<script type="math/tex">P_θ</script>实现为多尺度可变形深度卷积层，具有不同的核大小（3×3和5×5）</em></p>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;AGILEFORMER-SPATIALLY-AGILE-TRANSFORMER-UNET-FOR-MEDICAL-IMAGE-SEGMENTATION&quot;&gt;&lt;a href=&quot;#AGILEFORMER-SPATIALLY-AGILE-TRANSFORMER-UNET-</summary>
      
    
    
    
    <category term="计算机视觉" scheme="https://wangak.cc/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/"/>
    
    
    <category term="计算机视觉" scheme="https://wangak.cc/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/"/>
    
    <category term="图像分割" scheme="https://wangak.cc/tags/%E5%9B%BE%E5%83%8F%E5%88%86%E5%89%B2/"/>
    
    <category term="特征提取" scheme="https://wangak.cc/tags/%E7%89%B9%E5%BE%81%E6%8F%90%E5%8F%96/"/>
    
  </entry>
  
  <entry>
    <title>BEFUnet:A Hybrid CNN-Transformer Architecture for Precise Medical Image Segmentation</title>
    <link href="https://wangak.cc/posts/7a30900a.html"/>
    <id>https://wangak.cc/posts/7a30900a.html</id>
    <published>2024-05-13T16:00:00.000Z</published>
    <updated>2024-05-28T02:17:42.252Z</updated>
    
    <content type="html"><![CDATA[<h2 id="BEFUnet-A-Hybrid-CNN-Transformer-Architecture-for-Precise-Medical-Image-Segmentation"><a href="#BEFUnet-A-Hybrid-CNN-Transformer-Architecture-for-Precise-Medical-Image-Segmentation" class="headerlink" title="BEFUnet: A Hybrid CNN-Transformer Architecture for Precise Medical Image Segmentation"></a>BEFUnet: A Hybrid CNN-Transformer Architecture for Precise Medical Image Segmentation</h2><p>论文：《BEFUnet: A Hybrid CNN-Transformer Architecture for Precise Medical Image Segmentation》（arXiv 2024）</p><p>本文提出了一种创新的u型网络BEFUnet，该网络增强了体特征和边缘特征的融合，以实现精确的医学图像分割</p><p><strong>双分支编码器：</strong></p><p><img src="https://typoraimg.wangak.cc/2023/img/202405141006324.png" alt="image-20240504130415926" style="zoom:67%;"></p><ul><li><p><strong>边缘编码器：</strong>由四个阶段组成，每个阶段包含4个PDC块用于特征检测，并利用最大池化对各阶段之间的特征进行降采样来得到分层特征</p><p>注：PDC块包括一个深度卷积层、一个ReLU层和一个1×1的卷积层</p></li><li><p><strong>主体编码器：</strong>使用Swin-Transfomer对具有全局信息的高级特征进行编码</p></li></ul><p>将提取的边缘和体特征输入到LCAF模块进行融合</p><p><strong>LCAF：</strong>选择性地将边缘图和主体图进行交叉注意力，来融合边缘和主体的特征</p><p><img src="https://typoraimg.wangak.cc/2023/img/202405141006843.png" alt="image-20240504131627922" style="zoom:67%;"></p><p><strong>DLF模块：</strong>为确保层级之间的特征一致性，使用交叉注意力机制来跨尺度融合信息</p><p>较浅的层级包含更精确的定位信息，而较深的层级携带更适合解码器的更多语义信息，考虑到节省计算资源，只将最浅层（<script type="math/tex">P^l</script>)和最后一层（<script type="math/tex">P^s</script>)进行融合</p><p><img src="https://typoraimg.wangak.cc/2023/img/202405141006502.png" alt="image-20240504133235606" style="zoom:67%;"></p>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;BEFUnet-A-Hybrid-CNN-Transformer-Architecture-for-Precise-Medical-Image-Segmentation&quot;&gt;&lt;a href=&quot;#BEFUnet-A-Hybrid-CNN-Transformer-Arc</summary>
      
    
    
    
    <category term="计算机视觉" scheme="https://wangak.cc/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/"/>
    
    
    <category term="计算机视觉" scheme="https://wangak.cc/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/"/>
    
    <category term="图像分割" scheme="https://wangak.cc/tags/%E5%9B%BE%E5%83%8F%E5%88%86%E5%89%B2/"/>
    
    <category term="特征融合" scheme="https://wangak.cc/tags/%E7%89%B9%E5%BE%81%E8%9E%8D%E5%90%88/"/>
    
    <category term="双编码器" scheme="https://wangak.cc/tags/%E5%8F%8C%E7%BC%96%E7%A0%81%E5%99%A8/"/>
    
  </entry>
  
  <entry>
    <title>SegMamba:Long-range Sequential Modeling Mamba For 3D Medical Image Segmentation</title>
    <link href="https://wangak.cc/posts/f4ee98c4.html"/>
    <id>https://wangak.cc/posts/f4ee98c4.html</id>
    <published>2024-05-13T16:00:00.000Z</published>
    <updated>2024-05-28T01:55:18.223Z</updated>
    
    <content type="html"><![CDATA[<h2 id="SegMamba-Long-range-Sequential-Modeling-Mamba-For-3D-Medical-Image-Segmentation"><a href="#SegMamba-Long-range-Sequential-Modeling-Mamba-For-3D-Medical-Image-Segmentation" class="headerlink" title="SegMamba: Long-range Sequential Modeling Mamba For 3D Medical Image Segmentation"></a>SegMamba: Long-range Sequential Modeling Mamba For 3D Medical Image Segmentation</h2><p>论文：《SegMamba: Long-range Sequential Modeling Mamba For 3D Medical Image Segmentation》（arXiv 2024）</p><p>论文贡献：</p><ul><li>设计了ToM模块，用以增强三维特征的顺序建模</li><li>设计了门控空间卷积模块（GSC)，用以增强每个ToM之前空间维度上的特征表示</li></ul><h4 id="网络结构："><a href="#网络结构：" class="headerlink" title="网络结构："></a><strong>网络结构：</strong></h4><p><img src="https://typoraimg.wangak.cc/2023/img/202405141007030.png" alt="image-20240507091527951"></p><p><strong>1.Stem</strong></p><p>采用深度卷积，内核大小为7×7×7，填充为3×3×3，步幅为2×2×2。</p><p><strong>2.TSMamba块</strong></p><p>计算过程如下：</p><p><img src="https://typoraimg.wangak.cc/2023/img/202405141007633.png" alt="image-20240507091715749" style="zoom:67%;"></p><p><em>注：其中GSC和ToM分别表示所提出的门控空间卷积模块和三向Mamba模块</em></p><p><strong>3.门控空间卷积(GSC)</strong></p><p>门控空间卷积(GSC)用于提取mamba层之前的空间关系</p><p><img src="https://typoraimg.wangak.cc/2023/img/202405141007183.png" alt="image-20240507092130322" style="zoom:67%;"></p><p>计算过程如下：</p><p><img src="https://typoraimg.wangak.cc/2023/img/202405141007271.png" alt="image-20240507092149524" style="zoom:67%;"></p><p><strong>4.三向mamba（ToM)</strong></p><p>从三个方向计算特征依赖关系，将三维输入特征平铺成三个序列，进行相应的特征交互，得到融合后的三维特征</p><p>计算过程如下：</p><p><img src="https://typoraimg.wangak.cc/2023/img/202405141007074.png" alt="image-20240507092415793" style="zoom:67%;"></p>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;SegMamba-Long-range-Sequential-Modeling-Mamba-For-3D-Medical-Image-Segmentation&quot;&gt;&lt;a href=&quot;#SegMamba-Long-range-Sequential-Modeling-M</summary>
      
    
    
    
    <category term="计算机视觉" scheme="https://wangak.cc/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/"/>
    
    
    <category term="计算机视觉" scheme="https://wangak.cc/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/"/>
    
    <category term="图像分割" scheme="https://wangak.cc/tags/%E5%9B%BE%E5%83%8F%E5%88%86%E5%89%B2/"/>
    
    <category term="特征增强" scheme="https://wangak.cc/tags/%E7%89%B9%E5%BE%81%E5%A2%9E%E5%BC%BA/"/>
    
    <category term="mamba类" scheme="https://wangak.cc/tags/mamba%E7%B1%BB/"/>
    
  </entry>
  
  <entry>
    <title>Medical Image Segmentation via Cascaded Attention Decoding</title>
    <link href="https://wangak.cc/posts/3790531e.html"/>
    <id>https://wangak.cc/posts/3790531e.html</id>
    <published>2024-05-06T16:00:00.000Z</published>
    <updated>2024-05-07T11:47:50.554Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Medical-Image-Segmentation-via-Cascaded-Attention-Decoding"><a href="#Medical-Image-Segmentation-via-Cascaded-Attention-Decoding" class="headerlink" title="Medical Image Segmentation via Cascaded Attention Decoding"></a>Medical Image Segmentation via Cascaded Attention Decoding</h2><p><strong>论文：《Medical Image Segmentation via Cascaded Attention Decoding》（WACV 2023)</strong></p><p>这篇论文和之前看过的一些文章设计思路也差不多，通过对编码器四个阶段输出的特征进行了融合，不同之处在于这篇论文提出了AG和CAM两个模块进行特征的融合，先使用AG进行特征融合，再使用CAM进行增强</p><p><strong>级联注意解码器:</strong></p><p><img src="https://typoraimg.wangak.cc/2023/img/202405071929069.png" alt="image-20240426162057053" style="zoom:67%;"></p><p>UpConv:对特征进行上采样</p><p>CAM:用于增强特征映射</p><p>作者使用了四个CAM块聚合编码器四个阶段输出的特征，三个AG模块将先前解码器上采样的特征与跳跃连接的特征结合起来</p><p><strong>AG：用于级联的特征融合</strong></p><p><img src="https://typoraimg.wangak.cc/2023/img/202405071929850.png" alt="image-20240427213751108" style="zoom:67%;"></p><p><img src="https://typoraimg.wangak.cc/2023/img/202405071930154.png" alt="image-20240427213938485" style="zoom: 67%;"></p><p><strong>CAM:用于增强特征映射</strong></p><p><img src="https://typoraimg.wangak.cc/2023/img/202405071930371.png" alt="image-20240427214031982" style="zoom: 80%;"></p><p>CAM由通道注意力、空间注意力和一个卷积块组成，使用通道注意力、空间注意力可以来抑制背景信息</p><p><img src="https://typoraimg.wangak.cc/2023/img/202405071930603.png" alt="image-20240427214223908" style="zoom:67%;"></p><p>注：这里的卷积块是由两个3×3的卷积层组成，每个卷积层后是一个BN和一个ReLU</p><p><img src="https://typoraimg.wangak.cc/2023/img/202405071930828.png" alt="image-20240427214438165" style="zoom:67%;"></p><p><strong>多阶段损失和特征聚合：</strong></p><p>作者是通过对分层编码器的四个阶段输出的结果使用加性聚合的方式生成最后的预测图：</p><p><img src="https://typoraimg.wangak.cc/2023/img/202405071930176.png" alt="image-20240427214745978" style="zoom:67%;"></p><p><em>注1：p1、p2、p3、p4为四个预测头的特征映射，w、x、y、z为各个预测头像的权重。</em></p><p><em>注2：本文中作者将w、x、y、z均设为1</em></p><p>作者通过分别计算每个预测头的损失，来得到最终的损失：</p><p><img src="https://typoraimg.wangak.cc/2023/img/202405071930911.png" alt="image-20240427215042984" style="zoom:67%;"></p><p><em>注：实验中作者将α, β, γ,  ζ均设为了1</em></p><p><strong>整体的网络结构如下：</strong></p><p><img src="https://typoraimg.wangak.cc/2023/img/202405071930348.png" alt="image-20240427215222965" style="zoom: 50%;"></p>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;Medical-Image-Segmentation-via-Cascaded-Attention-Decoding&quot;&gt;&lt;a href=&quot;#Medical-Image-Segmentation-via-Cascaded-Attention-Decoding&quot; cl</summary>
      
    
    
    
    <category term="计算机视觉" scheme="https://wangak.cc/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/"/>
    
    
    <category term="计算机视觉" scheme="https://wangak.cc/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/"/>
    
    <category term="图像分割" scheme="https://wangak.cc/tags/%E5%9B%BE%E5%83%8F%E5%88%86%E5%89%B2/"/>
    
    <category term="特征融合" scheme="https://wangak.cc/tags/%E7%89%B9%E5%BE%81%E8%9E%8D%E5%90%88/"/>
    
    <category term="特征增强" scheme="https://wangak.cc/tags/%E7%89%B9%E5%BE%81%E5%A2%9E%E5%BC%BA/"/>
    
  </entry>
  
  <entry>
    <title>SegFormer3D</title>
    <link href="https://wangak.cc/posts/ee049a76.html"/>
    <id>https://wangak.cc/posts/ee049a76.html</id>
    <published>2024-05-06T16:00:00.000Z</published>
    <updated>2024-05-28T02:17:15.463Z</updated>
    
    <content type="html"><![CDATA[<h2 id="SegFormer3D"><a href="#SegFormer3D" class="headerlink" title="SegFormer3D"></a>SegFormer3D</h2><p><strong>论文：《SegFormer3D: an Efficient Transformer for 3D Medical Image Segmentation》（arXiv 2024）</strong></p><p>作者提出了SegFormer3D使用全mlp解码器来聚合局部和全局注意力特征，来产生高度准确的分割掩码</p><p><img src="https://typoraimg.wangak.cc/2023/img/202405071927153.png" alt="image-20240427155943100" style="zoom:67%;"></p><p><strong>对于编码器部分：</strong></p><ul><li><p>使用patch merging进行下采样：与池化操作相比克服了像素生成过程中的邻域信息丢失的问题，同时也能节省一定的运算量</p></li><li><p>使用efficient self-attention：捕获全局信息的同时，用缩放系数R减少了self-attention计算的时间复杂度</p><p><img src="https://typoraimg.wangak.cc/2023/img/202405071927313.png" alt="image-20240427161059247" style="zoom:67%;"></p></li><li><p>舍弃了固定的位置编码，使用mix ffn模块来提取位置信息：</p><p><img src="https://typoraimg.wangak.cc/2023/img/202405071927148.png" alt="image-20240427161359128" style="zoom: 50%;"></p></li></ul><p><strong>对于解码器部分：</strong>使用了全MLP的结构，使用了一个统一的模块完成了对不同尺度的特征的解码过程，简化了解码过程，确保在各种数据集中对体积特征进行高效且一致的解码，避免了过度参数化。</p><p><img src="https://typoraimg.wangak.cc/2023/img/202405071927399.png" alt="image-20240427161629423" style="zoom:67%;"></p><p><strong>参数量和性能的比较：</strong></p><p><img src="https://typoraimg.wangak.cc/2023/img/202405071927981.png" alt="image-20240427162144102" style="zoom: 50%;"></p><p><img src="https://typoraimg.wangak.cc/2023/img/202405071927799.png" alt="image-20240427162104575" style="zoom:67%;"></p><p>作者在编码器和解码器的设计中，都使用轻量化的操作（编码器部分的patch merging、efficient self-attention和解码器使用全mlp解码器来聚合局部和全局注意力特征），模型参数量和计算量都有明显的下降，但性能表现不如nnformer</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;SegFormer3D&quot;&gt;&lt;a href=&quot;#SegFormer3D&quot; class=&quot;headerlink&quot; title=&quot;SegFormer3D&quot;&gt;&lt;/a&gt;SegFormer3D&lt;/h2&gt;&lt;p&gt;&lt;strong&gt;论文：《SegFormer3D: an Effici</summary>
      
    
    
    
    <category term="计算机视觉" scheme="https://wangak.cc/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/"/>
    
    
    <category term="计算机视觉" scheme="https://wangak.cc/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/"/>
    
    <category term="图像分割" scheme="https://wangak.cc/tags/%E5%9B%BE%E5%83%8F%E5%88%86%E5%89%B2/"/>
    
    <category term="特征融合" scheme="https://wangak.cc/tags/%E7%89%B9%E5%BE%81%E8%9E%8D%E5%90%88/"/>
    
  </entry>
  
  <entry>
    <title>LHU-NET</title>
    <link href="https://wangak.cc/posts/c46bf14d.html"/>
    <id>https://wangak.cc/posts/c46bf14d.html</id>
    <published>2024-04-24T16:00:00.000Z</published>
    <updated>2024-04-25T11:20:57.093Z</updated>
    
    <content type="html"><![CDATA[<h2 id="LHU-NET-A-LIGHT-HYBRID-U-NET-FOR-COST-EFFICIENT-HIGH-PERFORMANCE-VOLUMETRIC-MEDICAL-IMAGE-SEGMENTATION"><a href="#LHU-NET-A-LIGHT-HYBRID-U-NET-FOR-COST-EFFICIENT-HIGH-PERFORMANCE-VOLUMETRIC-MEDICAL-IMAGE-SEGMENTATION" class="headerlink" title="LHU-NET: A LIGHT HYBRID U-NET FOR COST-EFFICIENT, HIGH-PERFORMANCE VOLUMETRIC MEDICAL IMAGE SEGMENTATION"></a>LHU-NET: A LIGHT HYBRID U-NET FOR COST-EFFICIENT, HIGH-PERFORMANCE VOLUMETRIC MEDICAL IMAGE SEGMENTATION</h2><p><strong>论文：《LHU-Net: A Light Hybrid U-Net for Cost-Efficient, High-Performance Volumetric Medical Image Segmentation》（arXiv 2024)</strong></p><p><img src="https://typoraimg.wangak.cc/2023/img/202404251917245.png" alt="image-20240414134752694" style="zoom:67%;"></p><p>LHU-Net：将基于卷积的块与混合注意力机制集成</p><p><strong>Init 阶段与Out阶段</strong>：</p><ul><li>该阶段从一个点卷积操作（PW-Conv）开始，应用于输入数据，调整通道维度以匹配后续级别的通道数。</li></ul><p><img src="/posts/c46bf14d.htm/Users\wangak\AppData\Roaming\Typora\typora-user-images\image-20240414134832658.png" alt="image-20240414134832658"></p><ul><li><p>同时对于输入直接应用ResBlock，将其输出输入到Out阶段，与解码器的输出进行连接，产生最后的结果</p><p><img src="/posts/c46bf14d.htm/Users\wangak\AppData\Roaming\Typora\typora-user-images\image-20240414135339973.png" alt="image-20240414135339973"></p></li></ul><p><em>注：<script type="math/tex">X_{CD}</script>​表示CNN解码器块输出</em></p><p><strong>CNN Blocks:</strong></p><p>初始空间维度需要大量的计算成本,Vit难以应用，故在此阶段的设计是为了优化参数效率和保留局部特征，在之后的阶段再提取全局特征</p><ul><li>Down Conv块的设计如下：</li></ul><p><img src="https://typoraimg.wangak.cc/2023/img/202404251917748.png" alt="image-20240414140026771"></p><p><strong>Hybrid Blocks（混合注意力）：</strong></p><p>在此阶段将局部细节和全局信息进行融合</p><ul><li><p><strong>Self-Adaptive Contextual Fusion Module：</strong>将空间注意力模块与卷积模块相结合，这种空间注意力模块将LKAd模块与自注意力机制的输出进行并行计算</p><p><img src="https://typoraimg.wangak.cc/2023/img/202404251917014.png" alt="image-20240414141611267"></p></li></ul><p>​     该模块最终的输出为：</p><p>​                                                                  <img src="https://typoraimg.wangak.cc/2023/img/202404251918107.png" alt="image-20240414141723308" style="zoom:67%;"></p><p>​     <em>注1：<script type="math/tex">δ_s</script>和<script type="math/tex">γ_s</script>​表示每个通道的可学习参数，控制两种不同注意机制的组合权值</em></p><p>​     <em>注2：Comb函数的定义如下：DW-Conv3 是一个 3×3×3 的卷积块</em></p><p><img src="https://typoraimg.wangak.cc/2023/img/202404251918397.png" alt="image-20240414142913765" style="zoom:67%;"></p><ul><li><p><strong>LKAd:</strong></p><p>其步骤如下：</p><ul><li>输入经过一个点卷积操作（Conv1），然后应用激活函数（GELU），以引入非线性和降低维度。</li><li>变换后的张量经过一系列深度卷积（DW-Conv）和深度膨胀卷积（DWD-Conv）操作，以提取多尺度特征并保留空间信息。</li><li>DDW-Conv3集成了可变形深度卷积可以自适应地对特征图进行采样，从而增强了模型捕获细粒度细节和长距离依赖的能力。</li></ul></li></ul><p><img src="https://typoraimg.wangak.cc/2023/img/202404251918886.png" alt="image-20240414142033147" style="zoom:67%;"></p>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;LHU-NET-A-LIGHT-HYBRID-U-NET-FOR-COST-EFFICIENT-HIGH-PERFORMANCE-VOLUMETRIC-MEDICAL-IMAGE-SEGMENTATION&quot;&gt;&lt;a href=&quot;#LHU-NET-A-LIGHT-HY</summary>
      
    
    
    
    <category term="计算机视觉" scheme="https://wangak.cc/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/"/>
    
    
    <category term="计算机视觉" scheme="https://wangak.cc/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/"/>
    
    <category term="图像分割" scheme="https://wangak.cc/tags/%E5%9B%BE%E5%83%8F%E5%88%86%E5%89%B2/"/>
    
    <category term="卷积+注意力" scheme="https://wangak.cc/tags/%E5%8D%B7%E7%A7%AF-%E6%B3%A8%E6%84%8F%E5%8A%9B/"/>
    
  </entry>
  
  <entry>
    <title>HCF-Net</title>
    <link href="https://wangak.cc/posts/d349cca5.html"/>
    <id>https://wangak.cc/posts/d349cca5.html</id>
    <published>2024-04-24T16:00:00.000Z</published>
    <updated>2024-05-07T06:34:28.076Z</updated>
    
    <content type="html"><![CDATA[<h2 id="HCF-Net-Hierarchical-Context-Fusion-Network-for-Infrared-Small-Object-Detection"><a href="#HCF-Net-Hierarchical-Context-Fusion-Network-for-Infrared-Small-Object-Detection" class="headerlink" title="HCF-Net: Hierarchical Context Fusion Network for Infrared Small Object Detection"></a>HCF-Net: Hierarchical Context Fusion Network for Infrared Small Object Detection</h2><p><strong>论文：《HCF-Net: Hierarchical Context Fusion Network for Infrared Small Object Detection》（arXiv 2024)</strong></p><p>PPA采用分层特征融合和注意机制来维护和增强对小物体的表示，确保关键信息通过多次下采样步骤得以保留。</p><p>DASI增强了U-Net中的跳跃连接，侧重于高维和低维特征的自适应选择和精细融合，以增强小物体的显著性。</p><p>位于网络深处的MDCR加强了多尺度特征提取和通道信息表示，捕捉各种感受野范围内的特征。</p><p><img src="https://typoraimg.wangak.cc/2023/img/202404251920292.png" alt="image-20240416133737554" style="zoom:67%;"></p><h4 id="1-PPA"><a href="#1-PPA" class="headerlink" title="1.PPA"></a>1.PPA</h4><p><img src="https://typoraimg.wangak.cc/2023/img/202404251920768.png" alt="image-20240416134505795" style="zoom:67%;"></p><p>对于小目标而言，经过多次下采样容易丢失关键信息，作者使用PPA模块取代了编码器和解码器中的传统卷积的操作，来解决此问题。</p><p>PPA使用了多分支提取策略，多分支策略有助于捕获物体的多尺度特征，从而提高对小目标的特征提取能力。其由三个平行分支组成：局部、全局、串行卷积。</p><p>PPA步骤如下：</p><ul><li>对于输入特征F,先使用点卷积调整通道数，然后输入到三个分支</li><li><p>通过三个分支分别计算得到<script type="math/tex">F_{local}、F_{global}、F_{conv}</script></p></li><li><p>将三个分支的输出结果相加得到最后的输出</p></li></ul><p><strong>Patch-Aware:</strong></p><p>使用 Unfold 和 reshape 操作将特征张量 F’ 划分为一组空间上连续的patch,对这些 patch 进行通道方向上的平均，得到大小为 (p × p, H’/p, W’/p) 的结果，接着使用 FFN 进行线性计算。随后，应用激活函数来获得线性计算特征在空间维度上的概率分布，并相应调整它们的权重,对加权结果进行特征选择，从 tokens 和通道中选择与任务相关的特征。</p><p><em>注：局部和全局分支的区分是通过patch的大小参数p来控制的</em></p><p><strong>特征融合和注意力：</strong></p><p>在进行了多分支的特征提取后，利用注意力机制进行自适应的特征增强，该注意力模块由通道注意力和空间注意力组成，其过程如下：</p><p><img src="https://typoraimg.wangak.cc/2023/img/202404251920747.png" alt="image-20240416143044284" style="zoom:67%;"></p><p><em>注：其中，⊗ 表示元素相乘，<script type="math/tex">F_c∈ R^{H'×W'×C'}</script>、<script type="math/tex">F_s∈ R^{H'×W'×C'}</script>分别表示通道注意力和空间注意力处理后的特征，<script type="math/tex">M_c∈ R^{1×1×C'}</script>是通道注意力图,<script type="math/tex">M_s∈ R^{H'×W'×1}</script>是空间注意力图。δ和 B 分别表示ReLU和BN，F’’是PPA的最终的输出</em></p><h4 id="2-维度感知选择性整合模块"><a href="#2-维度感知选择性整合模块" class="headerlink" title="2.维度感知选择性整合模块"></a><strong>2.维度感知选择性整合模块</strong></h4><p><img src="https://typoraimg.wangak.cc/2023/img/202404251920269.png" alt="image-20240416144400263" style="zoom: 80%;"></p><p>DASI能够根据物体的大小自适应地选择合适的特征进行融合，DASI通过卷积、插值等操作，将高维特征<script type="math/tex">F_h∈R^{H_h×W_h×C_h}</script>和低维特征<script type="math/tex">F_l∈R^{H_l×W_l×C_l}</script>与当前层的特征<script type="math/tex">F_u∈R^{H×W×C}</script>进行初步对齐。随后，它将这些特征在通道维度上分成四个相等的部分，从而得到 <script type="math/tex">{(h_i)}_{i=1}^4 ∈R^{H × W × C/4}</script>, <script type="math/tex">{(I_i)}_{i=1}^4 ∈R^{H × W × C/4}</script>, <script type="math/tex">{(u_i)}_{i=1}^4 ∈R^{H × W × C/4}</script>,其中 <script type="math/tex">h_i、 I_i和 u_i</script> 分别表示高维、低维和当前层特征的第 i 个分区特征。<br>该模块最终的输出结构为：</p><p><img src="https://typoraimg.wangak.cc/2023/img/202404251920852.png" alt="image-20240416145650808" style="zoom:67%;"></p><p><em>注： α是通过应用于<script type="math/tex">u_i</script>的激活函数所得到的值，当α> 0.5，则模型优先考虑细粒度特征，当α &lt; 0.5，则强调上下文特征。</em></p><h4 id="3-MDCR"><a href="#3-MDCR" class="headerlink" title="3.MDCR"></a>3.MDCR</h4><p>在 MDCR 中，引入了多个深度可分离卷积层，以不同的扩张率捕捉各种感受野大小的空间特征，从而能够对物体和背景之间的差异进行更详细的建模，增强其分辨小物体的能力。</p><p><img src="https://typoraimg.wangak.cc/2023/img/202404251920901.png" alt="image-20240416150546927" style="zoom:67%;"></p><p>将输入特征沿通道维度分成四部分，每个部分以不同的扩张率进行深度可分离卷积，然后通过对每个部分的通道交错重排来增强多尺度特征的多样性，最后使用点卷积将这四部分的信息进行融合。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;HCF-Net-Hierarchical-Context-Fusion-Network-for-Infrared-Small-Object-Detection&quot;&gt;&lt;a href=&quot;#HCF-Net-Hierarchical-Context-Fusion-Netwo</summary>
      
    
    
    
    <category term="计算机视觉" scheme="https://wangak.cc/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/"/>
    
    
    <category term="计算机视觉" scheme="https://wangak.cc/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/"/>
    
    <category term="图像分割" scheme="https://wangak.cc/tags/%E5%9B%BE%E5%83%8F%E5%88%86%E5%89%B2/"/>
    
    <category term="特征融合" scheme="https://wangak.cc/tags/%E7%89%B9%E5%BE%81%E8%9E%8D%E5%90%88/"/>
    
    <category term="下采样模块" scheme="https://wangak.cc/tags/%E4%B8%8B%E9%87%87%E6%A0%B7%E6%A8%A1%E5%9D%97/"/>
    
  </entry>
  
  <entry>
    <title>D-Net</title>
    <link href="https://wangak.cc/posts/ceff47a.html"/>
    <id>https://wangak.cc/posts/ceff47a.html</id>
    <published>2024-04-13T16:00:00.000Z</published>
    <updated>2024-04-27T12:58:50.976Z</updated>
    
    <content type="html"><![CDATA[<h2 id="D-Net-Dynamic-Large-Kernel-with-Dynamic-Feature-Fusion-for-Volumetric-Medical-Image-Segmentation"><a href="#D-Net-Dynamic-Large-Kernel-with-Dynamic-Feature-Fusion-for-Volumetric-Medical-Image-Segmentation" class="headerlink" title="D-Net: Dynamic Large Kernel with Dynamic Feature Fusion for Volumetric Medical Image Segmentation"></a>D-Net: Dynamic Large Kernel with Dynamic Feature Fusion for Volumetric Medical Image Segmentation</h2><p><strong>论文：《D-Net: Dynamic Large Kernel with Dynamic Feature Fusion for Volumetric Medical Image Segmentation》（arXiv 2024）</strong></p><p>主要贡献：</p><ul><li>提出用于通用特征提取的大核模块（DLK)，其采用多个大卷积核来捕获多尺度特征，然后利用动态选择机制，根据全局上下文信息自适应地吐出最重要的空间特征</li><li>提出动态特征融合模块，实现自适应的特征融合（DFF)，其根据全局信息融合多尺度局部特征</li><li>提出D-Net用于3d医学图像分割，其将DLK和DFF模块结合到了分层Vit模块中，实现了更高的分割准确性</li></ul><p>使用固定大小的核的卷积在自适应地捕获多尺度特征方面存在不足，因而作者提出了动态大核（DLK)与动态特征融合模块（DFF)</p><p>(顺序地聚合大核卷积来扩大感受野)</p><p><strong>Dynamic Large Kernel (DLK)：</strong></p><p><img src="https://typoraimg.wangak.cc/2023/img/202404141315413.png" alt="image-20240413124105291" style="zoom:67%;"></p><p>作者采用了级联大核卷积的方法，其卷积核尺寸和膨胀率逐渐增大。这样设计使得有效感受野逐渐增大，从而有效地捕获更广泛的信息，在更深、更大的感受野内提取的特征对输出的贡献更显著，使得DLK能够捕获更细致和更具信息量的特征。</p><ul><li>作者使用了两个具有大核的深度卷积进行级联：</li></ul><p><img src="https://typoraimg.wangak.cc/2023/img/202404141315469.png" alt="image-20240413124653183" style="zoom: 67%;"></p><p><em>注1：5，7是核大小，1，3是膨胀率</em></p><p><em>注2：通过级联大核卷积，DLK具有与23 × 23 × 23核大小的卷积相同的有效感受野</em></p><ul><li>随后，对两个级联的大核卷积的输出<script type="math/tex">X_1^l、X_2^l</script>应用平均池化(AVP)和最大池化（MAP)来建模局部特征之间的全局空间关系。</li></ul><p><img src="/posts/ceff47a.htm/Users\wangak\AppData\Roaming\Typora\typora-user-images\image-20240413125400295.png" alt="image-20240413125400295" style="zoom:67%;"></p><ul><li>动态选择机制：通过一个7×7×7的卷积层来实现不同空间描述符<script type="math/tex">(w_{avg},w_{map})</script>之间的信息交互,并使用Sigmoid激活函数来获得动态选择值<script type="math/tex">w_1，w_2</script></li></ul><p><img src="https://typoraimg.wangak.cc/2023/img/202404141315234.png" alt="image-20240413125635669" style="zoom:67%;"></p><p>​    <em>注：不同大核卷积的特征通过利用这些选择值来自适应地进行选择</em></p><ul><li><p>DLK最后的输出为：</p><p><img src="https://typoraimg.wangak.cc/2023/img/202404141315364.png" alt="image-20240413125821821" style="zoom:67%;"></p></li><li><p>DLK block的设计如下：</p></li></ul><p><img src="https://typoraimg.wangak.cc/2023/img/202404141315908.png" alt="image-20240413130239187" style="zoom:67%;"></p><p><strong>Dynamic Feature Fusion (DFF)：</strong></p><p><img src="https://typoraimg.wangak.cc/2023/img/202404141315381.png" alt="image-20240413130621926" style="zoom:67%;"></p><ul><li><p>将特征映射<script type="math/tex">F_1^l、F_2^l</script>沿通道方向进行连接，然后通过级联平均池化、卷积和Sigmoid来得到全局通道特征的重要性描述<script type="math/tex">w_{ch}</script></p><p><img src="https://typoraimg.wangak.cc/2023/img/202404141315609.png" alt="image-20240413131334972" style="zoom:67%;"></p></li><li><p>根据<script type="math/tex">w_{ch}</script>进行特征映射，然后利用1×1×1卷积进行映射(保留重要特征)</p></li></ul><p><img src="https://typoraimg.wangak.cc/2023/img/202404141315858.png" alt="image-20240413132012723" style="zoom:67%;"></p><ul><li>通过1×1×1卷积层、Sigmoid激活对特征映射<script type="math/tex">F_1^l、F_2^l</script>进行处理来捕获全局空间信息<script type="math/tex">w_{sp}</script></li></ul><p><img src="https://typoraimg.wangak.cc/2023/img/202404141315379.png" alt="image-20240413132340534" style="zoom:67%;"></p><p><strong>D-Net:</strong></p><p><img src="https://typoraimg.wangak.cc/2023/img/202404141315703.png" alt="image-20240413132535496" style="zoom:67%;"></p><p><strong>实验部分：</strong></p><p><strong>多器官分割任务：</strong></p><p><img src="https://typoraimg.wangak.cc/2023/img/202404141315736.png" alt="image-20240413132922149" style="zoom:67%;"></p>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;D-Net-Dynamic-Large-Kernel-with-Dynamic-Feature-Fusion-for-Volumetric-Medical-Image-Segmentation&quot;&gt;&lt;a href=&quot;#D-Net-Dynamic-Large-Kern</summary>
      
    
    
    
    <category term="计算机视觉" scheme="https://wangak.cc/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/"/>
    
    
    <category term="计算机视觉" scheme="https://wangak.cc/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/"/>
    
    <category term="图像分割" scheme="https://wangak.cc/tags/%E5%9B%BE%E5%83%8F%E5%88%86%E5%89%B2/"/>
    
    <category term="特征融合" scheme="https://wangak.cc/tags/%E7%89%B9%E5%BE%81%E8%9E%8D%E5%90%88/"/>
    
    <category term="卷积block" scheme="https://wangak.cc/tags/%E5%8D%B7%E7%A7%AFblock/"/>
    
  </entry>
  
  <entry>
    <title>BiFormer</title>
    <link href="https://wangak.cc/posts/f56fa5b4.html"/>
    <id>https://wangak.cc/posts/f56fa5b4.html</id>
    <published>2024-04-12T16:00:00.000Z</published>
    <updated>2024-04-13T00:00:00.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="BiFormer-Vision-Transformer-with-Bi-Level-Routing-Attention"><a href="#BiFormer-Vision-Transformer-with-Bi-Level-Routing-Attention" class="headerlink" title="BiFormer: Vision Transformer with Bi-Level Routing Attention"></a>BiFormer: Vision Transformer with Bi-Level Routing Attention</h2><p><strong>论文：《BiFormer: Vision Transformer with Bi-Level Routing Attention》（CVPR 2023)</strong></p><p><strong>文章贡献：</strong></p><ul><li>作者提出了一种新颖的双层路由机制，将其应用于传统的注意力机制中。</li><li>基于双层路由注意力机制，作者提出了一种名为BiFormer的通用视觉Transformer模型。</li></ul><p>1.注意力：</p><p><img src="https://typoraimg.wangak.cc/2023/img/202404131212635.png" alt="image-20240406101554182" style="zoom:67%;"></p><p>注：<script type="math/tex">\sqrt{C}</script>​是缩放因子用以避免梯度消失</p><p>多头注意力：对输入沿着通道维度分成h个块（头部），每个块使用一组独立的权重</p><p><img src="https://typoraimg.wangak.cc/2023/img/202404131212205.png" alt="image-20240406101842223" style="zoom: 67%;"></p><p><em>注1：<script type="math/tex">W_0</script>​是一个额外的线性变换用来组合所有的头</em></p><p><em>注2：MHSA的复杂度是<script type="math/tex">O(N^2)</script>,因为有N个查询，每个查询涉及N个键值对</em></p><p>2.双级路由注意力（BRA）</p><p>作者探索了一种动态的、查询感知的稀疏注意机制，在粗粒度的区域级别上过滤掉大多数不相关的键-值对，使得只有少部分的路由区域保留下来，在这些路由区域的并集上应用细粒度的令牌-令牌注意力。</p><p>算法步骤：</p><ul><li>区域划分和输入投影：对于输入<script type="math/tex">X\in{H×W×C}</script>,首先将其划分为<script type="math/tex">S×S</script>非重叠区域，每个区域包含<script type="math/tex">\large H×W\over{S^2}</script>个特征向量（通过对X进行reshape操作实现），然后进行线性投影得到Q、K、V:</li></ul><p><img src="https://typoraimg.wangak.cc/2023/img/202404131212896.png" alt="image-20240406103338375" style="zoom:67%;"></p><ul><li>带有向图的区域到区域路由:得到区域级的<script type="math/tex">Q^r、K^r\in{R^{S×S×C}}</script>（通过对每个区域内的Q、K取平均值得到）,然后得到区域到区域亲和图<script type="math/tex">A^r</script></li></ul><p><img src="https://typoraimg.wangak.cc/2023/img/202404131212827.png" alt="image-20240406104429624" style="zoom: 67%;"></p><p>注：<script type="math/tex">A^r</script>反应了两个区域在语义上的关联程度</p><ul><li>对<script type="math/tex">A^r</script>逐行进行top-k操作得到每个区域最相关的 k 个区域的索引，以此来修剪关联图</li></ul><p><img src="https://typoraimg.wangak.cc/2023/img/202404131212647.png" alt="image-20240406104825390" style="zoom:67%;"></p><p><em>注：<script type="math/tex">I^r</script>的第i行包含了第i个区域最相关的k个区域的索引</em></p><p>3.Token-to-token注意</p><p>得到了区域到区域路由索引矩阵<script type="math/tex">I^r</script>后，即可进行Token-to-token关注，对于每个区域i中的Q,根据 k 个路由区域中的所有键值对，收集K、V:</p><p><img src="https://typoraimg.wangak.cc/2023/img/202404131212214.png" alt="image-20240406105921662" style="zoom: 67%;"></p><p>然后进行注意力计算：</p><p><img src="https://typoraimg.wangak.cc/2023/img/202404131212315.png" alt="image-20240406110015427" style="zoom:67%;"></p><p><em>注：LCE是为了增强局部信息，其通过深度卷积实现</em></p><p>4.BRA的计算复杂度：包括三部分（线性投影、区域到区域路由和Token-to-token注意）</p><p>总计算量为:</p><p><img src="https://typoraimg.wangak.cc/2023/img/202404131212610.png" alt="image-20240406110454570" style="zoom:67%;"></p><p>作者所设计的网络模型如下：</p><p><img src="https://typoraimg.wangak.cc/2023/img/202404131212482.png" alt="image-20240406110612740" style="zoom:67%;"></p>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;BiFormer-Vision-Transformer-with-Bi-Level-Routing-Attention&quot;&gt;&lt;a href=&quot;#BiFormer-Vision-Transformer-with-Bi-Level-Routing-Attention&quot; </summary>
      
    
    
    
    <category term="计算机视觉" scheme="https://wangak.cc/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/"/>
    
    
    <category term="计算机视觉" scheme="https://wangak.cc/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/"/>
    
    <category term="图像分割" scheme="https://wangak.cc/tags/%E5%9B%BE%E5%83%8F%E5%88%86%E5%89%B2/"/>
    
  </entry>
  
  <entry>
    <title>EPT-Net</title>
    <link href="https://wangak.cc/posts/2dfc19b2.html"/>
    <id>https://wangak.cc/posts/2dfc19b2.html</id>
    <published>2024-04-01T16:00:00.000Z</published>
    <updated>2024-04-02T00:00:00.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="EPT-Net-Edge-Perception-Transformer-for-3D-Medical-Image-Segmentation"><a href="#EPT-Net-Edge-Perception-Transformer-for-3D-Medical-Image-Segmentation" class="headerlink" title="EPT-Net: Edge Perception Transformer for 3D Medical Image Segmentation"></a>EPT-Net: Edge Perception Transformer for 3D Medical Image Segmentation</h2><p><strong>论文：《EPT-Net: Edge Perception Transformer for 3D Medical Image Segmentation》（TMI 2023)</strong></p><p><strong>EPT-NET:</strong>在编码器中通过CNN提取网络的详细的底层特征，作者提出了双位置Transformer模块通过学习位置编码和像素空间位置编码的过程，增强了定位能力，解决了多器官之间定位不准确的问题，增强了网络对复杂器官形状的理解能力。</p><ul><li><p>提出了一种双位置嵌入Transformer，包括可学习位置嵌入和体素空间位置嵌入。利用该方法对位置编码进行优化，可以有效地捕捉医学图像中不同器官位置之间的内在相关性。</p></li><li><p>提出了一个边缘权重指导模块来学习浅特征中的边缘信息，它可以捕获相邻器官之间的微小粘附。这种设计是在不增加网络参数的情况下最小化边缘信息功能。</p></li></ul><p>卷积操作在局部像素周围进行计算，只能捕获局部特征，缺乏全局上下文信息的提取能力，可能会导致边缘信息的处理不足。</p><h3 id="网络结构"><a href="#网络结构" class="headerlink" title="网络结构"></a>网络结构</h3><p><img src="https://typoraimg.wangak.cc/2023/img/202404050928262.png" alt="image-20240326135759096" style="zoom:50%;"></p><p><strong>EPT-Net:</strong>基于U型网络，由DPT和EWG模块组成。</p><p><strong>双位置嵌入Transformer:</strong></p><p><img src="https://typoraimg.wangak.cc/2023/img/202404050928762.png" alt="image-20240326152549228" style="zoom:50%;"></p><ul><li><p><strong>Learnable Patch Embedding:</strong>使用位置嵌入，并且利用异步卷积，确保相邻的patch有特定的交互部分</p></li><li><p><strong>Voxel Spacial Positional Embedding:</strong>其通过一个3x3x3大小的深度卷积实现</p></li></ul><p><strong>Edge Weight Guidance Module(EWG):</strong></p><p><strong>浅层引导模块：</strong>用于提取低层特征中保留的边缘信息，其操作编码器的前两层，这两层通过一个3x3x3的卷积层调整到同一个分辨率进行级联，级联后的特征通过一个1x1x1的卷积层来得到浅层引导特征。</p><p><strong>加权注意力模块：</strong>通过特征像素之间的数学特性来评估每个特征点的优先级，通过测量目标特征和周围特征的<strong>线性可分性</strong>，<strong>优先级较高的特征点与周围的特征是线性不可分的。</strong></p><p>各特征点的优先级函数定义如下：<br>                                                  <img src="https://typoraimg.wangak.cc/2023/img/202404050929624.png" alt="image-20240326164429692" style="zoom: 67%;"></p><p><em>注1：<script type="math/tex">\hat{x_{i}}、\hat{y_{i}}</script>为特征点的局部信息，其为<script type="math/tex">x_i、y_i</script>通过线性变换得到，<script type="math/tex">ω_t</script>和<script type="math/tex">b_t</script>是变换的权值和偏置,<script type="math/tex">i</script>​为空间维度上的索引</em></p><p><em>注2：根据计算得到的特征点优先级，可以为每个特征点分配一个优先级系数。这个系数可以用来加权整个特征图，从而更好地捕获边缘信息。</em></p><p><script type="math/tex">ω_t</script>和<script type="math/tex">b_t</script>的定义如下：<br>                                                                          <img src="https://typoraimg.wangak.cc/2023/img/202404050929187.png" alt="image-20240330152301027" style="zoom: 67%;"></p><p><img src="https://typoraimg.wangak.cc/2023/img/202404050929943.png" alt="image-20240330152315586" style="zoom:67%;"></p><p>注：ξ是超参数，<script type="math/tex">m_i、v_i</script>是通道上除了<script type="math/tex">x_t</script>的所有特征点的均值和方差</p><p><script type="math/tex">\hat{p_t}</script>越小，表示特征点与周围的特征是线性不可分的，也就越重要，故定义优先级为<script type="math/tex">p=1/\hat{p_t}</script>​</p><p>所以最后的注意力计算为：</p><p><img src="https://typoraimg.wangak.cc/2023/img/202404050929675.png" alt="image-20240330154215190" style="zoom: 67%;"></p>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;EPT-Net-Edge-Perception-Transformer-for-3D-Medical-Image-Segmentation&quot;&gt;&lt;a href=&quot;#EPT-Net-Edge-Perception-Transformer-for-3D-Medical-</summary>
      
    
    
    
    <category term="计算机视觉" scheme="https://wangak.cc/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/"/>
    
    
    <category term="计算机视觉" scheme="https://wangak.cc/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/"/>
    
    <category term="图像分割" scheme="https://wangak.cc/tags/%E5%9B%BE%E5%83%8F%E5%88%86%E5%89%B2/"/>
    
  </entry>
  
  <entry>
    <title>3D Medical image segmentation using parallel transformers</title>
    <link href="https://wangak.cc/posts/c50c9556.html"/>
    <id>https://wangak.cc/posts/c50c9556.html</id>
    <published>2024-03-18T16:00:00.000Z</published>
    <updated>2024-03-20T12:01:42.978Z</updated>
    
    <content type="html"><![CDATA[<h2 id="3D-Medical-image-segmentation-using-parallel-transformers"><a href="#3D-Medical-image-segmentation-using-parallel-transformers" class="headerlink" title="3D Medical image segmentation using parallel transformers"></a>3D Medical image segmentation using parallel transformers</h2><p><strong>TransHRNet:</strong>为并行连接不同分辨率的流而设计</p><p>本文的贡献点：</p><ul><li>提出了一种基于Transfomer的新型深度神经网络（TransHRNet)，将不同分辨率的数据流并行连接，并融合不同分辨率的信息。</li><li>引入EffTrans模块来提高性能</li></ul><p><strong>网络结构：</strong></p><p><img src="https://typoraimg.wangak.cc/2023/img/202403202000079.png" alt="image-20240319122733944" style="zoom: 50%;"></p><p>​        TransHRNet首先使用3D CNN生成X的紧凑特征表示来捕获空间和深度信息，在此过程中不断扩大感受野，并以不同的尺度对特征进行编码，但在此过程中仍然未充分利用图像的信息。作者提出了一个特征增强模块（EffTrans)，利用Transformer编码器来学习全局空间中的长距离依赖关系，同时并行连接不同的分辨率流，在分辨率上重复执行信息交互。之后，解码器通过上采样和卷积操作产生最后的分割结果。</p><p><strong>将图像转换为序列：</strong></p><p>将特征张量的空间和深度维度合并成一个维度，从而得到一个关于图像X的patch嵌入的1D序列。</p><p><strong>位置编码：</strong>使用正余弦位置编码</p><p><img src="https://typoraimg.wangak.cc/2023/img/202403202000117.png" alt="image-20240319130344786" style="zoom: 67%;"></p><p>注：<script type="math/tex">\large \#∈{D, H,W},v =1/10000^{2k\over{C/{3}}},pos为当前编码在序列中的位置，k为位置编码维度的索引</script></p><p><strong>特征融合：</strong>特征增强模块重复融合多分辨率特征，以跨多分辨率交换信息</p><p>三种并行方式：</p><p><img src="https://typoraimg.wangak.cc/2023/img/202403202000398.png" alt="image-20240319132030789" style="zoom:50%;"></p><p>注：高分辨率（绿色）、中分辨率（蓝色）、低分辨率（淡粉色）</p><ul><li>高分辨率与其他分辨率通过上采样得到的结果进行融合，得到新的高分辨率特征</li><li><p>中分辨率、下采样的高分辨率和上采样的低分辨率进行融合，可以获得新的中分辨率特征。</p></li><li><p>低分辨率与其他分辨率通过下采样生成得到的结果进行融合，生成新的低分辨率特征。</p></li></ul><p><strong>Effective Transformer：</strong></p><p>首先使用DeLighT变换对输入进行降维，然后采用Spatial-Reduction Attention (SRA)层进一步降低学习高分辨率特征图的资源成本。最后，将维数从<script type="math/tex">d_m</script>降维到<script type="math/tex">d_m/4</script>，然后将维数从<script type="math/tex">d_m/4</script>展开到<script type="math/tex">d_m</script>，具体结构如下图：</p><p><img src="https://typoraimg.wangak.cc/2023/img/202403202001898.png" alt="image-20240319202446776" style="zoom:67%;"></p>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;3D-Medical-image-segmentation-using-parallel-transformers&quot;&gt;&lt;a href=&quot;#3D-Medical-image-segmentation-using-parallel-transformers&quot; clas</summary>
      
    
    
    
    <category term="计算机视觉" scheme="https://wangak.cc/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/"/>
    
    
    <category term="计算机视觉" scheme="https://wangak.cc/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/"/>
    
    <category term="图像分割" scheme="https://wangak.cc/tags/%E5%9B%BE%E5%83%8F%E5%88%86%E5%89%B2/"/>
    
  </entry>
  
  <entry>
    <title>DELIGHT DEEP AND LIGHT-WEIGHT TRANSFORMER</title>
    <link href="https://wangak.cc/posts/16e5c57e.html"/>
    <id>https://wangak.cc/posts/16e5c57e.html</id>
    <published>2024-03-18T16:00:00.000Z</published>
    <updated>2024-03-19T11:41:41.551Z</updated>
    
    <content type="html"><![CDATA[<h2 id="DELIGHT"><a href="#DELIGHT" class="headerlink" title="DELIGHT"></a>DELIGHT</h2><p><strong>论文：《DELIGHT DEEP AND LIGHT-WEIGHT TRANSFORMER》</strong></p><p>DeLighT比标准的基于Transfomer的模型具有更少的参数，在每个Transformer块中使用DeLighT转换</p><p>DeLighT架构促使在Transformer中用单头注意力和轻量级前馈层替代多头注意力和前馈层，从而减少了总网络参数和运算。</p><p>注：在多头注意力中，每个注意力头都有自己的参数矩阵，因此总参数量较大。而使用单头注意力，参数矩阵只需一个，可以减少参数量。多头注意力需要对每个头进行独立计算，然后将它们合并，而单头注意力只需要进行一次计算，因此在计算上更加高效。</p><h3 id="DeLighT-Transformer"><a href="#DeLighT-Transformer" class="headerlink" title="DeLighT Transformer"></a><strong>DeLighT Transformer</strong></h3><p><strong>DeLighT变换：</strong>将一个<script type="math/tex">d_m</script>维的输入向量映射到一个高维空间（Expansion），然后通过组线性变换（GLT)将其将维到一个<script type="math/tex">d_o</script>维的输出向量（Reduction)。</p><p><img src="https://typoraimg.wangak.cc/2023/img/202403191940652.png" alt="image-20240319143848797"></p><p><em>注：组线性变换（GLT)用于降低Transformer模型的计算复杂度，分成N个组，每个组共享权重矩阵，通过输入的特定部分来输出局部的特征表示，其比线性更高效</em></p><p>为了学习全局表示，DeLighT变换使用特征重排在组线性变换中共享信息。</p><p><strong>模型表达能力的增加：</strong>传统Transformer增加输入维度<script type="math/tex">d_m</script>以增加表达力，而为了增加DeLighT块的表达力，不是增加输入维度<script type="math/tex">d_m</script>，而是通过Expansion和Reduction阶段增加中间DeLighT变换的深度和宽度。这使得我们可以使用更小的维度来计算注意力，从而需要更少的操作。</p><p>注：DeLighT变换的配置参数：GLTs的层数N、宽度乘法器<script type="math/tex">w_m</script>、输入维度<script type="math/tex">d_m</script>、输出维度<script type="math/tex">d_o</script>和GLTs的最大组数<script type="math/tex">g_{max}</script></p><p><strong>DeLighT变换具体步骤如下：</strong></p><ul><li>Expansion：将<script type="math/tex">d_m</script>维输入投影到高维空间，<script type="math/tex">d_{max}=w_md_m</script></li></ul><p><img src="https://typoraimg.wangak.cc/2023/img/202403191940643.png" alt="image-20240319151835407" style="zoom:50%;"></p><p><img src="https://typoraimg.wangak.cc/2023/img/202403191941700.png" alt="image-20240319153553608" style="zoom:50%;"></p><p><em>注：</em></p><ul><li><em><script type="math/tex">l</script>为GLT的层数，<script type="math/tex">g^l</script>表示<script type="math/tex">l</script>层的组数，W、b为可学习的权值和偏置</em></li><li><em><script type="math/tex">F</script>函数将输入分为<script type="math/tex">g^l</script>组然后进行线性变换得到Y</em></li><li><em><script type="math/tex">H</script>函数首先对<script type="math/tex">Y^{l-1}</script>​进行重排，来增加不同组间的信息的交互，然后将其和X进行混合</em></li></ul><p><strong>下图显示了DeLighT转换中的扩展阶段：</strong></p><p><img src="https://typoraimg.wangak.cc/2023/img/202403191940523.png" alt="image-20240319153106416"></p><p><strong>DeLighT Block：</strong>一个包含n个输入token的序列，每个标记的维度为<script type="math/tex">d_m</script>。这些n个<script type="math/tex">d_m</script>维的输入首先被送入DeLighT变换，产生n个维度为<script type="math/tex">d_o</script>的输出，其中<script type="math/tex">d_o < d_m</script>。这些n个<script type="math/tex">d_o</script>维的输出同时经过三个线性层投影，产生<script type="math/tex">d_o</script>维的查询Q、键K和值V</p><p><img src="https://typoraimg.wangak.cc/2023/img/202403191941128.png" alt="image-20240319160401455" style="zoom:50%;"></p><p><em>注：该模块将计算注意力的成本降低了<script type="math/tex">d_m/d_o</script>，作者取<script type="math/tex">d_o=d_m/2</script>​</em></p><p><strong>Light-weight FFN:</strong>r为降维因子，第一层将输入维度<script type="math/tex">d_m</script>降维到<script type="math/tex">d_m/r</script>，第二层将<script type="math/tex">d_m/r</script>再扩展到<script type="math/tex">d_m</script>​，相比于Transformer减少了FFN的参数和计算量</p><h3 id="块的扩展"><a href="#块的扩展" class="headerlink" title="块的扩展"></a><strong>块的扩展</strong></h3><p>与Transformer block相比，DeLighT block块的深度更深（N+4)</p><p>提高模型性能的方法通常通过<strong>增加模型维度（宽度缩放）、堆叠更多的块（深度缩放）</strong>，然而这种方式在小数据集上不是很有效。</p><p>作者认为这是因为缩放模型的宽度和深度在块之间是均匀分配参数的，这会导致模型学习到冗余的参数，于是作者<strong>将模型缩放扩展到块的级别</strong></p><p><img src="https://typoraimg.wangak.cc/2023/img/202403191941032.png" alt="image-20240319192955781" style="zoom: 50%;"></p><p><strong>缩放DeLighT块:</strong>其深度和宽度分别由两个配置参数控制:GLT层数<script type="math/tex">N</script>和宽度乘法器<script type="math/tex">w_m</script>，通过引入了逐块缩放，创建了一个具有可变大小的DeLighT块的网络，在输入附近分配较浅和较窄的DeLighT块，在输出附近分配较深和较宽的DeLighT块。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;DELIGHT&quot;&gt;&lt;a href=&quot;#DELIGHT&quot; class=&quot;headerlink&quot; title=&quot;DELIGHT&quot;&gt;&lt;/a&gt;DELIGHT&lt;/h2&gt;&lt;p&gt;&lt;strong&gt;论文：《DELIGHT DEEP AND LIGHT-WEIGHT TRANSFOR</summary>
      
    
    
    
    <category term="计算机视觉" scheme="https://wangak.cc/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/"/>
    
    
    <category term="计算机视觉" scheme="https://wangak.cc/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/"/>
    
    <category term="图像分割" scheme="https://wangak.cc/tags/%E5%9B%BE%E5%83%8F%E5%88%86%E5%89%B2/"/>
    
  </entry>
  
  <entry>
    <title>VSA</title>
    <link href="https://wangak.cc/posts/73df481.html"/>
    <id>https://wangak.cc/posts/73df481.html</id>
    <published>2024-03-16T16:00:00.000Z</published>
    <updated>2024-03-19T04:14:29.444Z</updated>
    
    <content type="html"><![CDATA[<h2 id="VSA-Learning-Varied-Size-Window-Attention-in-Vision-Transformers"><a href="#VSA-Learning-Varied-Size-Window-Attention-in-Vision-Transformers" class="headerlink" title="VSA: Learning Varied-Size Window Attention in Vision Transformers"></a>VSA: Learning Varied-Size Window Attention in Vision Transformers</h2><p><strong>论文：《VSA: Learning Varied-Size Window Attention in Vision Transformers》（ECCV 2022)</strong></p><p>对于Swin Transfomer中的窗口注意力机制，如果窗口大小为可变的矩形窗口，其大小和位置直接从数据中学习，则transfomer可以从不同的窗口中捕获丰富的上下文，并学习更强大的对象特征表示。</p><p>VSA使用窗口回归模块根据每个默认窗口中的token来预测目标窗口的大小和位置</p><p><img src="https://typoraimg.wangak.cc/2023/img/202403191210588.png" alt="image-20240314104423831" style="zoom: 67%;"></p><p>VSA将可学习的可变大小的窗口注意力引入到Transformer中</p><p><strong>Varied-size window attention (VSA)的注意力机制:</strong>VSA允许查询令牌（query tokens）关注远处的区域，并赋予网络确定目标窗口大小（即注意力区域）的灵活性</p><ul><li><p>VSA先将输入特征划分为几个窗口，这些窗口的大小基于预定义的w，这样的窗口被称为<strong>默认窗口</strong></p></li><li><p>然后从默认窗口获取查询特征：通过一个线性变换来实现</p><p><img src="https://typoraimg.wangak.cc/2023/img/202403191210967.png" alt="image-20240315133836606" style="zoom:67%;"></p></li><li><p><strong>VSR模块:</strong>用于估计每个默认窗口的目标窗口的大小和位置</p></li></ul><p>​        包括一个平均池化层（average pooling layer）、一个LeakyReLU激活层和一个步长为1的1×1卷积层，按顺序排列。池化层的核大小和步幅遵循默认窗口的大小。</p><p><img src="https://typoraimg.wangak.cc/2023/img/202403191210935.png" alt="image-20240315133540975"></p><p><em>注1：<script type="math/tex">S_w</script>表示目标窗口相对于默认窗口位置的水平和垂直方向上的缩放比例,<script type="math/tex">O_w</script>是目标窗口相对于默认窗口的水平和垂直方向上的偏移量。</em></p><p><em>注2：<script type="math/tex">S_w、O_w\in R^{2×N}</script>,N为注意力头的个数</em></p><p><strong>VSA的计算过程：</strong></p><ul><li><p>首先，从特征图X得到K、V:</p><p><img src="https://typoraimg.wangak.cc/2023/img/202403191210047.png" alt="image-20240315135546889" style="zoom:67%;"></p><p><em>注：<script type="math/tex">K、V\in R^{H×W×C}</script></em></p></li><li><p>VSA模块分别在K、V上从每个不同大小的窗口（目标窗口）均匀采样M个特征，M设为w*w来使其计算的复杂度和窗口注意力相当。</p></li><li><p>对于Q、K、W进行注意力计算</p><p><em>注：由于K、V是从不同的位置采样得到的，因此使用相对位置嵌入可能无法很好地描述空间关系，所以在MHSA层之前采用<strong>条件位置嵌入（CPE)《Conditional positional encodings for vision transformers.（2021）》</strong>将空间关系提供给模型。</em></p></li></ul><p><img src="https://typoraimg.wangak.cc/2023/img/202403191210438.png" alt="image-20240315140839238" style="zoom:67%;"></p><p>​    <em>注：<script type="math/tex">Z^{l-1}</script>为前一个Transformer的输出特征，CPE由一个深度卷积层实现，其核大小设为窗口大小</em></p><p>模型的网络结构如下：<br><img src="https://typoraimg.wangak.cc/2023/img/202403191210313.png" alt="image-20240315142709174" style="zoom:67%;"></p><h3 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h3><p>对于语义分割任务：VSA模块可以提高baseline的性能<br><img src="https://typoraimg.wangak.cc/2023/img/202403191210600.png" alt="image-20240315143148652"></p>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;VSA-Learning-Varied-Size-Window-Attention-in-Vision-Transformers&quot;&gt;&lt;a href=&quot;#VSA-Learning-Varied-Size-Window-Attention-in-Vision-Tran</summary>
      
    
    
    
    <category term="计算机视觉" scheme="https://wangak.cc/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/"/>
    
    
    <category term="计算机视觉" scheme="https://wangak.cc/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/"/>
    
    <category term="图像分割" scheme="https://wangak.cc/tags/%E5%9B%BE%E5%83%8F%E5%88%86%E5%89%B2/"/>
    
  </entry>
  
  <entry>
    <title>TransNeXt</title>
    <link href="https://wangak.cc/posts/965265e1.html"/>
    <id>https://wangak.cc/posts/965265e1.html</id>
    <published>2024-03-07T16:00:00.000Z</published>
    <updated>2024-03-08T06:36:44.519Z</updated>
    
    <content type="html"><![CDATA[<h2 id="TransNeXt"><a href="#TransNeXt" class="headerlink" title="TransNeXt"></a>TransNeXt</h2><p><strong>论文：《TransNeXt: Robust Foveal Visual Perception for Vision Transformers》（CVPR 2024）</strong></p><h3 id="网络结构"><a href="#网络结构" class="headerlink" title="网络结构"></a>网络结<strong>构</strong></h3><p><img src="https://typoraimg.wangak.cc/2023/img/202403081436489.png" alt="image-20240308133954690"></p><p>像素聚焦注意力机制：在每个查询附近具有细粒度感知，同时保持对全局的粗粒度感知（结合了滑动窗口注意力和集中注意力）</p><p><strong>像素聚焦注意力(pixel-focused attention (PFA) )</strong>具体过程如下：</p><p><img src="https://typoraimg.wangak.cc/2023/img/202403081436294.png" alt="image-20240307093447355" style="zoom: 67%;"></p><p><em>注1：定义输入特征图上以 <script type="math/tex">（i, j）</script>为中心的滑动窗口中的像素集合为<script type="math/tex">ρ(i, j）</script> 。对于固定的窗口大小 <script type="math/tex">k×k</script> ， <script type="math/tex">||ρ(i, j)|| = k^2</script>。同时, 作者定义从特征图池化得到的像素集合为的 σ(<script type="math/tex">X</script>) 。对于池化大小 <script type="math/tex">Hp×Wp</script>，<script type="math/tex">||σ(X) || = Hp×Wp</script>。</em></p><p><em>注2：PFA由滑动窗口注意力和池化窗口注意力两部分组成</em></p><p><strong>池化：</strong>平均池化操作会严重丢失信息，所以在池化之前使用单层神经网络进行投影和激活，提前压缩提取有用的信息，从而提高下采样后的信息压缩率</p><p>下采样算子（Activate and Pool）如下：</p><p><img src="https://typoraimg.wangak.cc/2023/img/202403081436127.png" alt="image-20240307094953654" style="zoom: 67%;"></p><p><strong>LKV通过添加一个可学习的查询嵌入（QE)，来聚合多样的注意力：</strong></p><p><img src="https://typoraimg.wangak.cc/2023/img/202403081436731.png" alt="image-20240308093422092" style="zoom:67%;"></p><p><strong>QLV破坏了键和值之间的一对一对应关系，使当前查询学习更多的隐式相对位置信息</strong></p><p><img src="https://typoraimg.wangak.cc/2023/img/202403081436695.png" alt="image-20240308094738641" style="zoom:67%;"></p><p><strong>长度缩放余弦注意力:</strong>可以有效增强视觉模型的训练稳定性</p><p><img src="https://typoraimg.wangak.cc/2023/img/202403081436647.png" alt="image-20240308100847389" style="zoom:67%;"></p><p><em>注1：注意力设计应表现出熵不变性，以促进对未知长度的更好泛化，<script type="math/tex">τ log N</script>是为了保持熵的不变性和忽略常数项。</em></p><p><em>注2:<script type="math/tex">τ</script>是可学习参数，N表示每个查询交互的有效键的计数（在本文中N为<script type="math/tex">N(i,j) = ∥ρ(i, j)∥+∥σ(X)∥− ∥µ(i, j)∥</script>),<script type="math/tex">\hat{Q}、\hat{K}</script>是L2正则化后的结果</em></p><p><strong>位置偏置：</strong></p><ul><li><strong>池化特征路径：</strong>log-CPB方法，即使用一个具有 ReLU 激活函数的 2 层 MLP 来计算从查询点到空间相对坐标之间的位置偏差</li></ul><p><em>注：池化使得特征的空间信息变得模糊或丢失，直接使用可学习的位置偏差可能无法有效地捕获到像素级别的位置偏差，所以使用对数间隔连续位置偏差（log-CPB）方法，通过 MLP 网络计算位置偏差，可以更好地捕获到细粒度的位置信息，从而提高模型对多尺度图像输入的泛化能力。</em></p><ul><li><strong>滑动窗口路径</strong>：在这条路径上，作者直接使用了一个可学习的位置偏差 B(i,j)∼ρ(i,j)。</li></ul><p><strong>聚合像素聚焦注意力(AA)：</strong></p><p><img src="https://typoraimg.wangak.cc/2023/img/202403081436129.png" alt="image-20240308103811904" style="zoom: 67%;"></p><p>注：∆(i,j) ~ σ(X)为Q(i,j)和Kσ(X)之间的空间相对坐标</p><p><strong>卷积GLU通道混合器：</strong></p><p><img src="https://typoraimg.wangak.cc/2023/img/202403081436975.png" alt="image-20240308110209837"></p><p><strong>SE 机制</strong>的全局平均池过于粗粒度</p><p><strong>卷积GLU通道混合器</strong>中的通道注意力是基于每个像素点的最近邻特征计算的（深度卷积），而且其可以提供一定的位置信息</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;TransNeXt&quot;&gt;&lt;a href=&quot;#TransNeXt&quot; class=&quot;headerlink&quot; title=&quot;TransNeXt&quot;&gt;&lt;/a&gt;TransNeXt&lt;/h2&gt;&lt;p&gt;&lt;strong&gt;论文：《TransNeXt: Robust Foveal Visua</summary>
      
    
    
    
    <category term="计算机视觉" scheme="https://wangak.cc/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/"/>
    
    
    <category term="计算机视觉" scheme="https://wangak.cc/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/"/>
    
    <category term="图像分割" scheme="https://wangak.cc/tags/%E5%9B%BE%E5%83%8F%E5%88%86%E5%89%B2/"/>
    
  </entry>
  
  <entry>
    <title>H2Former</title>
    <link href="https://wangak.cc/posts/457d0431.html"/>
    <id>https://wangak.cc/posts/457d0431.html</id>
    <published>2024-03-05T16:00:00.000Z</published>
    <updated>2024-03-08T06:36:15.049Z</updated>
    
    <content type="html"><![CDATA[<h2 id="H2Former"><a href="#H2Former" class="headerlink" title="H2Former"></a>H2Former</h2><p><strong>论文：《H2Former: An Efficient Hierarchical Hybrid Transformer for Medical Image Segmentation》（TMI 2023）</strong></p><p><strong>文章贡献：</strong></p><ul><li>提出了一个分层混合模型，整合了CNN的局部信息、多尺度通道注意力特征和Transformer的长距离特征在一个统一的块内。</li><li>一个轻量级的多尺度通道注意力（MSCA）</li><li>在三个2D和两个3D医学图像分割任务上证明了模型的有效性</li></ul><h3 id="网络结构："><a href="#网络结构：" class="headerlink" title="网络结构："></a>网络结构：</h3><p><img src="https://typoraimg.wangak.cc/2023/img/202403081435594.png" alt="image-20240306092858101" style="zoom:67%;"></p><p><strong>Conv stem:</strong>由一个卷积层和一个最大池化层，将输入的图像划分成多个patch，并对每个小块进行特征提取。</p><p><strong>Hybrid Transformer Block：</strong>在特征提取过程中引入多尺度通道注意力、卷积层和Transformer层，以解决卷积的局部性和Transformer单一尺度特征的局限性。</p><ul><li><p><strong>Multi-Scale Channel Attention (MSCA)：</strong>捕捉不同形状和尺度的多尺度特征，并通过通道注意力机制对这些特征进行校准，从而去增强模型的表达能力</p><p>对于每个特征图，MSCA将其划分为多个尺度的token，并将这些token连接起来形成多尺度token(该过程通过使用具有不同大小的卷积核，步长为1的卷积操作来实现)</p><p>具体操作如下：</p><p><img src="https://typoraimg.wangak.cc/2023/img/202403081435419.png" alt="image-20240306154618072" style="zoom: 67%;"></p><p><img src="https://typoraimg.wangak.cc/2023/img/202403081435989.png" alt="image-20240306154640535" style="zoom:67%;"></p></li></ul><p>​      注：MST是multi-scale tokenization layers，其是使用s×s的核，步长为1的卷积实现的*</p><p>​    之后再使用多尺度通道注意力对提取的特征进行校准：</p><p>​                                                <img src="https://typoraimg.wangak.cc/2023/img/202403081436595.png" alt="image-20240306155304766" style="zoom: 67%;"></p><p>​    注：GAP(全局平均池化)、σ（Sigmoid激活）、Conv1d(kernel为3的一维卷积层）</p><p><img src="https://typoraimg.wangak.cc/2023/img/202403081436773.png" alt="image-20240306154143774" style="zoom: 67%;"></p><p>​        为了降低MSCA的计算复杂度，大尺度时采用低维，小尺度时采用高维。对于第1阶段的高分辨率特征图，采用4 × 4、8 × 8、16 × 16和32 × 32四种尺度，输出维度分别为C/2、C/4、C/8和C/8。其余阶段，使用2 × 2和4 × 4两种尺度，输出维度分别为C/2、C/4。通过这样的方式，MSCA可以在合理的计算复杂度下提取多尺度特征。</p><ul><li><p><strong>Conv Block:</strong>采用卷积块提取局部空间特征</p></li><li><p><strong>Transformer Block:</strong>为了使Transfomer块能够感知多尺度通道特征和局部空间特征，将多尺度通道特征和局部空间特征集成为Transfomer块的输入。</p><p><em>注：MHSA是基于窗口的多头自注意</em></p></li></ul><h3 id="实验部分："><a href="#实验部分：" class="headerlink" title="实验部分："></a>实验部分：</h3><p><img src="https://typoraimg.wangak.cc/2023/img/202403081436089.png" alt="image-20240306161724728" style="zoom: 50%;"><img src="https://typoraimg.wangak.cc/2023/img/202403081436692.png" alt="image-20240306161741487" style="zoom: 50%;"></p><p><em>注1：H2Former相较于纯Transformer方法、纯CNN方法具有比较明显的优势</em></p><p><em>注2：表一为皮肤病变分割任务，表二是息肉病灶分割任务</em></p><p><strong>分割结果的可视化的对比：</strong></p><p><img src="https://typoraimg.wangak.cc/2023/img/202403081436935.png" alt="image-20240306162509652" style="zoom:67%;"></p><p><em>注：前两行是皮肤病变分割的结果，后两行是息肉分割的结果。</em></p><p><em>通过对比发现：第一排的皮肤图像被毛发遮挡，第三排的息肉图像也被遮挡，但该模型仍可以产生清晰的边界，而其他模型的分割性能会因遮挡而下降</em></p>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;H2Former&quot;&gt;&lt;a href=&quot;#H2Former&quot; class=&quot;headerlink&quot; title=&quot;H2Former&quot;&gt;&lt;/a&gt;H2Former&lt;/h2&gt;&lt;p&gt;&lt;strong&gt;论文：《H2Former: An Efficient Hierarchica</summary>
      
    
    
    
    <category term="计算机视觉" scheme="https://wangak.cc/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/"/>
    
    
    <category term="计算机视觉" scheme="https://wangak.cc/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/"/>
    
    <category term="图像分割" scheme="https://wangak.cc/tags/%E5%9B%BE%E5%83%8F%E5%88%86%E5%89%B2/"/>
    
  </entry>
  
  <entry>
    <title>MedSegDiff-v2</title>
    <link href="https://wangak.cc/posts/2d77667e.html"/>
    <id>https://wangak.cc/posts/2d77667e.html</id>
    <published>2024-03-04T16:00:00.000Z</published>
    <updated>2024-03-06T13:06:37.351Z</updated>
    
    <content type="html"><![CDATA[<h2 id="MedSegDiff-v2"><a href="#MedSegDiff-v2" class="headerlink" title="MedSegDiff-v2"></a>MedSegDiff-v2</h2><p><strong>本文的贡献包括：</strong></p><ul><li>第一个将Transformer集成到基于扩散的通用医学图像分割模型中。 </li><li>提出了具有U-SA(Uncertain Spatial Attention)的Anchor Condition以减小扩散方差。</li><li>提出了具有SS-Former的Semantic Condition，以建模分割噪声和语义特征的相互作用。 </li><li>在包括5种图像模态的20个器官分割任务上实现了SOTA性能。</li></ul><h3 id="总体架构"><a href="#总体架构" class="headerlink" title="总体架构"></a>总体架构<img src="https://typoraimg.wangak.cc/2023/img/202403041215805.png" alt="image-20240104151754725"></h3><p><strong>条件模型（绿色UNet)：</strong>从原始图像中提取分割特征</p><p><strong>扩散模型（蓝色UNet)：</strong>有两个输入分别是锚点条件（蓝色箭头）和噪声分割信息（黑色箭头）</p><h3 id="锚定条件与U-SA"><a href="#锚定条件与U-SA" class="headerlink" title="锚定条件与U-SA"></a>锚定条件与U-SA</h3><p><strong>锚点条件：</strong>将条件模型的解码分割特征整合到扩散模型的编码器特征中（蓝色箭头）</p><p><strong>U-SA机制</strong>用于从条件模型中提取一个粗糙的锚点特征，并将其整合到扩散模型中，为扩散模型提供了一个正确的预测范围，同时也让它进一步完善了预测结果，具体做法如下：</p><p><img src="https://typoraimg.wangak.cc/2023/img/202403041215835.png" alt="image-20240104161300625" style="zoom:67%;"></p><p><em>注1：<script type="math/tex">f_c^{-1}</script>是来自原始图像的条件分割特征，<script type="math/tex">f_d^0</script>是噪声分割图像的扩散特征</em></p><p><em>注2：<script type="math/tex">k_{Gauss}</script>为高斯卷积核目的是做平滑处理，然后取最大值是为了保留最相关的信息并且消除一些噪声和不必要的细节</em></p><h3 id="语义条件与SS-Former"><a href="#语义条件与SS-Former" class="headerlink" title="语义条件与SS-Former"></a>语义条件与SS-Former</h3><p><strong>SS-Former：</strong>使得模型可以学习条件语义特征与噪声信息的交互。</p><p>具体设计如下：</p><p><img src="https://typoraimg.wangak.cc/2023/img/202403041215516.png" alt="image-20240104164023507"></p><p><strong><script type="math/tex">M=(F(c_0)W_q)(F(e)W_k)^T</script></strong></p><p><em>注1：绿色为语义信息，蓝色为噪声信息</em></p><p><em>注2：<script type="math/tex">W_q</script>和<script type="math/tex">W_k</script>是可学习的权重参数，它们用于线性映射，以调整语义信息和噪声信息之间的关系</em></p><p><strong>NBP-Filter:</strong>将语义信息和噪声信息交互得到的结果M调整到统一的频率范围(扩散模型生成的过程中具有一定的随机性，这种随机性会对数据进行一些扰动，NBP-Filter降低扩散生成过程中引入的随机性的影响)</p><p>NBP-Filter<strong>从坐标图学习权重图，坐标图的每个点代表了不同的频率分量，并且使用两个MLP层将时间信息投影到两个值</strong>，这两个值分别用于缩放和移位，然后使用得到的缩放因子和移位因子去调整坐标图的表示。</p><h2 id="实验部分"><a href="#实验部分" class="headerlink" title="实验部分"></a>实验部分</h2><p><strong>表1：MedSegDiff-V2在不同图像模态上与SOTA分割方法的比较。灰色背景表示这些方法是为特定任务提出的。</strong></p><p><img src="https://typoraimg.wangak.cc/2023/img/202403041215578.png" alt="image-20240105132907016"></p><p>注：REFUGE2（眼底图像）、BraTs （脑肿瘤分割)、TNMIX(甲状腺结节分割)、ISIC（皮肤病变分割）</p><p><strong>表2：使用Dice分数评估MedSegDiff-V2与SOTA分割方法在AMOS数据集上的比较。最佳结果以粗体表示。</strong></p><p><img src="https://typoraimg.wangak.cc/2023/img/202403041215186.png" alt="image-20240105133848749"></p><p><img src="https://typoraimg.wangak.cc/2023/img/202403041215066.png" alt="image-20240105134743328" style="zoom:50%;"></p><p><strong>表3：MedSegDiff-V2在BTCV数据集上与其他先进分割方法的比较结果的表格。Dice Score用于评估模型性能，最佳结果以粗体标注。</strong></p><p><img src="https://typoraimg.wangak.cc/2023/img/202403041215270.png" alt="image-20240105135108153"></p><h3 id="消融实验"><a href="#消融实验" class="headerlink" title="消融实验"></a><strong>消融实验</strong></h3><p><img src="https://typoraimg.wangak.cc/2023/img/202403041215511.png" alt="image-20240105135511006"></p><p>提出的U-SA在所有数据集上均优于先前的空间注意力。</p><p>仅使用SS-Former提供了较小的改进，但与NBP-Filter结合使用则带来了显著的改进，证明了提出的SS-Former设计的有效性。</p><h3 id="采样次数对模型效果的影响"><a href="#采样次数对模型效果的影响" class="headerlink" title="采样次数对模型效果的影响"></a>采样次数对模型效果的影响</h3><p>扩散模型具有一定的随机性，所以需要进行多次采样，然后将采样后的结果集成在一起。</p><p><strong>作者评估了各种基于扩散的医学分割模型的采样次数对模型效果的影响</strong><img src="https://typoraimg.wangak.cc/2023/img/202403041215838.png" alt="image-20240105140410979" style="zoom: 67%;"></p><p>最佳性能在大约50个集成后实现</p><p>MedSegDiff-V2其他扩散方法进行比较时，观察到它需要更少的采样次数来收敛。</p><p>MedSegDiff-V2更优越的起始点和更稳定的预测可以导致更高的性能上限</p><p><strong>作者在REFUGE2-Cup数据集上讨论了样本多样性对模型效果的影响</strong></p><p><img src="https://typoraimg.wangak.cc/2023/img/202403041215579.png" alt="image-20240105143354543" style="zoom:67%;"></p><p><strong>CI(置信区间):</strong>置信区间越大，一致性越好。</p><p><strong>GED(广义能量距离)：</strong>GED越低，表示一致性越好。</p><p>U-SA在CI方面较低，而在GED方面较高，表明样本的多样性较大，表明其生成的样本大部分落在目标的不确定性区域内，效果不好。</p><p>而单独使用SS-Former而没有U-SA时，该模型在CI最高、GED最低的情况下达到了最好的一致性，未能充分利用扩散模型的多样性集成能力。</p><p>将U-SA和SS-Former组合成MedSegDiff-V2，性能得到了显著提高，表明SS-Former有助于减轻U-SA中生成的噪声，而U-SA为模型提供了更多的多样性，从而相互改进。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;MedSegDiff-v2&quot;&gt;&lt;a href=&quot;#MedSegDiff-v2&quot; class=&quot;headerlink&quot; title=&quot;MedSegDiff-v2&quot;&gt;&lt;/a&gt;MedSegDiff-v2&lt;/h2&gt;&lt;p&gt;&lt;strong&gt;本文的贡献包括：&lt;/strong&gt;&lt;</summary>
      
    
    
    
    <category term="计算机视觉" scheme="https://wangak.cc/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/"/>
    
    
    <category term="计算机视觉" scheme="https://wangak.cc/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/"/>
    
    <category term="图像分割" scheme="https://wangak.cc/tags/%E5%9B%BE%E5%83%8F%E5%88%86%E5%89%B2/"/>
    
  </entry>
  
  <entry>
    <title>ConvFormer</title>
    <link href="https://wangak.cc/posts/badf7b43.html"/>
    <id>https://wangak.cc/posts/badf7b43.html</id>
    <published>2024-03-03T16:00:00.000Z</published>
    <updated>2024-03-08T06:35:46.756Z</updated>
    
    <content type="html"><![CDATA[<h2 id="ConvFormer"><a href="#ConvFormer" class="headerlink" title="ConvFormer"></a>ConvFormer</h2><p><strong>论文：《ConvFormer: Plug-and-Play CNN-Style Transformers for Improving Medical Image Segmentation》（MICCAI 2023）</strong></p><p><strong>以往方法的不足：</strong></p><p>由于训练数据的不足，transformers效果较差（另一方面，医学图像本身的高冗余性）</p><p>在CNN-Transformer混合方法中，一方面，训练数据不足会使得transformers学习到次优的长距离依赖性，另一方面，直接将CNNs与transformers结合会使得网络偏向于学习CNNs，因为与transformers相比，CNNs的收敛性更容易实现，特别是在小规模训练数据上。</p><p>为解决该问题，作者提出了一个名为<strong>ConvFormer</strong>的即插即用模块</p><h3 id="网络结构："><a href="#网络结构：" class="headerlink" title="网络结构："></a><strong>网络结构：</strong></h3><p><img src="https://typoraimg.wangak.cc/2023/img/202403081435986.png" alt="image-20240304145207066" style="zoom:67%;"></p><p><strong>卷积+池化：</strong></p><p>上图最左边是传统的ViT，用一个个的patch作为自注意力的输入。</p><p>而作者用二维图像直接建立足够长的长程依赖，而不是分割为一堆一维序列，对于一个输入的图像通过卷积和池化来降低分辨率。</p><p><em>注：</em></p><p><em>1.上图中CBR是指卷积、批量归一化和Relu的组合</em></p><p><em>2.d为ViT中的每个patch大小S的对数</em></p><p>在此过程中，<img src="/posts/badf7b43.htm/Users\wangak\AppData\Roaming\Typora\typora-user-images\image-20240304150529624.png" alt="image-20240304150529624" style="zoom: 25%;"></p><p>其中，Cm对应于ViT中的嵌入维度</p><p><strong>CNN风格的自关注：</strong>为卷积+池化模块处理后的特征构建了一个自适应的卷积核</p><ul><li><p>利用可学习的矩阵乘上输入进来的特征，得到QKV：</p><p><img src="https://typoraimg.wangak.cc/2023/img/202403081435314.png" alt="image-20240304151242512" style="zoom:50%;"></p></li><li><p>Q和K按照如下方式进行计算(余弦相似度)，对应于ViT里面的注意力分数计算：</p><p><img src="https://typoraimg.wangak.cc/2023/img/202403081435848.png" alt="image-20240304151521415" style="zoom: 67%;"></p></li></ul><p><em>注1：Q和K作为分母，这样一来I矩阵的元素就不太容易变成0（如果某些位置的注意力值为0，那么表示模型在计算该位置的输出时不考虑与其他位置的相关性，可能导致模型在捕捉输入序列中重要的依赖关系和特征时出现问题。）</em></p><p><em>注2：<script type="math/tex">c_q</script>对应于ViT中𝑄、𝐾和𝑉的嵌入维度</em></p><ul><li>引入一个可学习的高斯距离图M：</li></ul><p><img src="https://typoraimg.wangak.cc/2023/img/202403081435731.png" alt="image-20240304151953190" style="zoom: 67%;"></p><p><em>注1：𝜃 ∈ (0, 1) 是一个可学习的网络参数，用于控制𝐴的感受野，𝛼 是一个超参数，用于控制感受野的倾向性。</em></p><p><em>注2：𝜃与感受野成正比。𝛼越大,𝐴越倾向于具有全局感受野。</em></p><p><strong>CFFN：</strong>仅由1 × 1卷积、批处理归一化和Relu两种组合组成。通过替换ViT中的线性投影和层归一化，CFFN使ConvFormer完全基于CNN，避免了CNN-Transformer混合方法在训练过程中CNN和Transformer之间的冲突。</p><h3 id="实验："><a href="#实验：" class="headerlink" title="实验："></a>实验：</h3><p><img src="https://typoraimg.wangak.cc/2023/img/202403081435131.png" alt="image-20240304160737425" style="zoom:67%;"></p><p><em>注：ACDC(心脏诊断)、ISIC（皮肤病变）、ICH（血肿分割）</em></p>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;ConvFormer&quot;&gt;&lt;a href=&quot;#ConvFormer&quot; class=&quot;headerlink&quot; title=&quot;ConvFormer&quot;&gt;&lt;/a&gt;ConvFormer&lt;/h2&gt;&lt;p&gt;&lt;strong&gt;论文：《ConvFormer: Plug-and-Play </summary>
      
    
    
    
    <category term="计算机视觉" scheme="https://wangak.cc/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/"/>
    
    
    <category term="计算机视觉" scheme="https://wangak.cc/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/"/>
    
    <category term="图像分割" scheme="https://wangak.cc/tags/%E5%9B%BE%E5%83%8F%E5%88%86%E5%89%B2/"/>
    
  </entry>
  
  <entry>
    <title>智能指针</title>
    <link href="https://wangak.cc/posts/38a918c7.html"/>
    <id>https://wangak.cc/posts/38a918c7.html</id>
    <published>2023-12-27T16:00:00.000Z</published>
    <updated>2023-12-28T00:00:00.000Z</updated>
    
    <content type="html"><![CDATA[<h3 id="1-RAll"><a href="#1-RAll" class="headerlink" title="1.RAll"></a>1.RAll</h3><p>RAII：用于有效地管理资源的获取和释放。</p><p>基本思想：资源的获取应当在对象的构造函数中进行，而资源的释放则应当在对象的析构函数中进行。</p><p><strong>RAII 的主要优势：</strong></p><ul><li>RAII 可以确保资源的正确获取和释放，避免了手动管理资源时可能发生的错误。</li><li>当使用 RAII 时，如果在构造函数中发生异常，对象会在析构函数中自动被销毁，从而保证资源被正确释放。</li></ul><h3 id="2-智能指针"><a href="#2-智能指针" class="headerlink" title="2.智能指针"></a>2.智能指针</h3><h4 id="2-1-普通指针存在的问题"><a href="#2-1-普通指针存在的问题" class="headerlink" title="2.1 普通指针存在的问题"></a>2.1 普通指针存在的问题</h4><p><strong>内存泄漏：</strong> 使用普通指针时，需要手动分配和释放内存，这就需要确保在适当的时候调用 <code>delete</code> 或 <code>delete[]</code> 来释放动态分配的内存，否则会导致会导致内存泄漏。</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;iostream&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    <span class="comment">// 动态分配一个整数的内存</span></span><br><span class="line">    <span class="type">int</span>* ptr = <span class="keyword">new</span> <span class="type">int</span>;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 其他代码...</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">// 忘记释放内存</span></span><br><span class="line">    <span class="comment">// delete ptr;  // 此行代码注释掉了，导致内存泄漏</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><em>注：普通指针不会自动调用析构函数。</em></p><p><strong>悬挂指针：</strong>程序中的某个部分释放了一块动态分配的内存，而其他部分仍然持有指向该内存的指针，并尝试使用或修改这个指针所指向的内存时，就会导致悬挂指针问题。</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;iostream&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    <span class="type">int</span>* ptr = <span class="keyword">new</span> <span class="type">int</span>;  <span class="comment">// 分配动态内存</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">delete</span> ptr;  <span class="comment">// 释放内存</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">// ptr 现在是悬挂指针，指向已释放的内存</span></span><br><span class="line">    <span class="comment">// 下面的访问操作是未定义行为</span></span><br><span class="line">    std::cout &lt;&lt; *ptr &lt;&lt; std::endl;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h4 id="2-2-std-shared-ptr"><a href="#2-2-std-shared-ptr" class="headerlink" title="2.2 std::shared_ptr"></a>2.2 std::shared_ptr</h4><p><strong><code>std::shared_ptr:</code></strong>共享式智能指针,允许多个指针共享对同一对象的所有权，通过引用计数机制来管理资源的生命周期。</p><p><strong>创建和初始化：</strong></p><ul><li><strong>方法一：使用<code>std::make_shared</code>(好)</strong></li></ul><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 使用 std::make_shared 创建 shared_ptr</span></span><br><span class="line">std::shared_ptr&lt;<span class="type">int</span>&gt; sharedPtr1 = std::<span class="built_in">make_shared</span>&lt;<span class="type">int</span>&gt;(<span class="number">42</span>)</span><br></pre></td></tr></table></figure><ul><li><strong>方法二：使用构造函数</strong></li></ul><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 使用构造函数创建 shared_ptr</span></span><br><span class="line"><span class="function">std::shared_ptr&lt;<span class="type">int</span>&gt; <span class="title">sharedPtr2</span><span class="params">(<span class="keyword">new</span> <span class="type">int</span>(<span class="number">42</span>))</span></span>;</span><br></pre></td></tr></table></figure><p><strong>共享所有权：</strong></p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;memory&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">MyClass</span> &#123;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="built_in">MyClass</span>() &#123;</span><br><span class="line">        std::cout &lt;&lt; <span class="string">&quot;MyClass constructed.&quot;</span> &lt;&lt; std::endl;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    ~<span class="built_in">MyClass</span>() &#123;</span><br><span class="line">        std::cout &lt;&lt; <span class="string">&quot;MyClass destructed.&quot;</span> &lt;&lt; std::endl;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    <span class="comment">// 创建 shared_ptr 共享同一个对象</span></span><br><span class="line">    std::shared_ptr&lt;MyClass&gt; sharedPtr1 = std::<span class="built_in">make_shared</span>&lt;MyClass&gt;();</span><br><span class="line">    std::shared_ptr&lt;MyClass&gt; sharedPtr2 = sharedPtr1;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 当最后一个 shared_ptr 离开作用域时，对象的析构函数会被调用</span></span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><strong>引用计数：</strong>使用计数器，记录当前有多少个指针（引用）指向该资源。当计数器为零时，表示没有任何指针指向该资源，资源可以被释放。</p><p><em>注：<code>std::shared_ptr</code> 会为每个共享的对象分配一个控制块，这个控制块包含引用计数、指向实际对象的指针等信息。</em></p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;memory&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;iostream&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    std::shared_ptr&lt;<span class="type">int</span>&gt; sharedPtr1 = std::<span class="built_in">make_shared</span>&lt;<span class="type">int</span>&gt;(<span class="number">42</span>);</span><br><span class="line">    std::shared_ptr&lt;<span class="type">int</span>&gt; sharedPtr2 = sharedPtr1;</span><br><span class="line">    </span><br><span class="line">    std::cout &lt;&lt; <span class="string">&quot;use_count: &quot;</span> &lt;&lt; sharedPtr1.<span class="built_in">use_count</span>() &lt;&lt; std::endl;  <span class="comment">// 输出 2</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><strong>std::shared_ptr的问题：循环引用</strong></p><p>循环引用是指两个或多个对象相互引用，形成一个环状结构，导致它们的引用计数永远不会降为零。这种情况可能导致内存泄漏，因为对象的资源（如动态分配的内存）将无法被释放。</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;memory&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;iostream&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">ClassB</span>;  <span class="comment">// 前向声明</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">ClassA</span> &#123;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    std::shared_ptr&lt;ClassB&gt; bPtr;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">ClassB</span> &#123;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    std::shared_ptr&lt;ClassA&gt; aPtr;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    <span class="comment">// 创建对象并建立循环引用</span></span><br><span class="line">    std::shared_ptr&lt;ClassA&gt; aPtr = std::<span class="built_in">make_shared</span>&lt;ClassA&gt;();</span><br><span class="line">    std::shared_ptr&lt;ClassB&gt; bPtr = std::<span class="built_in">make_shared</span>&lt;ClassB&gt;();</span><br><span class="line"></span><br><span class="line">    aPtr-&gt;bPtr = bPtr;  <span class="comment">// ClassA 包含 ClassB</span></span><br><span class="line">    bPtr-&gt;aPtr = aPtr;  <span class="comment">// ClassB 包含 ClassA</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">// 对象的引用计数永远不会降为零，导致内存泄漏</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><strong>解决循环引用的方法：</strong></p><ul><li><strong>使用 <code>std::weak_ptr</code> 打破循环引用</strong></li></ul><h4 id="2-3-std-weak-ptr"><a href="#2-3-std-weak-ptr" class="headerlink" title="2.3 std::weak_ptr"></a>2.3 std::weak_ptr</h4><p><code>std::weak_ptr:</code>用于解决循环引用和避免 <code>std::shared_ptr</code> 的引用计数增加导致的内存泄漏问题,通常用于与 <code>std::shared_ptr</code> 共同工作</p><p><code>std::weak_ptr</code> 不会增加对象的引用计数，因此它不会影响对象的生命周期。</p><p><strong>解决循环引用问题：当两个对象相互持有对方的 <code>std::shared_ptr</code> 时，其中一个或两个需要使用 <code>std::weak_ptr</code>，以避免形成循环引用，从而防止内存泄漏。</strong></p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;memory&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">ClassB</span>;</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">ClassA</span> &#123;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    std::shared_ptr&lt;ClassB&gt; bPtr;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">ClassB</span> &#123;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    std::weak_ptr&lt;ClassA&gt; aWeakPtr;  <span class="comment">// 使用 std::weak_ptr 避免循环引用</span></span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    <span class="comment">// 创建 shared_ptr1 和 shared_ptr2</span></span><br><span class="line">    std::shared_ptr&lt;ClassA&gt; aPtr = std::<span class="built_in">make_shared</span>&lt;ClassA&gt;();</span><br><span class="line">    std::shared_ptr&lt;ClassB&gt; bPtr = std::<span class="built_in">make_shared</span>&lt;ClassB&gt;();</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 将 shared_ptr1 和 shared_ptr2 关联起来</span></span><br><span class="line">    aPtr-&gt;bPtr = bPtr;</span><br><span class="line">    bPtr-&gt;aWeakPtr = aPtr;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// ...</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><strong>获取 <code>std::shared_ptr</code>：</strong>用 <code>std::weak_ptr</code> 的 <code>lock</code> 成员函数来获取一个指向共享对象的 <code>std::shared_ptr</code></p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">std::shared_ptr&lt;<span class="type">int</span>&gt; sharedPtr = std::<span class="built_in">make_shared</span>&lt;<span class="type">int</span>&gt;(<span class="number">42</span>);</span><br><span class="line"><span class="function">std::weak_ptr&lt;<span class="type">int</span>&gt; <span class="title">weakPtr</span><span class="params">(sharedPtr)</span></span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 获取 shared_ptr</span></span><br><span class="line">std::shared_ptr&lt;<span class="type">int</span>&gt; sharedPtrCopy = weakPtr.<span class="built_in">lock</span>();</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> (sharedPtrCopy) &#123;</span><br><span class="line">    <span class="comment">// 共享对象存在</span></span><br><span class="line">    <span class="comment">// 使用 sharedPtrCopy...</span></span><br><span class="line">&#125; <span class="keyword">else</span> &#123;</span><br><span class="line">    <span class="comment">// 共享对象已销毁</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><strong>判断对象是否存在：</strong> 可以使用 <code>expired</code> 成员函数检查 <code>std::weak_ptr</code> 引用的对象是否已经被销毁。</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">std::shared_ptr&lt;<span class="type">int</span>&gt; sharedPtr = std::<span class="built_in">make_shared</span>&lt;<span class="type">int</span>&gt;(<span class="number">42</span>);</span><br><span class="line"><span class="function">std::weak_ptr&lt;<span class="type">int</span>&gt; <span class="title">weakPtr</span><span class="params">(sharedPtr)</span></span>;</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> (!weakPtr.<span class="built_in">expired</span>()) &#123;</span><br><span class="line">    <span class="comment">// 共享对象存在</span></span><br><span class="line">    <span class="comment">// ...</span></span><br><span class="line">&#125; <span class="keyword">else</span> &#123;</span><br><span class="line">    <span class="comment">// 共享对象已销毁</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h4 id="2-4-std-unique-ptr"><a href="#2-4-std-unique-ptr" class="headerlink" title="2.4 std::unique_ptr"></a>2.4 std::unique_ptr</h4><p><strong><code>std::unique_ptr:</code></strong>与 <code>std::shared_ptr</code> 不同，<code>std::unique_ptr</code> 具有“独占”的所有权语义，即同一时刻只能有一个 <code>std::unique_ptr</code> 指向一个特定的对象。当 <code>std::unique_ptr</code> 被销毁或通过 <code>std::move</code> 转移所有权时，它所管理的对象将被销毁。</p><ul><li><strong>创建 <code>std::unique_ptr</code></strong></li></ul><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;memory&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    <span class="comment">// 使用 std::make_unique 创建 std::unique_ptr</span></span><br><span class="line">    std::unique_ptr&lt;<span class="type">int</span>&gt; uniquePtr1 = std::<span class="built_in">make_unique</span>&lt;<span class="type">int</span>&gt;(<span class="number">42</span>);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 使用构造函数创建 std::unique_ptr</span></span><br><span class="line">    <span class="function">std::unique_ptr&lt;<span class="type">int</span>&gt; <span class="title">uniquePtr2</span><span class="params">(<span class="keyword">new</span> <span class="type">int</span>(<span class="number">42</span>))</span></span>;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><ul><li><strong>移动所有权</strong></li></ul><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;memory&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    std::unique_ptr&lt;<span class="type">int</span>&gt; uniquePtr1 = std::<span class="built_in">make_unique</span>&lt;<span class="type">int</span>&gt;(<span class="number">42</span>);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 移动所有权</span></span><br><span class="line">    std::unique_ptr&lt;<span class="type">int</span>&gt; uniquePtr2 = std::<span class="built_in">move</span>(uniquePtr1);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><em>注：相对于 <code>std::shared_ptr</code>，<code>std::unique_ptr</code> 是一种更轻量级的智能指针，因为它不需要维护引用计数。</em></p>]]></content>
    
    
      
      
    <summary type="html">&lt;h3 id=&quot;1-RAll&quot;&gt;&lt;a href=&quot;#1-RAll&quot; class=&quot;headerlink&quot; title=&quot;1.RAll&quot;&gt;&lt;/a&gt;1.RAll&lt;/h3&gt;&lt;p&gt;RAII：用于有效地管理资源的获取和释放。&lt;/p&gt;
&lt;p&gt;基本思想：资源的获取应当在对象的构造函数中进行，而</summary>
      
    
    
    
    <category term="C++" scheme="https://wangak.cc/categories/C/"/>
    
    
    <category term="C++" scheme="https://wangak.cc/tags/C/"/>
    
  </entry>
  
  <entry>
    <title>C++零碎知识</title>
    <link href="https://wangak.cc/posts/8948f962.html"/>
    <id>https://wangak.cc/posts/8948f962.html</id>
    <published>2023-12-26T16:00:00.000Z</published>
    <updated>2023-12-27T00:00:00.000Z</updated>
    
    <content type="html"><![CDATA[<h3 id="1-静态绑定与动态绑定"><a href="#1-静态绑定与动态绑定" class="headerlink" title="1.静态绑定与动态绑定"></a>1.静态绑定与动态绑定</h3><p>“绑定”指的是将一个名字（例如变量名或函数名）与一个特定的实体（变量或函数）关联起来的过程。</p><p><strong>静态绑定：</strong>在<strong>编译阶段</strong>确定函数调用关系，编译器根据变量的<strong>声明类型</strong>或函数的定义位置来选择调用哪个函数</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Base</span> &#123;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="function"><span class="type">void</span> <span class="title">foo</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Derived</span> : <span class="keyword">public</span> Base &#123;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="function"><span class="type">void</span> <span class="title">foo</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br><span class="line"><span class="comment">//调用的函数由变量的声明类型所决定</span></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    Base obj;</span><br><span class="line">    obj.<span class="built_in">foo</span>();  <span class="comment">// 静态绑定，调用Base类的foo()函数</span></span><br><span class="line"></span><br><span class="line">    Derived obj2;</span><br><span class="line">    obj2.<span class="built_in">foo</span>(); <span class="comment">// 静态绑定，调用Derived类的foo()函数</span></span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><strong>动态绑定：</strong>在<strong>运行时</strong>确定函数的调用关系，根据对象的<strong>实际类型</strong>调用相应的函数。</p><p><em>注：动态绑定适用于虚函数，通过在基类中声明函数为虚函数，可以在派生类中重写该函数，并在运行时根据<strong>对象的实际类型</strong>调用相应的函数。</em></p><p>动态绑定的条件：</p><ul><li>必须通过指针来调用</li><li>该指针是向上转型的</li><li>调用的是虚函数</li></ul><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Base</span> &#123;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="function"><span class="keyword">virtual</span> <span class="type">void</span> <span class="title">foo</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Derived</span> : <span class="keyword">public</span> Base &#123;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="function"><span class="type">void</span> <span class="title">foo</span><span class="params">()</span> <span class="keyword">override</span> </span>&#123;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    Base* obj = <span class="keyword">new</span> <span class="built_in">Derived</span>();</span><br><span class="line">    obj-&gt;<span class="built_in">foo</span>();  <span class="comment">// 动态绑定，调用Derived类的foo()函数</span></span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><em>注：动态绑定与静态绑定相比具有更高的灵活性，但速度也会较慢</em></p><h3 id="2-两种转型"><a href="#2-两种转型" class="headerlink" title="2.两种转型"></a>2.两种转型</h3><h4 id="2-1-向上转型"><a href="#2-1-向上转型" class="headerlink" title="2.1 向上转型"></a>2.1 向上转型</h4><p><strong>向上转型：</strong>派生类向基类转换的过程，是隐式的，不需要显式的类型转换。</p><p><em>注：在向上转型的过程中没有发生对象的拷贝，而是将派生类对象的地址赋给基类指针，基类指针可以访问基类中定义的成员，但不能访问派生类特有的成员。向上转型体现了<strong>指针的多态性</strong>，可以用来实现<strong>动态绑定</strong>。</em></p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Base</span> &#123;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="function"><span class="type">void</span> <span class="title">baseFunction</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Derived</span> : <span class="keyword">public</span> Base &#123;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="function"><span class="type">void</span> <span class="title">derivedFunction</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    Derived derivedObj;</span><br><span class="line">    </span><br><span class="line">    <span class="comment">// 向上转型，将Derived对象的地址赋给Base指针</span></span><br><span class="line">    Base* basePtr = &amp;derivedObj;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 可以通过基类指针调用基类的成员函数</span></span><br><span class="line">    basePtr-&gt;<span class="built_in">baseFunction</span>();</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>向上转型比较安全，可以由编译器自动完成，不会有数据的丢失，在编译期间转换，如果转换失败会抛出编译错误，所以可以及时地发现错误。</p><p><strong>安全的原因：</strong></p><ul><li>基类指针只能访问基类成员，降低了误用的风险</li><li>向上转型只是将派生类对象的地址赋给基类指针，而不会改变对象本身的内存结构</li><li>在向上转型中，编译器能够静态地检查类型兼容性。如果存在不兼容的类型关系，编译时会发出错误，避免了一些在运行时才能检测到的问题。</li></ul><h4 id="2-2-向下转型"><a href="#2-2-向下转型" class="headerlink" title="2.2 向下转型"></a>2.2 向下转型</h4><p><strong>向下转型：</strong>从基类向派生类转换的过程，是显式的，需要使用类型转换操作符。</p><p><strong>静态转型：</strong><code>static_cast</code></p><p>静态转型是在编译时进行的转型，不提供运行时类型检查。</p><p><em>注：如果静态转型过程中出现错误，可能会导致未定义行为（如数据损坏、程序错误等）</em></p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Base</span> &#123;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="function"><span class="keyword">virtual</span> <span class="type">void</span> <span class="title">foo</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        std::cout &lt;&lt; <span class="string">&quot;Base class&quot;</span> &lt;&lt; std::endl;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Derived</span> : <span class="keyword">public</span> Base &#123;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="function"><span class="type">void</span> <span class="title">bar</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        std::cout &lt;&lt; <span class="string">&quot;Derived class&quot;</span> &lt;&lt; std::endl;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    Base* basePtr = <span class="keyword">new</span> <span class="built_in">Derived</span>();</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 使用 static_cast 进行向下转型</span></span><br><span class="line">    Derived* derivedPtr = <span class="built_in">static_cast</span>&lt;Derived*&gt;(basePtr);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 使用派生类指针调用派生类成员函数</span></span><br><span class="line">    derivedPtr-&gt;<span class="built_in">bar</span>();</span><br><span class="line"></span><br><span class="line">    <span class="keyword">delete</span> basePtr;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><strong>动态转型：</strong><code>dynamic_cast</code></p><p>动态转型是在运行时进行的转型，提供了类型安全检查。</p><p><em>注：动态转型只能用于含有虚函数的类层次结构，即只能用于多态类型之间的转换。多态类型是指至少有一个虚函数的类或结构体。</em></p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Base</span> &#123;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="function"><span class="keyword">virtual</span> <span class="type">void</span> <span class="title">foo</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        std::cout &lt;&lt; <span class="string">&quot;Base class&quot;</span> &lt;&lt; std::endl;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Derived</span> : <span class="keyword">public</span> Base &#123;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="function"><span class="type">void</span> <span class="title">bar</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        std::cout &lt;&lt; <span class="string">&quot;Derived class&quot;</span> &lt;&lt; std::endl;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    Base* basePtr = <span class="keyword">new</span> <span class="built_in">Derived</span>();</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 使用 dynamic_cast 进行向下转型</span></span><br><span class="line">    Derived* derivedPtr = <span class="built_in">dynamic_cast</span>&lt;Derived*&gt;(basePtr);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (derivedPtr) &#123;</span><br><span class="line">        <span class="comment">// 转型成功，使用派生类指针调用派生类成员函数</span></span><br><span class="line">        derivedPtr-&gt;<span class="built_in">bar</span>();</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">        <span class="comment">// 转型失败，可能是由于对象不是Derived类型</span></span><br><span class="line">        std::cout &lt;&lt; <span class="string">&quot;Dynamic casting failed.&quot;</span> &lt;&lt; std::endl;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">delete</span> basePtr;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><em>注：如果转型不安全，<code>dynamic_cast</code> 返回空指针（或引用），而不是导致未定义的行为。</em></p><h3 id="3-左值引用与右值引用"><a href="#3-左值引用与右值引用" class="headerlink" title="3.左值引用与右值引用"></a>3.左值引用与右值引用</h3><h4 id="3-1-左值引用"><a href="#3-1-左值引用" class="headerlink" title="3.1 左值引用"></a>3.1 左值引用</h4><p><strong>左值引用:</strong>给变量取别名，可以减少一层拷贝</p><ul><li><p><strong>修改引用对象的值:</strong>左值引用允许对左值进行引用，可以修改其值。</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">int</span> x = <span class="number">42</span>;</span><br><span class="line"><span class="type">int</span>&amp; lvalueRef = x;</span><br><span class="line">lvalueRef = <span class="number">10</span>;  <span class="comment">// 修改 x 的值</span></span><br></pre></td></tr></table></figure></li><li><p><strong>传递引用参数：</strong>使函数直接操作传入的参数，而不是通过复制产生新的对象,避免不必要的对象复制。</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="type">void</span> <span class="title">modifyValue</span><span class="params">(<span class="type">int</span>&amp; value)</span> </span>&#123;</span><br><span class="line">    value *= <span class="number">2</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    <span class="type">int</span> x = <span class="number">5</span>;</span><br><span class="line">    <span class="built_in">modifyValue</span>(x);  <span class="comment">// 传递 x 的引用，函数可以修改 x 的值</span></span><br><span class="line">    <span class="comment">// 现在 x 的值为 10</span></span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li><li><p><strong>函数返回引用：</strong>可以返回左值引用，避免创建临时对象。</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">int</span> x = <span class="number">42</span>;</span><br><span class="line"><span class="function"><span class="type">int</span>&amp; <span class="title">getReference</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    <span class="keyword">return</span> x;</span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    <span class="type">int</span>&amp; ref = <span class="built_in">getReference</span>();</span><br><span class="line">    ref = <span class="number">10</span>;  <span class="comment">// 修改 x 的值</span></span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li></ul><h4 id="3-2-右值引用"><a href="#3-2-右值引用" class="headerlink" title="3.2 右值引用"></a>3.2 右值引用</h4><p><strong>右值：</strong>一个表达式，通常是一些临时对象、字面常量、表达式的计算结果等。</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="type">int</span> <span class="title">getResult</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    <span class="keyword">return</span> <span class="number">42</span>;  <span class="comment">// 函数返回一个右值</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    <span class="type">int</span> x = <span class="number">10</span>;  <span class="comment">// x 是左值</span></span><br><span class="line">    <span class="type">int</span> y = x + <span class="number">5</span>;  <span class="comment">// x + 5 是一个右值</span></span><br><span class="line"></span><br><span class="line">    <span class="type">int</span>&amp;&amp; rvalueRef = <span class="built_in">getResult</span>();  <span class="comment">// getResult() 返回的是右值</span></span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><em>注：左值可以取地址，右值不能被取地址</em></p><p><strong>左值引用只能引用左值，经过const修饰的左值引用，既可以引用左值，也可以引用右值：</strong></p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">const</span> <span class="type">int</span>&amp; constLvalueRef = <span class="number">42</span>;  <span class="comment">// 常量左值引用引用右值</span></span><br></pre></td></tr></table></figure><p><em>注：右值是不能被修改的值，所以左值引用被const修饰后才能引用右值</em></p><p><strong>右值引用可以引用move以后的左值:</strong>move相当于一个强制转换，将左值转换为右值</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;utility&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    <span class="type">int</span> x = <span class="number">42</span>;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 使用 std::move 将左值 x 转换为右值引用</span></span><br><span class="line">    <span class="type">int</span>&amp;&amp; rvalueRef = std::<span class="built_in">move</span>(x);</span><br><span class="line"></span><br><span class="line">    std::cout &lt;&lt; <span class="string">&quot;x after std::move: &quot;</span> &lt;&lt; x &lt;&lt; std::endl;  <span class="comment">// 输出 x 的值，已经被 std::move 转换过</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><strong>右值移动：</strong></p><ul><li><p><strong>移动语义：</strong>旨在提高对对象的资源管理效率，允许在对象资源的<strong>所有权转移（资源窃取）</strong>时，避免昂贵的深拷贝操作，而采用更经济高效的移动操作。</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">MyString</span> &#123;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="comment">// 移动构造函数</span></span><br><span class="line">    <span class="built_in">MyString</span>(MyString&amp;&amp; other) : <span class="built_in">data</span>(other.data), <span class="built_in">size</span>(other.size) &#123;</span><br><span class="line">        other.data = <span class="literal">nullptr</span>;</span><br><span class="line">        other.size = <span class="number">0</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">private</span>:</span><br><span class="line">    <span class="type">char</span>* data;</span><br><span class="line">    <span class="type">size_t</span> size;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    MyString source = <span class="string">&quot;Hello&quot;</span>;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 移动构造函数被调用，资源的所有权从 source 转移到 destination</span></span><br><span class="line">    MyString destination = std::<span class="built_in">move</span>(source);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 此时 source 不再拥有资源</span></span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li><li><p><strong>完美转发：</strong>实现一种通用的、保留原参数特性的参数传递机制（即接收左值作为参数，也可以接收右值作为参数）。</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;utility&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">MyClass</span> &#123;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="keyword">template</span> &lt;<span class="keyword">typename</span> T&gt;</span><br><span class="line">    <span class="built_in">MyClass</span>(T&amp;&amp; arg) : <span class="built_in">data</span>(std::forward&lt;T&gt;(arg)) &#123;</span><br><span class="line">        <span class="comment">// 构造函数中的完美转发</span></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">private</span>:</span><br><span class="line">    <span class="type">int</span> data;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    <span class="type">int</span> x = <span class="number">42</span>;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 通过完美转发调用构造函数</span></span><br><span class="line">    <span class="function">MyClass <span class="title">obj1</span><span class="params">(x)</span></span>;     <span class="comment">// 左值</span></span><br><span class="line">    <span class="function">MyClass <span class="title">obj2</span><span class="params">(<span class="number">10</span>)</span></span>;    <span class="comment">// 右值</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><em>注：<code>std::forward</code>的作用是保留原始参数的左值或右值性质，以及 const 修饰符，实现一种通用的参数传递机制，其位于头文件 <code>&lt;utility&gt;</code> 中，并定义在命名空间 <code>std</code> 中</em></p></li></ul><h3 id="4-模板（泛化、全特化、偏特化）"><a href="#4-模板（泛化、全特化、偏特化）" class="headerlink" title="4.模板（泛化、全特化、偏特化）"></a>4.模板（泛化、全特化、偏特化）</h3><h4 id="4-1-模板泛化"><a href="#4-1-模板泛化" class="headerlink" title="4.1 模板泛化"></a>4.1 模板泛化</h4><p>模板泛化是不关心具体的类型，而是提供了通用的实现。</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 泛化的类模板</span></span><br><span class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> T&gt;</span><br><span class="line"><span class="keyword">class</span> <span class="title class_">MyClass</span> &#123;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="comment">// 泛化的成员函数</span></span><br><span class="line">    <span class="function"><span class="type">void</span> <span class="title">process</span><span class="params">(T data)</span> </span>&#123;</span><br><span class="line">        std::cout &lt;&lt; <span class="string">&quot;Generic process: &quot;</span> &lt;&lt; data &lt;&lt; std::endl;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><h4 id="4-2-全特化"><a href="#4-2-全特化" class="headerlink" title="4.2 全特化"></a>4.2 全特化</h4><p>成员函数的全特化</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 成员函数的特化</span></span><br><span class="line"><span class="keyword">template</span> &lt;&gt;</span><br><span class="line"><span class="type">void</span> MyClass&lt;<span class="type">int</span>&gt;::<span class="built_in">process</span>(<span class="type">int</span> data) &#123;</span><br><span class="line">    std::cout &lt;&lt; <span class="string">&quot;Specialized process for int: &quot;</span> &lt;&lt; data * <span class="number">2</span> &lt;&lt; std::endl;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>类模板全特化：</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 模板的特化</span></span><br><span class="line"><span class="keyword">template</span> &lt;&gt;</span><br><span class="line"><span class="keyword">class</span> <span class="title class_">MyClass</span>&lt;<span class="type">int</span>&gt; &#123;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="function"><span class="type">void</span> <span class="title">process</span><span class="params">(<span class="type">int</span> data)</span> </span>&#123;</span><br><span class="line">        std::cout &lt;&lt; <span class="string">&quot;Specialized implementation for int&quot;</span> &lt;&lt; std::endl;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><h4 id="4-3-偏特化"><a href="#4-3-偏特化" class="headerlink" title="4.3 偏特化"></a>4.3 偏特化</h4><p>模板偏特化是指在泛化的模板基础上，对其中的某一部分进行特化。</p><p><strong>模板参数数量的偏特化：</strong>特化部分参数，还存在一部分参数使用通用的模板定义。</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 泛化的类模板，有两个模板参数</span></span><br><span class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> T, <span class="keyword">typename</span> U&gt;</span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Pair</span> &#123;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    T first;</span><br><span class="line">    U second;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 模板参数数量的偏特化，对第一个模板参数进行特化</span></span><br><span class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> U&gt;</span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Pair</span>&lt;<span class="type">int</span>, U&gt; &#123;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="type">int</span> first;</span><br><span class="line">    U second;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><p><strong>模板参数范围的偏特化：</strong>对模板的参数范围进行缩小</p><ul><li><strong>const 特化:</strong></li></ul><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 泛化的类模板</span></span><br><span class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> T&gt;</span><br><span class="line"><span class="keyword">class</span> <span class="title class_">MyClass</span> &#123;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="function"><span class="type">void</span> <span class="title">process</span><span class="params">(T data)</span> </span>&#123;</span><br><span class="line">        std::cout &lt;&lt; <span class="string">&quot;Generic process: &quot;</span> &lt;&lt; data &lt;&lt; std::endl;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="comment">// const 特化</span></span><br><span class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> T&gt;</span><br><span class="line"><span class="keyword">class</span> <span class="title class_">MyClass</span>&lt;<span class="type">const</span> T&gt; &#123;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="function"><span class="type">void</span> <span class="title">process</span><span class="params">(<span class="type">const</span> T data)</span> </span>&#123;</span><br><span class="line">        std::cout &lt;&lt; <span class="string">&quot;Specialized process for const type: &quot;</span> &lt;&lt; data &lt;&lt; std::endl;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><ul><li><strong>指针特化：</strong></li></ul><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 泛化的类模板</span></span><br><span class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> T&gt;</span><br><span class="line"><span class="keyword">class</span> <span class="title class_">MyContainer</span> &#123;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="function"><span class="type">void</span> <span class="title">setValue</span><span class="params">(T value)</span> </span>&#123;</span><br><span class="line">        std::cout &lt;&lt; <span class="string">&quot;Generic setValue: &quot;</span> &lt;&lt; value &lt;&lt; std::endl;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 指针特化</span></span><br><span class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> T&gt;</span><br><span class="line"><span class="keyword">class</span> <span class="title class_">MyContainer</span>&lt;T*&gt; &#123;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="function"><span class="type">void</span> <span class="title">setValue</span><span class="params">(T* ptr)</span> </span>&#123;</span><br><span class="line">        std::cout &lt;&lt; <span class="string">&quot;Specialized setValue for pointers: &quot;</span> &lt;&lt; *ptr &lt;&lt; std::endl;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><ul><li><strong>左值引用特化:</strong></li></ul><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 泛化的类模板</span></span><br><span class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> T&gt;</span><br><span class="line"><span class="keyword">class</span> <span class="title class_">MyReference</span> &#123;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="function"><span class="type">void</span> <span class="title">printValue</span><span class="params">(T value)</span> </span>&#123;</span><br><span class="line">        std::cout &lt;&lt; <span class="string">&quot;Generic printValue: &quot;</span> &lt;&lt; value &lt;&lt; std::endl;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 左值引用特化</span></span><br><span class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> T&gt;</span><br><span class="line"><span class="keyword">class</span> <span class="title class_">MyReference</span>&lt;T&amp;&gt; &#123;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="function"><span class="type">void</span> <span class="title">printValue</span><span class="params">(T&amp; ref)</span> </span>&#123;</span><br><span class="line">        std::cout &lt;&lt; <span class="string">&quot;Specialized printValue for lvalue references: &quot;</span> &lt;&lt; ref &lt;&lt; std::endl;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><ul><li><strong>右值引用特化:</strong></li></ul><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 泛化的类模板</span></span><br><span class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> T&gt;</span><br><span class="line"><span class="keyword">class</span> <span class="title class_">MyRValueReference</span> &#123;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="function"><span class="type">void</span> <span class="title">printValue</span><span class="params">(T value)</span> </span>&#123;</span><br><span class="line">        std::cout &lt;&lt; <span class="string">&quot;Generic printValue: &quot;</span> &lt;&lt; value &lt;&lt; std::endl;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 右值引用特化</span></span><br><span class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> T&gt;</span><br><span class="line"><span class="keyword">class</span> <span class="title class_">MyRValueReference</span>&lt;T&amp;&amp;&gt; &#123;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="function"><span class="type">void</span> <span class="title">printValue</span><span class="params">(T&amp;&amp; rvalue)</span> </span>&#123;</span><br><span class="line">        std::cout &lt;&lt; <span class="string">&quot;Specialized printValue for rvalue references: &quot;</span> &lt;&lt; rvalue &lt;&lt; std::endl;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><p><em>注：函数模板是不能偏特化的，只有类模板可以进行偏特化。函数模板可以不显式指定类型。</em></p>]]></content>
    
    
      
      
    <summary type="html">&lt;h3 id=&quot;1-静态绑定与动态绑定&quot;&gt;&lt;a href=&quot;#1-静态绑定与动态绑定&quot; class=&quot;headerlink&quot; title=&quot;1.静态绑定与动态绑定&quot;&gt;&lt;/a&gt;1.静态绑定与动态绑定&lt;/h3&gt;&lt;p&gt;“绑定”指的是将一个名字（例如变量名或函数名）与一个特定的实体（变量</summary>
      
    
    
    
    <category term="C++" scheme="https://wangak.cc/categories/C/"/>
    
    
    <category term="C++" scheme="https://wangak.cc/tags/C/"/>
    
  </entry>
  
  <entry>
    <title>SpectFormer</title>
    <link href="https://wangak.cc/posts/aa01fe8.html"/>
    <id>https://wangak.cc/posts/aa01fe8.html</id>
    <published>2023-12-26T16:00:00.000Z</published>
    <updated>2024-03-19T04:13:10.998Z</updated>
    
    <content type="html"><![CDATA[<h2 id="SpectFormer"><a href="#SpectFormer" class="headerlink" title="SpectFormer"></a>SpectFormer</h2><p><strong>论文：《SpectFormer: Frequency and Attention is what you need in a Vision Transformer》（arxiv 2023）</strong></p><p>频域层和多头注意力层结合起来，可以使Transformer能够捕捉到适当的特征表示，提升模型的特征建模能力，从而提升模型的性能。</p><p>频域层由一个快速傅里叶变换层（FFT)和一个逆傅里叶层（IFFT)构成</p><p><em>注：FFT和IFFT的操作，也可使用小波变换和逆小波变换来实现</em></p><p><img src="https://typoraimg.wangak.cc/2023/img/202312272028245.png" alt="image-20231224161116413" style="zoom:67%;"></p><p>FFT把图像信息转到频域空间，然后可以对频域信号进行操作，使用具有可学习权重参数<script type="math/tex">W_c</script>的门控层来确定每个频率分量的权重，以便适当地捕获图像的线条和边缘，如可以去除低频部分，保留高频部分，用于突出图像的主要特征，然后通过IFFT做一个逆变换，把频域图还原到时域中。频域层之后用层归一化和多层感知器 (MLP) 块用于通道混合。</p><p>SpecFormer考虑了局部特征，这有助于捕获局部频率，以及更深层的全局特征，这有助于捕获长期依赖关系。</p><p>与GFNnet对比：SpecFormer可以更加清晰地捕获局部特征，如图像的线条和边缘</p><p><em>注：GFNet是完全使用频域层来进行建模</em></p><p><img src="https://typoraimg.wangak.cc/2023/img/202312272028521.png" alt="image-20231224161421871"></p><p><em>频域层和注意力层结合，通过频域层捕获局部信息，注意力层捕获全局信息，同时可以灵活调整频域层和注意力层各自的层数</em></p><p><strong>混合建模形式的实验验证：</strong></p><p><img src="https://typoraimg.wangak.cc/2023/img/202312272028064.png" alt="image-20231224163639771" style="zoom:67%;"></p><p><em>注：Inverse SpecFormer是将频域层和注意力层对调，即注意力层在前，频域层在后</em></p>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;SpectFormer&quot;&gt;&lt;a href=&quot;#SpectFormer&quot; class=&quot;headerlink&quot; title=&quot;SpectFormer&quot;&gt;&lt;/a&gt;SpectFormer&lt;/h2&gt;&lt;p&gt;&lt;strong&gt;论文：《SpectFormer: Frequency</summary>
      
    
    
    
    <category term="计算机视觉" scheme="https://wangak.cc/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/"/>
    
    
    <category term="计算机视觉" scheme="https://wangak.cc/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/"/>
    
    <category term="图像分割" scheme="https://wangak.cc/tags/%E5%9B%BE%E5%83%8F%E5%88%86%E5%89%B2/"/>
    
  </entry>
  
</feed>
