<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>丹青两幻</title>
  
  <subtitle>欢迎~</subtitle>
  <link href="https://wangak.cc/atom.xml" rel="self"/>
  
  <link href="https://wangak.cc/"/>
  <updated>2024-10-12T07:51:32.065Z</updated>
  <id>https://wangak.cc/</id>
  
  <author>
    <name>丹青两幻</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>ASSNet:Adaptive Semantic Segmentation Network for Microtumors and Multi-Organ Segmentation</title>
    <link href="https://wangak.cc/posts/425a95d.html"/>
    <id>https://wangak.cc/posts/425a95d.html</id>
    <published>2024-10-11T16:00:00.000Z</published>
    <updated>2024-10-12T07:51:32.065Z</updated>
    
    <content type="html"><![CDATA[<h2 id="ASSNet-Adaptive-Semantic-Segmentation-Network-for-Microtumors-and-Multi-Organ-Segmentation"><a href="#ASSNet-Adaptive-Semantic-Segmentation-Network-for-Microtumors-and-Multi-Organ-Segmentation" class="headerlink" title="ASSNet: Adaptive Semantic Segmentation Network for Microtumors and Multi-Organ Segmentation"></a>ASSNet: Adaptive Semantic Segmentation Network for Microtumors and Multi-Organ Segmentation</h2><p><strong>论文：</strong></p><p><strong>《ASSNet: Adaptive Semantic Segmentation Network for Microtumors and Multi-Organ Segmentation》（arXiv 2024）</strong></p><p><strong>主要贡献：</strong></p><ul><li>设计了一种结合ResUnet和Swin-transformer优点的混合模型ASSNet，该模型具有窗口注意力、空间注意力、U型架构和残差连接，以实现高效分割。</li><li>提出了一种自适应特征融合(AFF)解码器，该解码器能够最大化利用多尺度特征，同时捕捉远程依赖并细化目标边界。</li></ul><p>网络整体结构：</p><p><img src="https://typoraimg.wangak.cc/2023/img/202410121507439.png" alt="image-20241008195242686" style="zoom:50%;"></p><p><strong>编码器：</strong></p><p><img src="https://typoraimg.wangak.cc/2023/img/202410121507617.png" alt="image-20241012125942275" style="zoom: 50%;"></p><ul><li>通过在不同分辨率上应用窗口注意力，编码器能够捕捉到图像中的多尺度信息</li><li>引入了一个增强的多层感知器，以在特征提取过程中显式地建模长距离依赖关系</li><li>EFFN结合深度卷积和像素卷积来增强局部特征的提取能力</li></ul><p><strong>Adaptive Feature Fusion (AFF) Decoder:</strong></p><p>该解码器包含三个关键组件:长距离依赖(LRD)块、多尺度特征融合(MFF)块和自适应语义中心(ASC)块。</p><p>Adaptive Feature Fusion (AFF) Decoder是为了解决多尺度特征融合和长距离依赖建模问题而设计的。</p><p><img src="https://typoraimg.wangak.cc/2023/img/202410121507151.png" alt="image-20241012130531795" style="zoom: 50%;"></p><ul><li><p><strong>长距离依赖块(LRD)：</strong>通过一系列的卷积操作建模远距离像素间的关联。</p></li><li><p><strong>多尺度特征融合块(MFF):</strong>接收来自编码器不同分辨率层次的特征图,通过跳跃连接（skip connections）将这些多尺度特征与当前解码层的特征图进行融合。然后通过膨胀卷积扩张卷积处理这些融合的特征，提取出每个尺度下的关键信息。</p></li><li><p><strong>自适应语义中心块(ASC):</strong>类似于传统边缘检测方法,作者使用一种自适应平均池化（Adaptive Average Pooling）操作，结合全连接层对通道特征进行增强。</p></li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;ASSNet-Adaptive-Semantic-Segmentation-Network-for-Microtumors-and-Multi-Organ-Segmentation&quot;&gt;&lt;a href=&quot;#ASSNet-Adaptive-Semantic-Segme</summary>
      
    
    
    
    <category term="计算机视觉" scheme="https://wangak.cc/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/"/>
    
    
    <category term="计算机视觉" scheme="https://wangak.cc/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/"/>
    
    <category term="图像分割" scheme="https://wangak.cc/tags/%E5%9B%BE%E5%83%8F%E5%88%86%E5%89%B2/"/>
    
  </entry>
  
  <entry>
    <title>RotCAtt-TransUNet++:Novel Deep Neural Network for Sophisticated Cardiac Segmentation</title>
    <link href="https://wangak.cc/posts/2d5c3529.html"/>
    <id>https://wangak.cc/posts/2d5c3529.html</id>
    <published>2024-09-27T16:00:00.000Z</published>
    <updated>2024-10-12T07:51:21.498Z</updated>
    
    <content type="html"><![CDATA[<h2 id="RotCAtt-TransUNet-Novel-Deep-Neural-Network-for-Sophisticated-Cardiac-Segmentation"><a href="#RotCAtt-TransUNet-Novel-Deep-Neural-Network-for-Sophisticated-Cardiac-Segmentation" class="headerlink" title="RotCAtt-TransUNet++: Novel Deep Neural Network for Sophisticated Cardiac Segmentation"></a>RotCAtt-TransUNet++: Novel Deep Neural Network for Sophisticated Cardiac Segmentation</h2><p><strong>论文：《RotCAtt-TransUNet++: Novel Deep Neural Network for Sophisticated Cardiac Segmentation》（arXiv 2024）</strong></p><ul><li><p>提出了一种新颖的旋转注意力机制（Rotatory Attention），用于捕捉体数据中相邻切片的信息。</p></li><li><p>编码器部分引入了UNet++的嵌套跳跃连接和密集的多尺度下采样，确保了在不同尺度上保留关键特征</p></li><li>设计了一个通道交叉注意力机制，通过引导Transformer特征的通道和信息过滤，解决了编码器与解码器特征间语义模糊的问题。</li></ul><p>Transformer层有助于捕获片内交互，而旋转注意机制处理片间连接。</p><p><img src="https://typoraimg.wangak.cc/2023/img/202409281659093.png" alt="image-20240927152207198" style="zoom: 50%;"></p><ul><li><p>在跳跃连接处对不同深度和不同分辨率的特征图进行了融合</p></li><li><p>对前三个特征映射X1、X2、X3以不同的patch大小进行线性嵌入（<script type="math/tex">Z_i</script>),然后，通过transformer块捕捉patch之间的交互，并通过旋转注意力机制聚合相邻切片的信息</p></li></ul><h4 id="Linear-Embedding-与-Positional-Embedding"><a href="#Linear-Embedding-与-Positional-Embedding" class="headerlink" title="Linear Embedding 与 Positional Embedding"></a><strong>Linear Embedding 与 Positional Embedding</strong></h4><p>对于不同分辨率的特征图采用不同大小的patch（这样做可以保持特征图的代表性，不同分辨率的特征图包含不同尺度的信息），较小的patch有助于提取图像的局部细节特征，而较大的patch则有助于提取全局或上下文特征，然后对使用卷积操作映射到一个统一的维度。</p><h4 id="Rotatory-Attention-Block"><a href="#Rotatory-Attention-Block" class="headerlink" title="Rotatory Attention Block"></a><strong>Rotatory Attention Block</strong></h4><p>作者使用旋转注意力机制来提取切片间的特征</p><p><img src="https://typoraimg.wangak.cc/2023/img/202409281659406.png" alt="image-20240927155203358" style="zoom: 50%;"></p><p>该机制来源于自然语言处理中的左-中-右分离神经网络，用于捕捉句子中相邻单词之间的上下文关系。（相邻的元素对理解的中心有重要贡献）</p><p><strong>输入：</strong>旋转注意力机制将批量大小视为多个连续切片的集合，并选择性地处理三个连续的切片—左切片（前一个切片）、目标切片（当前切片）和右切片（下一个切片）。     </p><p><strong>Left:</strong></p><ul><li>对左切片使用<script type="math/tex">W_k</script>和<script type="math/tex">W_v</script>来得到<script type="math/tex">K_l</script>和<script type="math/tex">V_l</script>​</li><li>使用对目标切片的平均池化的结果<script type="math/tex">r^t</script>作为查询，通过与<script type="math/tex">K_l</script>和<script type="math/tex">V_l</script>进行注意力的计算，得到左上下文融合后的特征</li></ul><p><strong>Right:</strong>同Left</p><p><strong>目标上下文交互：</strong>通过<strong>单注意力（SA）</strong>,将左上下文和右上下文整合到目标切片之中</p><p>最后，通过通道维度拼接和取平均的操作，得到最后的输出特征。</p><h4 id="Channel-wise-Attention-Gate-for-Feature-Fusion"><a href="#Channel-wise-Attention-Gate-for-Feature-Fusion" class="headerlink" title="Channel-wise Attention Gate for Feature Fusion"></a><strong>Channel-wise Attention Gate for Feature Fusion</strong></h4><ul><li><p>对输入特征图进行全局平均池化，生成一个描述各通道重要性的向量，通过线性层对该向量进行变换，生成一个注意力掩码。</p></li><li><p>将生成的注意力掩码与解码器的特征图进行逐元素相乘，从而得到加权后的特征图。</p></li><li>加权后的特征图与解码器的原始特征图进行连接，以增强解码过程中的信息流动，进一步提升分割精度。</li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;RotCAtt-TransUNet-Novel-Deep-Neural-Network-for-Sophisticated-Cardiac-Segmentation&quot;&gt;&lt;a href=&quot;#RotCAtt-TransUNet-Novel-Deep-Neural-Ne</summary>
      
    
    
    
    <category term="计算机视觉" scheme="https://wangak.cc/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/"/>
    
    
    <category term="计算机视觉" scheme="https://wangak.cc/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/"/>
    
    <category term="图像分割" scheme="https://wangak.cc/tags/%E5%9B%BE%E5%83%8F%E5%88%86%E5%89%B2/"/>
    
  </entry>
  
  <entry>
    <title>MobileUNETR:A Lightweight End-To-End Hybrid Vision Transformer For Efficient Medical Image Segmentation</title>
    <link href="https://wangak.cc/posts/c34814e1.html"/>
    <id>https://wangak.cc/posts/c34814e1.html</id>
    <published>2024-09-18T16:00:00.000Z</published>
    <updated>2024-09-19T02:12:31.180Z</updated>
    
    <content type="html"><![CDATA[<h3 id="MobileUNETR-A-Lightweight-End-To-End-Hybrid-Vision-Transformer-For-Efficient-Medical-Image-Segmentation"><a href="#MobileUNETR-A-Lightweight-End-To-End-Hybrid-Vision-Transformer-For-Efficient-Medical-Image-Segmentation" class="headerlink" title="MobileUNETR: A Lightweight End-To-End Hybrid Vision Transformer For Efficient Medical Image Segmentation"></a>MobileUNETR: A Lightweight End-To-End Hybrid Vision Transformer For Efficient Medical Image Segmentation</h3><p><strong>论文：《MobileUNETR: A Lightweight End-To-End Hybrid Vision Transformer For Efficient Medical Image Segmentation》（ECCV 2024）</strong></p><p><img src="https://typoraimg.wangak.cc/2023/img/202409191008537.png" alt="image-20240918195745497"></p><p><strong>Hybrid Encoder Block:</strong>利用深度可分离卷积将特征投影到高维，再利用MobileViT block捕获局部和全局信息</p><p><strong>Hybrid Decoder Block:</strong>首先，使用转置卷积进行上采样；然后，将得到的特征与跳跃连接的特征进行拼接；最后，使用MobileViT block得到改进后的分割结果</p><h4 id="MobileViT-block"><a href="#MobileViT-block" class="headerlink" title="MobileViT block"></a>MobileViT block</h4><p><img src="https://typoraimg.wangak.cc/2023/img/202409191008262.png" alt="image-20240918200915992" style="zoom:67%;"></p><ul><li><strong>Local representations:</strong>先使用n×n卷积进行特征提取，再使用1×1卷积调整通道数，获取局部特征</li><li><strong>global representations:</strong>通过Unfold和Fold操作实现，对相同颜色的小色块会进行Attention，以此来减小Attention计算的复杂度，来获取全局特征</li><li><strong>Fusion:</strong>通过与原始特征图的拼接，来实现融合</li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;h3 id=&quot;MobileUNETR-A-Lightweight-End-To-End-Hybrid-Vision-Transformer-For-Efficient-Medical-Image-Segmentation&quot;&gt;&lt;a href=&quot;#MobileUNETR-A-Lig</summary>
      
    
    
    
    <category term="计算机视觉" scheme="https://wangak.cc/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/"/>
    
    
    <category term="计算机视觉" scheme="https://wangak.cc/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/"/>
    
    <category term="图像分割" scheme="https://wangak.cc/tags/%E5%9B%BE%E5%83%8F%E5%88%86%E5%89%B2/"/>
    
  </entry>
  
  <entry>
    <title>Improving Transformers with Dynamically Composable Multi-Head Attention</title>
    <link href="https://wangak.cc/posts/2d10fdc6.html"/>
    <id>https://wangak.cc/posts/2d10fdc6.html</id>
    <published>2024-06-24T16:00:00.000Z</published>
    <updated>2024-09-19T02:11:38.681Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Improving-Transformers-with-Dynamically-Composable-Multi-Head-Attention"><a href="#Improving-Transformers-with-Dynamically-Composable-Multi-Head-Attention" class="headerlink" title="Improving Transformers with Dynamically Composable Multi-Head Attention"></a>Improving Transformers with Dynamically Composable Multi-Head Attention</h2><p><strong>论文：《Improving Transformers with Dynamically Composable Multi-Head Attention》（ICML 2024）</strong></p><p><strong>多头注意力（MHA)的不足：</strong>对于多头注意力（MHA)，其注意力头是独立工作的，这种独立性限制了每个头能捕捉到的特征和关系的多样性，注意力矩阵存在低秩瓶颈和冗余。</p><p>作者提出了<strong>动态组合多头注意力（DCMHA)</strong>,其解决了MHA的不足，通过动态组合注意力头来提高模型的表达能力。</p><p>动态可组合多头注意力（DCMHA）中，核心是一个 <strong>Compose 函数</strong>，它根据查询 <script type="math/tex">Q_i</script>和键 <script type="math/tex">K_j</script>以及可训练参数 <script type="math/tex">\theta</script>，将它们的注意力向量 <script type="math/tex">A_{:ij}</script>∈<script type="math/tex">{R}^H</script>A:ij∈RH 转换为新的向量 <script type="math/tex">A'_{:ij}</script>​</p><p>注：假设T、S是查询和键序列长度，用<script type="math/tex">A_h</script>​表示第h个头的注意力矩阵</p><p>​    <script type="math/tex">A_{:ij}</script>表示注意力向量，它是查询向量 <script type="math/tex">Q_i</script>和键向量 <script type="math/tex">K_j</script>之间的注意力得分向量</p><p><img src="https://typoraimg.wangak.cc/2023/img/202406251351820.png" alt="image-20240612150016488" style="zoom: 67%;"></p><p>​                                <img src="https://typoraimg.wangak.cc/2023/img/202406251351020.png" alt="image-20240612150213603" style="zoom:67%;">         </p><p>​        为了实现 DCMHA，在 MHA 的计算中插入两个 Compose 函数，其中一个在 softmax 之前应用于注意力分数张量 <script type="math/tex">A_S</script>，另一个在 softmax 之后应用于注意力权重张量 <script type="math/tex">A_W</script>，步骤如下：</p><ul><li>注意力分数 <script type="math/tex">A_S</script> 计算：</li></ul><p><img src="https://typoraimg.wangak.cc/2023/img/202406251351826.png" alt="image-20240612150526297" style="zoom: 67%;"></p><ul><li>注意力权重 <script type="math/tex">A_W</script> 计算：</li></ul><p><img src="https://typoraimg.wangak.cc/2023/img/202406251351155.png" alt="image-20240612150547712" style="zoom:67%;"></p><ul><li><p>DCMHA模块最后的输出为：</p><p><img src="https://typoraimg.wangak.cc/2023/img/202406251351933.png" alt="image-20240612150756075" style="zoom:67%;"></p><p><strong>Compose 函数:</strong></p></li></ul><p><img src="https://typoraimg.wangak.cc/2023/img/202406251351096.png" alt="image-20240612161027897" style="zoom:67%;"></p><p>注意力向量<script type="math/tex">A_{:ij}</script>经过5个分支进行变化，然后相加</p><ul><li><p>第一个分支（基础投影）：<script type="math/tex">A_{:ij}</script>首先由一个权重矩阵<script type="math/tex">W_b</script>进行投影</p></li><li><p>第二个分支（查询的动态投影）：<script type="math/tex">A_{:ij}</script>通过 <script type="math/tex">w_{q1}∈R^{H×R}</script>投影到低维 <script type="math/tex">R</script>,再通过<script type="math/tex">w_{q2}∈R^{R×H}</script>投影回原始维度<script type="math/tex">H</script>,其中动态权重 <script type="math/tex">w_{q1}</script> 和 <script type="math/tex">w_{q2}</script>  由 查询向量<script type="math/tex">Q_i</script> 计算得出。</p></li><li><p>第三个分支（查询的动态门控）：<script type="math/tex">A_{:ij}</script>乘以一个门控权重<script type="math/tex">w_{qg}</script>控制每个头保留或忘记原始分数。</p></li><li><p>第四个分支（键的动态投影）：类似于第二个分支</p></li><li><p>第五个分支（键的动态门控）：类似于第三个分支</p></li></ul><p>最终的 <script type="math/tex">A'_{:ij}</script>为：</p><p><img src="https://typoraimg.wangak.cc/2023/img/202406251351275.png" alt="image-20240612162302939" style="zoom:50%;"></p><p><em>注：DCMHA的可训练参数<script type="math/tex">\theta</script>为：{<script type="math/tex">W_b</script>、<script type="math/tex">w_{q1}</script>、<script type="math/tex">w_{q2}</script>、<script type="math/tex">w_{qg}</script>、<script type="math/tex">w_{k1}</script>、<script type="math/tex">w_{k2}</script>、<script type="math/tex">w_{kg}</script>}</em></p>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;Improving-Transformers-with-Dynamically-Composable-Multi-Head-Attention&quot;&gt;&lt;a href=&quot;#Improving-Transformers-with-Dynamically-Composabl</summary>
      
    
    
    
    <category term="计算机视觉" scheme="https://wangak.cc/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/"/>
    
    
    <category term="计算机视觉" scheme="https://wangak.cc/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/"/>
    
    <category term="图像分割" scheme="https://wangak.cc/tags/%E5%9B%BE%E5%83%8F%E5%88%86%E5%89%B2/"/>
    
  </entry>
  
  <entry>
    <title>Frequency Channel Attention Networks</title>
    <link href="https://wangak.cc/posts/eb050326.html"/>
    <id>https://wangak.cc/posts/eb050326.html</id>
    <published>2024-06-12T16:00:00.000Z</published>
    <updated>2024-06-23T04:05:01.869Z</updated>
    
    <content type="html"><![CDATA[<h2 id="FcaNet-Frequency-Channel-Attention-Networks"><a href="#FcaNet-Frequency-Channel-Attention-Networks" class="headerlink" title="FcaNet: Frequency Channel Attention Networks"></a>FcaNet: Frequency Channel Attention Networks</h2><p>通道注意力机制通常会为每个通道分配一个标量权重，用于加权通道特征图。然而，这种简单的标量表示可能无法充分表达通道之间的复杂关系，因此，设计更有效的通道注意力机制需要考虑如何更好地捕捉和利用通道之间的非线性关系，以充分挖掘通道特征图中的信息。</p><h4 id="DTC（离散余弦变换）"><a href="#DTC（离散余弦变换）" class="headerlink" title="DTC（离散余弦变换）"></a>DTC（离散余弦变换）</h4><p>DCT，即离散余弦变换，常用图像压缩算法，步骤如下：</p><ul><li>首先将图像分割成8x8或16x16的小块；</li><li>DCT变换，对每个小块进行DCT变换；</li><li>舍弃高频系数（AC系数），保留低频信息（DC系数）。高频系数一般保存的是图像的边界、纹理信息，低频信息主要是保存的图像中平坦区域信息。</li><li>图像的低频和高频，高频区域指的是空域图像中突变程度大的区域（比如目标边界区域），通常的纹理丰富区域。</li></ul><p>二维DCT变换就是将二维图像从空间域转换到频率域。形象的说，就是计算出图像由哪些二维余弦波构成，其主要用于数据或图像的压缩，能够将空间域的信号转换到频域上，具有良好的去相关性的性能。二维的DTC公式如下：</p><p><img src="https://typoraimg.wangak.cc/2023/img/202406131003312.png" alt="image-20240516105445387" style="zoom:67%;"></p><p>二维的逆DTC公式如下:</p><p><img src="https://typoraimg.wangak.cc/2023/img/202406131003180.png" alt="image-20240516105508741" style="zoom:67%;"></p><p>注：逆变换，通过<strong>所有</strong>频率分量的在某点的叠加可以恢复像素值。</p><p>我们称二者的共有项为基函数:</p><p><img src="https://typoraimg.wangak.cc/2023/img/202406131003442.png" alt="image-20240516105537223" style="zoom:67%;"></p><h4 id="method"><a href="#method" class="headerlink" title="method"></a>method</h4><p>传统的通道注意方法致力于构建各种通道重要性权重函数，这种权重函数要求每个通道都有一个标量来进行计算，由于计算开销有限，简单有效的全局平均池化（GAP）成为了他们的不二之选。但是一个潜在的问题是GAP是否能够捕获丰富的输入信息，也就是说，仅仅平均值是否足够表示通道注意力中的各个通道。<br><strong>GAP的不足与分析：</strong><br> 1）不同的通道可能拥有相同的平均值，而其代表的语义信息是不相同的；<br> 2）从频率分析的角度，可以证明GAP等价于DCT的最低频率，仅仅使用GAP相当于丢弃了其他许多包含着通道特征的信息；<br> 3）CBAM还表示，仅使用GAP是不够的，因此额外引入了GMP。</p><p><img src="https://typoraimg.wangak.cc/2023/img/202406131003894.png" alt="image-20240516140310965" style="zoom: 50%;"></p><p><img src="https://typoraimg.wangak.cc/2023/img/202406131003276.png" alt="image-20240516140405302" style="zoom: 67%;"></p><p><img src="https://typoraimg.wangak.cc/2023/img/202406131003179.png" alt="image-20240516140522027" style="zoom: 50%;"></p><p><img src="https://typoraimg.wangak.cc/2023/img/202406131003247.png" alt="image-20240516140545094" style="zoom:67%;"></p><p><strong>以往的通道注意力（比如下图SE）只考虑了GAP（最低频的分量信息），导致丢失了大量可利用的信息</strong>。</p><h4 id="利用频率信息重构通道注意力"><a href="#利用频率信息重构通道注意力" class="headerlink" title="利用频率信息重构通道注意力"></a>利用频率信息重构通道注意力</h4><p>将通道划分成n等分，之前我们提到“DCT可以被看作图像中每个输入的加权和”，在这里通过两步准则（后面实验会介绍），选择出收益最大的部分频率分量，与对应分组相乘，就得到我们的多光谱通道注意力。</p><p><img src="https://typoraimg.wangak.cc/2023/img/202406131003328.png" alt="image-20240516141438646" style="zoom: 67%;"></p><p><img src="https://typoraimg.wangak.cc/2023/img/202406131004303.png" alt="image-20240516141553753" style="zoom:59%;"></p>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;FcaNet-Frequency-Channel-Attention-Networks&quot;&gt;&lt;a href=&quot;#FcaNet-Frequency-Channel-Attention-Networks&quot; class=&quot;headerlink&quot; title=&quot;FcaNet</summary>
      
    
    
    
    <category term="计算机视觉" scheme="https://wangak.cc/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/"/>
    
    
    <category term="计算机视觉" scheme="https://wangak.cc/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/"/>
    
    <category term="图像分割" scheme="https://wangak.cc/tags/%E5%9B%BE%E5%83%8F%E5%88%86%E5%89%B2/"/>
    
  </entry>
  
  <entry>
    <title>AGILEFORMER:SPATIALLY AGILE TRANSFORMER UNET FOR MEDICAL IMAGE SEGMENTATION</title>
    <link href="https://wangak.cc/posts/6b854769.html"/>
    <id>https://wangak.cc/posts/6b854769.html</id>
    <published>2024-06-12T16:00:00.000Z</published>
    <updated>2024-09-02T06:26:13.893Z</updated>
    
    <content type="html"><![CDATA[<h2 id="SCConv-Spatial-and-Channel-Reconstruction-Convolution-for-Feature-Redundancy"><a href="#SCConv-Spatial-and-Channel-Reconstruction-Convolution-for-Feature-Redundancy" class="headerlink" title="SCConv: Spatial and Channel Reconstruction Convolution for Feature Redundancy"></a>SCConv: Spatial and Channel Reconstruction Convolution for Feature Redundancy</h2><p><strong>论文：《SCConv: Spatial and Channel Reconstruction Convolution for Feature Redundancy》（cvpr2023）</strong></p><p><strong>SCConv由空间重构单元（SRU)和通道重构单元（CRU)组成。</strong></p><ul><li>SRU采用分离重构的方法来抑制空间冗余</li><li>CRU采用分离变换融合的策略减少通道冗余</li></ul><h4 id="SCConv的结构"><a href="#SCConv的结构" class="headerlink" title="SCConv的结构"></a><strong>SCConv的结构</strong></h4><p><img src="https://typoraimg.wangak.cc/2023/img/202406131012152.png" alt="image-20240529144607573"></p><p>利用SRU运算获得空间细化特征<script type="math/tex">X^w</script>,然后利用CRU运算获得通道细化特征Y,通过SCConv可以减少中间特征映射之间的冗余并增强CNN的特征表示</p><p><strong>空间重构单元（SRU)：</strong></p><p><img src="https://typoraimg.wangak.cc/2023/img/202406131012043.png" alt="image-20240529152914871" style="zoom: 50%;"></p><p>空间重构单元(SRU)利用了分离和重构操作，分离操作的目的是将信息丰富的特征图与空间内容对应的信息较少的特征图分离开来</p><ul><li>利用组归一化（GN)中的因子<script type="math/tex">\gamma</script>来评估不同特征图的信息内容：</li></ul><p><img src="https://typoraimg.wangak.cc/2023/img/202406131012790.png" alt="image-20240529150814239" style="zoom:67%;"></p><p><em>注：因子<script type="math/tex">\gamma</script>是可训练的参数<script type="math/tex">\gamma\in{R^C}</script>,可利用其测量通道的像素方差，更丰富的空间信息反映了更多空间像素的变化，从而导致更大的<script type="math/tex">\gamma</script></em></p><ul><li>归一化后的权重<script type="math/tex">W_{\gamma}</script>通过下式得到：</li></ul><p><img src="https://typoraimg.wangak.cc/2023/img/202406131012984.png" alt="image-20240529151546208" style="zoom:67%;"></p><p><img src="https://typoraimg.wangak.cc/2023/img/202406131012200.png" alt="image-20240529151624068" style="zoom:67%;"></p><p><em>注:通过Gate门控来得到信息权重<script type="math/tex">W_1</script>和非信息权重<script type="math/tex">W_2</script>（Gate门控通过设置一个阈值，大于阈值置为1得到信息权重，小于置为0得到非信息权重）</em></p><ul><li><p>然后用输入特征X分别乘以信息权重<script type="math/tex">W_1</script>和非信息权重<script type="math/tex">W_2</script>，得到信息量大的<script type="math/tex">X_1^w</script>和信息量小的<script type="math/tex">X_2^w</script></p></li><li><p>最后，使用交叉重构运算将加权后的两个不同的信息特征结合起来</p></li></ul><p><img src="https://typoraimg.wangak.cc/2023/img/202406131012343.png" alt="image-20240529152948213" style="zoom:67%;"></p><p>​    <em>注：∪是concat操作</em></p><p><strong>通道重构单元（CRU)：</strong></p><p><img src="https://typoraimg.wangak.cc/2023/img/202406131012084.png" alt="image-20240529154145674" style="zoom:67%;"></p><p>使用CRU来取代了标准卷积，其通过三个操作符实现（Split、Transform、Fuse)</p><ul><li><p><strong>Split:</strong>将输入的<script type="math/tex">X^w</script>的通道分割为αC和(1-α)C两部分（α是分割比），之后再使用1×1卷积来压缩特征通道，来提高计算效率</p></li><li><p><strong>Transform:</strong></p><ul><li><p>对<script type="math/tex">X_{up}</script>​​使用（GWC、PWC)取代标准卷积来降低计算成本，之后再对输出进行汇总：</p><p><img src="https://typoraimg.wangak.cc/2023/img/202406131012988.png" alt="image-20240529154614185" style="zoom:67%;"></p></li><li><p>对<script type="math/tex">X_{low}</script>使用PWC作为对<script type="math/tex">X_{up}</script>的补充</p></li></ul></li></ul><p><img src="https://typoraimg.wangak.cc/2023/img/202406131012549.png" alt="image-20240529155523501" style="zoom:67%;"></p><ul><li>Fuse:对<script type="math/tex">Y_1</script>和<script type="math/tex">Y_2</script>进行融合</li></ul><p><img src="https://typoraimg.wangak.cc/2023/img/202406131012883.png" alt="image-20240529155734692" style="zoom:67%;"></p><p><img src="https://typoraimg.wangak.cc/2023/img/202406131012620.png" alt="image-20240529155800507" style="zoom: 67%;"></p><p><img src="https://typoraimg.wangak.cc/2023/img/202406131013076.png" alt="image-20240529155813158" style="zoom:67%;"></p>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;SCConv-Spatial-and-Channel-Reconstruction-Convolution-for-Feature-Redundancy&quot;&gt;&lt;a href=&quot;#SCConv-Spatial-and-Channel-Reconstruction-Co</summary>
      
    
    
    
    <category term="计算机视觉" scheme="https://wangak.cc/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/"/>
    
    
    <category term="计算机视觉" scheme="https://wangak.cc/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/"/>
    
    <category term="图像分割" scheme="https://wangak.cc/tags/%E5%9B%BE%E5%83%8F%E5%88%86%E5%89%B2/"/>
    
  </entry>
  
  <entry>
    <title>AGILEFORMER:SPATIALLY AGILE TRANSFORMER UNET FOR MEDICAL IMAGE SEGMENTATION</title>
    <link href="https://wangak.cc/posts/6b854769.html"/>
    <id>https://wangak.cc/posts/6b854769.html</id>
    <published>2024-05-13T16:00:00.000Z</published>
    <updated>2024-05-28T02:18:35.262Z</updated>
    
    <content type="html"><![CDATA[<h2 id="AGILEFORMER-SPATIALLY-AGILE-TRANSFORMER-UNET-FOR-MEDICAL-IMAGE-SEGMENTATION"><a href="#AGILEFORMER-SPATIALLY-AGILE-TRANSFORMER-UNET-FOR-MEDICAL-IMAGE-SEGMENTATION" class="headerlink" title="AGILEFORMER: SPATIALLY AGILE TRANSFORMER UNET FOR MEDICAL IMAGE SEGMENTATION"></a>AGILEFORMER: SPATIALLY AGILE TRANSFORMER UNET FOR MEDICAL IMAGE SEGMENTATION</h2><p>论文：《AGILEFORMER: SPATIALLY AGILE TRANSFORMER UNET FOR MEDICAL IMAGE SEGMENTATION》（arXiv 2024）</p><ul><li><p>作者使用了一种新的patch embedding取代了vit-unet中标准的patch embedding</p></li><li><p>采用空间动态自注意力来捕获空间变化特征</p></li><li><p>提出了一种新的多尺度可变形位置编码</p></li></ul><p><img src="https://typoraimg.wangak.cc/2023/img/202405141004754.png" alt="image-20240511095459528"></p><h4 id="可变形的patch-embedding"><a href="#可变形的patch-embedding" class="headerlink" title="可变形的patch embedding"></a><strong>可变形的patch embedding</strong></h4><p>通过可变形卷积卷积核的采样位置可以根据输入的特征图进行微小的偏移，从而可以实现更灵活的特征提取。通过引入可变形的采样位置，使得补丁嵌入可以更好地适应不规则的结构，从而提高了特征提取的灵活性和准确性。</p><ul><li><p>第一个patch embedding：使用两个连续的可变形卷积层，这两个连续重叠的可变形patch embedding可以更好地提取局部特征，弥补了自注意力中局部性的不足</p></li><li><p>下采样层：通过3×3卷积完成下采样</p></li></ul><h4 id="空间动态自注意力"><a href="#空间动态自注意力" class="headerlink" title="空间动态自注意力"></a><strong>空间动态自注意力</strong></h4><p><strong>可变形多头自注意力：</strong></p><p>第h个头的计算过程如下：</p><p><img src="https://typoraimg.wangak.cc/2023/img/202405141004778.png" alt="image-20240511100930114" style="zoom:67%;"></p><p>注：f是输入的特征图，ϕ是插值函数，用于生成偏移后的特征图<script type="math/tex">\hat{f}</script>,<script type="math/tex">∆p_h</script>是第h个头部生成的偏移量,其通过一个卷积层生成</p><p><strong>邻域多头自注意力：</strong></p><p>与标准自注意力不同，标准自注意力计算特征图f中每个位置p的元素与其他位置元素的相似度，而邻域注意力只利用位置p周围k个最近邻的信息来计算注意力权重，而不是与所有位置的元素计算相似度。减少了标准自注意力的计算复杂度，从二次降至近似于空间维度线性的复杂度。重新引入了局部操作到自注意力中，使得模型具有平移等变性，从而提高了保留局部信息的能力。</p><h4 id="多尺度可变形位置编码"><a href="#多尺度可变形位置编码" class="headerlink" title="多尺度可变形位置编码"></a>多尺度可变形位置编码</h4><p><img src="https://typoraimg.wangak.cc/2023/img/202405141004551.png" alt="image-20240511102232095" style="zoom:67%;"></p><p>通过在跨多个尺度对不规则采样的位置信息进行编码。</p><p>公式如下：</p><p><img src="https://typoraimg.wangak.cc/2023/img/202405141004922.png" alt="image-20240511102056849"></p><p><em>注：f是输入特征图，<script type="math/tex">P_θ</script>实现为多尺度可变形深度卷积层，具有不同的核大小（3×3和5×5）</em></p>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;AGILEFORMER-SPATIALLY-AGILE-TRANSFORMER-UNET-FOR-MEDICAL-IMAGE-SEGMENTATION&quot;&gt;&lt;a href=&quot;#AGILEFORMER-SPATIALLY-AGILE-TRANSFORMER-UNET-</summary>
      
    
    
    
    <category term="计算机视觉" scheme="https://wangak.cc/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/"/>
    
    
    <category term="计算机视觉" scheme="https://wangak.cc/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/"/>
    
    <category term="图像分割" scheme="https://wangak.cc/tags/%E5%9B%BE%E5%83%8F%E5%88%86%E5%89%B2/"/>
    
    <category term="特征提取" scheme="https://wangak.cc/tags/%E7%89%B9%E5%BE%81%E6%8F%90%E5%8F%96/"/>
    
  </entry>
  
  <entry>
    <title>BEFUnet:A Hybrid CNN-Transformer Architecture for Precise Medical Image Segmentation</title>
    <link href="https://wangak.cc/posts/7a30900a.html"/>
    <id>https://wangak.cc/posts/7a30900a.html</id>
    <published>2024-05-13T16:00:00.000Z</published>
    <updated>2024-05-28T02:18:35.264Z</updated>
    
    <content type="html"><![CDATA[<h2 id="BEFUnet-A-Hybrid-CNN-Transformer-Architecture-for-Precise-Medical-Image-Segmentation"><a href="#BEFUnet-A-Hybrid-CNN-Transformer-Architecture-for-Precise-Medical-Image-Segmentation" class="headerlink" title="BEFUnet: A Hybrid CNN-Transformer Architecture for Precise Medical Image Segmentation"></a>BEFUnet: A Hybrid CNN-Transformer Architecture for Precise Medical Image Segmentation</h2><p>论文：《BEFUnet: A Hybrid CNN-Transformer Architecture for Precise Medical Image Segmentation》（arXiv 2024）</p><p>本文提出了一种创新的u型网络BEFUnet，该网络增强了体特征和边缘特征的融合，以实现精确的医学图像分割</p><p><strong>双分支编码器：</strong></p><p><img src="https://typoraimg.wangak.cc/2023/img/202405141006324.png" alt="image-20240504130415926" style="zoom:67%;"></p><ul><li><p><strong>边缘编码器：</strong>由四个阶段组成，每个阶段包含4个PDC块用于特征检测，并利用最大池化对各阶段之间的特征进行降采样来得到分层特征</p><p>注：PDC块包括一个深度卷积层、一个ReLU层和一个1×1的卷积层</p></li><li><p><strong>主体编码器：</strong>使用Swin-Transfomer对具有全局信息的高级特征进行编码</p></li></ul><p>将提取的边缘和体特征输入到LCAF模块进行融合</p><p><strong>LCAF：</strong>选择性地将边缘图和主体图进行交叉注意力，来融合边缘和主体的特征</p><p><img src="https://typoraimg.wangak.cc/2023/img/202405141006843.png" alt="image-20240504131627922" style="zoom:67%;"></p><p><strong>DLF模块：</strong>为确保层级之间的特征一致性，使用交叉注意力机制来跨尺度融合信息</p><p>较浅的层级包含更精确的定位信息，而较深的层级携带更适合解码器的更多语义信息，考虑到节省计算资源，只将最浅层（<script type="math/tex">P^l</script>)和最后一层（<script type="math/tex">P^s</script>)进行融合</p><p><img src="https://typoraimg.wangak.cc/2023/img/202405141006502.png" alt="image-20240504133235606" style="zoom:67%;"></p>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;BEFUnet-A-Hybrid-CNN-Transformer-Architecture-for-Precise-Medical-Image-Segmentation&quot;&gt;&lt;a href=&quot;#BEFUnet-A-Hybrid-CNN-Transformer-Arc</summary>
      
    
    
    
    <category term="计算机视觉" scheme="https://wangak.cc/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/"/>
    
    
    <category term="计算机视觉" scheme="https://wangak.cc/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/"/>
    
    <category term="图像分割" scheme="https://wangak.cc/tags/%E5%9B%BE%E5%83%8F%E5%88%86%E5%89%B2/"/>
    
    <category term="特征融合" scheme="https://wangak.cc/tags/%E7%89%B9%E5%BE%81%E8%9E%8D%E5%90%88/"/>
    
    <category term="双编码器" scheme="https://wangak.cc/tags/%E5%8F%8C%E7%BC%96%E7%A0%81%E5%99%A8/"/>
    
  </entry>
  
  <entry>
    <title>SegMamba:Long-range Sequential Modeling Mamba For 3D Medical Image Segmentation</title>
    <link href="https://wangak.cc/posts/f4ee98c4.html"/>
    <id>https://wangak.cc/posts/f4ee98c4.html</id>
    <published>2024-05-13T16:00:00.000Z</published>
    <updated>2024-05-28T01:55:18.223Z</updated>
    
    <content type="html"><![CDATA[<h2 id="SegMamba-Long-range-Sequential-Modeling-Mamba-For-3D-Medical-Image-Segmentation"><a href="#SegMamba-Long-range-Sequential-Modeling-Mamba-For-3D-Medical-Image-Segmentation" class="headerlink" title="SegMamba: Long-range Sequential Modeling Mamba For 3D Medical Image Segmentation"></a>SegMamba: Long-range Sequential Modeling Mamba For 3D Medical Image Segmentation</h2><p>论文：《SegMamba: Long-range Sequential Modeling Mamba For 3D Medical Image Segmentation》（arXiv 2024）</p><p>论文贡献：</p><ul><li>设计了ToM模块，用以增强三维特征的顺序建模</li><li>设计了门控空间卷积模块（GSC)，用以增强每个ToM之前空间维度上的特征表示</li></ul><h4 id="网络结构："><a href="#网络结构：" class="headerlink" title="网络结构："></a><strong>网络结构：</strong></h4><p><img src="https://typoraimg.wangak.cc/2023/img/202405141007030.png" alt="image-20240507091527951"></p><p><strong>1.Stem</strong></p><p>采用深度卷积，内核大小为7×7×7，填充为3×3×3，步幅为2×2×2。</p><p><strong>2.TSMamba块</strong></p><p>计算过程如下：</p><p><img src="https://typoraimg.wangak.cc/2023/img/202405141007633.png" alt="image-20240507091715749" style="zoom:67%;"></p><p><em>注：其中GSC和ToM分别表示所提出的门控空间卷积模块和三向Mamba模块</em></p><p><strong>3.门控空间卷积(GSC)</strong></p><p>门控空间卷积(GSC)用于提取mamba层之前的空间关系</p><p><img src="https://typoraimg.wangak.cc/2023/img/202405141007183.png" alt="image-20240507092130322" style="zoom:67%;"></p><p>计算过程如下：</p><p><img src="https://typoraimg.wangak.cc/2023/img/202405141007271.png" alt="image-20240507092149524" style="zoom:67%;"></p><p><strong>4.三向mamba（ToM)</strong></p><p>从三个方向计算特征依赖关系，将三维输入特征平铺成三个序列，进行相应的特征交互，得到融合后的三维特征</p><p>计算过程如下：</p><p><img src="https://typoraimg.wangak.cc/2023/img/202405141007074.png" alt="image-20240507092415793" style="zoom:67%;"></p>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;SegMamba-Long-range-Sequential-Modeling-Mamba-For-3D-Medical-Image-Segmentation&quot;&gt;&lt;a href=&quot;#SegMamba-Long-range-Sequential-Modeling-M</summary>
      
    
    
    
    <category term="计算机视觉" scheme="https://wangak.cc/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/"/>
    
    
    <category term="计算机视觉" scheme="https://wangak.cc/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/"/>
    
    <category term="图像分割" scheme="https://wangak.cc/tags/%E5%9B%BE%E5%83%8F%E5%88%86%E5%89%B2/"/>
    
    <category term="特征增强" scheme="https://wangak.cc/tags/%E7%89%B9%E5%BE%81%E5%A2%9E%E5%BC%BA/"/>
    
    <category term="mamba类" scheme="https://wangak.cc/tags/mamba%E7%B1%BB/"/>
    
  </entry>
  
  <entry>
    <title>SegFormer3D</title>
    <link href="https://wangak.cc/posts/ee049a76.html"/>
    <id>https://wangak.cc/posts/ee049a76.html</id>
    <published>2024-05-06T16:00:00.000Z</published>
    <updated>2024-05-28T02:18:35.265Z</updated>
    
    <content type="html"><![CDATA[<h2 id="SegFormer3D"><a href="#SegFormer3D" class="headerlink" title="SegFormer3D"></a>SegFormer3D</h2><p><strong>论文：《SegFormer3D: an Efficient Transformer for 3D Medical Image Segmentation》（arXiv 2024）</strong></p><p>作者提出了SegFormer3D使用全mlp解码器来聚合局部和全局注意力特征，来产生高度准确的分割掩码</p><p><img src="https://typoraimg.wangak.cc/2023/img/202405071927153.png" alt="image-20240427155943100" style="zoom:67%;"></p><p><strong>对于编码器部分：</strong></p><ul><li><p>使用patch merging进行下采样：与池化操作相比克服了像素生成过程中的邻域信息丢失的问题，同时也能节省一定的运算量</p></li><li><p>使用efficient self-attention：捕获全局信息的同时，用缩放系数R减少了self-attention计算的时间复杂度</p><p><img src="https://typoraimg.wangak.cc/2023/img/202405071927313.png" alt="image-20240427161059247" style="zoom:67%;"></p></li><li><p>舍弃了固定的位置编码，使用mix ffn模块来提取位置信息：</p><p><img src="https://typoraimg.wangak.cc/2023/img/202405071927148.png" alt="image-20240427161359128" style="zoom: 50%;"></p></li></ul><p><strong>对于解码器部分：</strong>使用了全MLP的结构，使用了一个统一的模块完成了对不同尺度的特征的解码过程，简化了解码过程，确保在各种数据集中对体积特征进行高效且一致的解码，避免了过度参数化。</p><p><img src="https://typoraimg.wangak.cc/2023/img/202405071927399.png" alt="image-20240427161629423" style="zoom:67%;"></p><p><strong>参数量和性能的比较：</strong></p><p><img src="https://typoraimg.wangak.cc/2023/img/202405071927981.png" alt="image-20240427162144102" style="zoom: 50%;"></p><p><img src="https://typoraimg.wangak.cc/2023/img/202405071927799.png" alt="image-20240427162104575" style="zoom:67%;"></p><p>作者在编码器和解码器的设计中，都使用轻量化的操作（编码器部分的patch merging、efficient self-attention和解码器使用全mlp解码器来聚合局部和全局注意力特征），模型参数量和计算量都有明显的下降，但性能表现不如nnformer</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;SegFormer3D&quot;&gt;&lt;a href=&quot;#SegFormer3D&quot; class=&quot;headerlink&quot; title=&quot;SegFormer3D&quot;&gt;&lt;/a&gt;SegFormer3D&lt;/h2&gt;&lt;p&gt;&lt;strong&gt;论文：《SegFormer3D: an Effici</summary>
      
    
    
    
    <category term="计算机视觉" scheme="https://wangak.cc/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/"/>
    
    
    <category term="计算机视觉" scheme="https://wangak.cc/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/"/>
    
    <category term="图像分割" scheme="https://wangak.cc/tags/%E5%9B%BE%E5%83%8F%E5%88%86%E5%89%B2/"/>
    
    <category term="特征融合" scheme="https://wangak.cc/tags/%E7%89%B9%E5%BE%81%E8%9E%8D%E5%90%88/"/>
    
  </entry>
  
  <entry>
    <title>Medical Image Segmentation via Cascaded Attention Decoding</title>
    <link href="https://wangak.cc/posts/3790531e.html"/>
    <id>https://wangak.cc/posts/3790531e.html</id>
    <published>2024-05-06T16:00:00.000Z</published>
    <updated>2024-05-07T11:47:50.554Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Medical-Image-Segmentation-via-Cascaded-Attention-Decoding"><a href="#Medical-Image-Segmentation-via-Cascaded-Attention-Decoding" class="headerlink" title="Medical Image Segmentation via Cascaded Attention Decoding"></a>Medical Image Segmentation via Cascaded Attention Decoding</h2><p><strong>论文：《Medical Image Segmentation via Cascaded Attention Decoding》（WACV 2023)</strong></p><p>这篇论文和之前看过的一些文章设计思路也差不多，通过对编码器四个阶段输出的特征进行了融合，不同之处在于这篇论文提出了AG和CAM两个模块进行特征的融合，先使用AG进行特征融合，再使用CAM进行增强</p><p><strong>级联注意解码器:</strong></p><p><img src="https://typoraimg.wangak.cc/2023/img/202405071929069.png" alt="image-20240426162057053" style="zoom:67%;"></p><p>UpConv:对特征进行上采样</p><p>CAM:用于增强特征映射</p><p>作者使用了四个CAM块聚合编码器四个阶段输出的特征，三个AG模块将先前解码器上采样的特征与跳跃连接的特征结合起来</p><p><strong>AG：用于级联的特征融合</strong></p><p><img src="https://typoraimg.wangak.cc/2023/img/202405071929850.png" alt="image-20240427213751108" style="zoom:67%;"></p><p><img src="https://typoraimg.wangak.cc/2023/img/202405071930154.png" alt="image-20240427213938485" style="zoom: 67%;"></p><p><strong>CAM:用于增强特征映射</strong></p><p><img src="https://typoraimg.wangak.cc/2023/img/202405071930371.png" alt="image-20240427214031982" style="zoom: 80%;"></p><p>CAM由通道注意力、空间注意力和一个卷积块组成，使用通道注意力、空间注意力可以来抑制背景信息</p><p><img src="https://typoraimg.wangak.cc/2023/img/202405071930603.png" alt="image-20240427214223908" style="zoom:67%;"></p><p>注：这里的卷积块是由两个3×3的卷积层组成，每个卷积层后是一个BN和一个ReLU</p><p><img src="https://typoraimg.wangak.cc/2023/img/202405071930828.png" alt="image-20240427214438165" style="zoom:67%;"></p><p><strong>多阶段损失和特征聚合：</strong></p><p>作者是通过对分层编码器的四个阶段输出的结果使用加性聚合的方式生成最后的预测图：</p><p><img src="https://typoraimg.wangak.cc/2023/img/202405071930176.png" alt="image-20240427214745978" style="zoom:67%;"></p><p><em>注1：p1、p2、p3、p4为四个预测头的特征映射，w、x、y、z为各个预测头像的权重。</em></p><p><em>注2：本文中作者将w、x、y、z均设为1</em></p><p>作者通过分别计算每个预测头的损失，来得到最终的损失：</p><p><img src="https://typoraimg.wangak.cc/2023/img/202405071930911.png" alt="image-20240427215042984" style="zoom:67%;"></p><p><em>注：实验中作者将α, β, γ,  ζ均设为了1</em></p><p><strong>整体的网络结构如下：</strong></p><p><img src="https://typoraimg.wangak.cc/2023/img/202405071930348.png" alt="image-20240427215222965" style="zoom: 50%;"></p>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;Medical-Image-Segmentation-via-Cascaded-Attention-Decoding&quot;&gt;&lt;a href=&quot;#Medical-Image-Segmentation-via-Cascaded-Attention-Decoding&quot; cl</summary>
      
    
    
    
    <category term="计算机视觉" scheme="https://wangak.cc/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/"/>
    
    
    <category term="计算机视觉" scheme="https://wangak.cc/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/"/>
    
    <category term="图像分割" scheme="https://wangak.cc/tags/%E5%9B%BE%E5%83%8F%E5%88%86%E5%89%B2/"/>
    
    <category term="特征融合" scheme="https://wangak.cc/tags/%E7%89%B9%E5%BE%81%E8%9E%8D%E5%90%88/"/>
    
    <category term="特征增强" scheme="https://wangak.cc/tags/%E7%89%B9%E5%BE%81%E5%A2%9E%E5%BC%BA/"/>
    
  </entry>
  
  <entry>
    <title>HCF-Net</title>
    <link href="https://wangak.cc/posts/d349cca5.html"/>
    <id>https://wangak.cc/posts/d349cca5.html</id>
    <published>2024-04-24T16:00:00.000Z</published>
    <updated>2024-05-07T06:34:28.076Z</updated>
    
    <content type="html"><![CDATA[<h2 id="HCF-Net-Hierarchical-Context-Fusion-Network-for-Infrared-Small-Object-Detection"><a href="#HCF-Net-Hierarchical-Context-Fusion-Network-for-Infrared-Small-Object-Detection" class="headerlink" title="HCF-Net: Hierarchical Context Fusion Network for Infrared Small Object Detection"></a>HCF-Net: Hierarchical Context Fusion Network for Infrared Small Object Detection</h2><p><strong>论文：《HCF-Net: Hierarchical Context Fusion Network for Infrared Small Object Detection》（arXiv 2024)</strong></p><p>PPA采用分层特征融合和注意机制来维护和增强对小物体的表示，确保关键信息通过多次下采样步骤得以保留。</p><p>DASI增强了U-Net中的跳跃连接，侧重于高维和低维特征的自适应选择和精细融合，以增强小物体的显著性。</p><p>位于网络深处的MDCR加强了多尺度特征提取和通道信息表示，捕捉各种感受野范围内的特征。</p><p><img src="https://typoraimg.wangak.cc/2023/img/202404251920292.png" alt="image-20240416133737554" style="zoom:67%;"></p><h4 id="1-PPA"><a href="#1-PPA" class="headerlink" title="1.PPA"></a>1.PPA</h4><p><img src="https://typoraimg.wangak.cc/2023/img/202404251920768.png" alt="image-20240416134505795" style="zoom:67%;"></p><p>对于小目标而言，经过多次下采样容易丢失关键信息，作者使用PPA模块取代了编码器和解码器中的传统卷积的操作，来解决此问题。</p><p>PPA使用了多分支提取策略，多分支策略有助于捕获物体的多尺度特征，从而提高对小目标的特征提取能力。其由三个平行分支组成：局部、全局、串行卷积。</p><p>PPA步骤如下：</p><ul><li>对于输入特征F,先使用点卷积调整通道数，然后输入到三个分支</li><li><p>通过三个分支分别计算得到<script type="math/tex">F_{local}、F_{global}、F_{conv}</script></p></li><li><p>将三个分支的输出结果相加得到最后的输出</p></li></ul><p><strong>Patch-Aware:</strong></p><p>使用 Unfold 和 reshape 操作将特征张量 F’ 划分为一组空间上连续的patch,对这些 patch 进行通道方向上的平均，得到大小为 (p × p, H’/p, W’/p) 的结果，接着使用 FFN 进行线性计算。随后，应用激活函数来获得线性计算特征在空间维度上的概率分布，并相应调整它们的权重,对加权结果进行特征选择，从 tokens 和通道中选择与任务相关的特征。</p><p><em>注：局部和全局分支的区分是通过patch的大小参数p来控制的</em></p><p><strong>特征融合和注意力：</strong></p><p>在进行了多分支的特征提取后，利用注意力机制进行自适应的特征增强，该注意力模块由通道注意力和空间注意力组成，其过程如下：</p><p><img src="https://typoraimg.wangak.cc/2023/img/202404251920747.png" alt="image-20240416143044284" style="zoom:67%;"></p><p><em>注：其中，⊗ 表示元素相乘，<script type="math/tex">F_c∈ R^{H'×W'×C'}</script>、<script type="math/tex">F_s∈ R^{H'×W'×C'}</script>分别表示通道注意力和空间注意力处理后的特征，<script type="math/tex">M_c∈ R^{1×1×C'}</script>是通道注意力图,<script type="math/tex">M_s∈ R^{H'×W'×1}</script>是空间注意力图。δ和 B 分别表示ReLU和BN，F’’是PPA的最终的输出</em></p><h4 id="2-维度感知选择性整合模块"><a href="#2-维度感知选择性整合模块" class="headerlink" title="2.维度感知选择性整合模块"></a><strong>2.维度感知选择性整合模块</strong></h4><p><img src="https://typoraimg.wangak.cc/2023/img/202404251920269.png" alt="image-20240416144400263" style="zoom: 80%;"></p><p>DASI能够根据物体的大小自适应地选择合适的特征进行融合，DASI通过卷积、插值等操作，将高维特征<script type="math/tex">F_h∈R^{H_h×W_h×C_h}</script>和低维特征<script type="math/tex">F_l∈R^{H_l×W_l×C_l}</script>与当前层的特征<script type="math/tex">F_u∈R^{H×W×C}</script>进行初步对齐。随后，它将这些特征在通道维度上分成四个相等的部分，从而得到 <script type="math/tex">{(h_i)}_{i=1}^4 ∈R^{H × W × C/4}</script>, <script type="math/tex">{(I_i)}_{i=1}^4 ∈R^{H × W × C/4}</script>, <script type="math/tex">{(u_i)}_{i=1}^4 ∈R^{H × W × C/4}</script>,其中 <script type="math/tex">h_i、 I_i和 u_i</script> 分别表示高维、低维和当前层特征的第 i 个分区特征。<br>该模块最终的输出结构为：</p><p><img src="https://typoraimg.wangak.cc/2023/img/202404251920852.png" alt="image-20240416145650808" style="zoom:67%;"></p><p><em>注： α是通过应用于<script type="math/tex">u_i</script>的激活函数所得到的值，当α> 0.5，则模型优先考虑细粒度特征，当α &lt; 0.5，则强调上下文特征。</em></p><h4 id="3-MDCR"><a href="#3-MDCR" class="headerlink" title="3.MDCR"></a>3.MDCR</h4><p>在 MDCR 中，引入了多个深度可分离卷积层，以不同的扩张率捕捉各种感受野大小的空间特征，从而能够对物体和背景之间的差异进行更详细的建模，增强其分辨小物体的能力。</p><p><img src="https://typoraimg.wangak.cc/2023/img/202404251920901.png" alt="image-20240416150546927" style="zoom:67%;"></p><p>将输入特征沿通道维度分成四部分，每个部分以不同的扩张率进行深度可分离卷积，然后通过对每个部分的通道交错重排来增强多尺度特征的多样性，最后使用点卷积将这四部分的信息进行融合。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;HCF-Net-Hierarchical-Context-Fusion-Network-for-Infrared-Small-Object-Detection&quot;&gt;&lt;a href=&quot;#HCF-Net-Hierarchical-Context-Fusion-Netwo</summary>
      
    
    
    
    <category term="计算机视觉" scheme="https://wangak.cc/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/"/>
    
    
    <category term="计算机视觉" scheme="https://wangak.cc/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/"/>
    
    <category term="图像分割" scheme="https://wangak.cc/tags/%E5%9B%BE%E5%83%8F%E5%88%86%E5%89%B2/"/>
    
    <category term="特征融合" scheme="https://wangak.cc/tags/%E7%89%B9%E5%BE%81%E8%9E%8D%E5%90%88/"/>
    
    <category term="下采样模块" scheme="https://wangak.cc/tags/%E4%B8%8B%E9%87%87%E6%A0%B7%E6%A8%A1%E5%9D%97/"/>
    
  </entry>
  
  <entry>
    <title>LHU-NET</title>
    <link href="https://wangak.cc/posts/c46bf14d.html"/>
    <id>https://wangak.cc/posts/c46bf14d.html</id>
    <published>2024-04-24T16:00:00.000Z</published>
    <updated>2024-04-25T11:20:57.093Z</updated>
    
    <content type="html"><![CDATA[<h2 id="LHU-NET-A-LIGHT-HYBRID-U-NET-FOR-COST-EFFICIENT-HIGH-PERFORMANCE-VOLUMETRIC-MEDICAL-IMAGE-SEGMENTATION"><a href="#LHU-NET-A-LIGHT-HYBRID-U-NET-FOR-COST-EFFICIENT-HIGH-PERFORMANCE-VOLUMETRIC-MEDICAL-IMAGE-SEGMENTATION" class="headerlink" title="LHU-NET: A LIGHT HYBRID U-NET FOR COST-EFFICIENT, HIGH-PERFORMANCE VOLUMETRIC MEDICAL IMAGE SEGMENTATION"></a>LHU-NET: A LIGHT HYBRID U-NET FOR COST-EFFICIENT, HIGH-PERFORMANCE VOLUMETRIC MEDICAL IMAGE SEGMENTATION</h2><p><strong>论文：《LHU-Net: A Light Hybrid U-Net for Cost-Efficient, High-Performance Volumetric Medical Image Segmentation》（arXiv 2024)</strong></p><p><img src="https://typoraimg.wangak.cc/2023/img/202404251917245.png" alt="image-20240414134752694" style="zoom:67%;"></p><p>LHU-Net：将基于卷积的块与混合注意力机制集成</p><p><strong>Init 阶段与Out阶段</strong>：</p><ul><li>该阶段从一个点卷积操作（PW-Conv）开始，应用于输入数据，调整通道维度以匹配后续级别的通道数。</li></ul><p><img src="/posts/c46bf14d.htm/Users\wangak\AppData\Roaming\Typora\typora-user-images\image-20240414134832658.png" alt="image-20240414134832658"></p><ul><li><p>同时对于输入直接应用ResBlock，将其输出输入到Out阶段，与解码器的输出进行连接，产生最后的结果</p><p><img src="/posts/c46bf14d.htm/Users\wangak\AppData\Roaming\Typora\typora-user-images\image-20240414135339973.png" alt="image-20240414135339973"></p></li></ul><p><em>注：<script type="math/tex">X_{CD}</script>​表示CNN解码器块输出</em></p><p><strong>CNN Blocks:</strong></p><p>初始空间维度需要大量的计算成本,Vit难以应用，故在此阶段的设计是为了优化参数效率和保留局部特征，在之后的阶段再提取全局特征</p><ul><li>Down Conv块的设计如下：</li></ul><p><img src="https://typoraimg.wangak.cc/2023/img/202404251917748.png" alt="image-20240414140026771"></p><p><strong>Hybrid Blocks（混合注意力）：</strong></p><p>在此阶段将局部细节和全局信息进行融合</p><ul><li><p><strong>Self-Adaptive Contextual Fusion Module：</strong>将空间注意力模块与卷积模块相结合，这种空间注意力模块将LKAd模块与自注意力机制的输出进行并行计算</p><p><img src="https://typoraimg.wangak.cc/2023/img/202404251917014.png" alt="image-20240414141611267"></p></li></ul><p>​     该模块最终的输出为：</p><p>​                                                                  <img src="https://typoraimg.wangak.cc/2023/img/202404251918107.png" alt="image-20240414141723308" style="zoom:67%;"></p><p>​     <em>注1：<script type="math/tex">δ_s</script>和<script type="math/tex">γ_s</script>​表示每个通道的可学习参数，控制两种不同注意机制的组合权值</em></p><p>​     <em>注2：Comb函数的定义如下：DW-Conv3 是一个 3×3×3 的卷积块</em></p><p><img src="https://typoraimg.wangak.cc/2023/img/202404251918397.png" alt="image-20240414142913765" style="zoom:67%;"></p><ul><li><p><strong>LKAd:</strong></p><p>其步骤如下：</p><ul><li>输入经过一个点卷积操作（Conv1），然后应用激活函数（GELU），以引入非线性和降低维度。</li><li>变换后的张量经过一系列深度卷积（DW-Conv）和深度膨胀卷积（DWD-Conv）操作，以提取多尺度特征并保留空间信息。</li><li>DDW-Conv3集成了可变形深度卷积可以自适应地对特征图进行采样，从而增强了模型捕获细粒度细节和长距离依赖的能力。</li></ul></li></ul><p><img src="https://typoraimg.wangak.cc/2023/img/202404251918886.png" alt="image-20240414142033147" style="zoom:67%;"></p>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;LHU-NET-A-LIGHT-HYBRID-U-NET-FOR-COST-EFFICIENT-HIGH-PERFORMANCE-VOLUMETRIC-MEDICAL-IMAGE-SEGMENTATION&quot;&gt;&lt;a href=&quot;#LHU-NET-A-LIGHT-HY</summary>
      
    
    
    
    <category term="计算机视觉" scheme="https://wangak.cc/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/"/>
    
    
    <category term="计算机视觉" scheme="https://wangak.cc/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/"/>
    
    <category term="图像分割" scheme="https://wangak.cc/tags/%E5%9B%BE%E5%83%8F%E5%88%86%E5%89%B2/"/>
    
    <category term="卷积+注意力" scheme="https://wangak.cc/tags/%E5%8D%B7%E7%A7%AF-%E6%B3%A8%E6%84%8F%E5%8A%9B/"/>
    
  </entry>
  
  <entry>
    <title>D-Net</title>
    <link href="https://wangak.cc/posts/ceff47a.html"/>
    <id>https://wangak.cc/posts/ceff47a.html</id>
    <published>2024-04-13T16:00:00.000Z</published>
    <updated>2024-04-27T12:58:50.976Z</updated>
    
    <content type="html"><![CDATA[<h2 id="D-Net-Dynamic-Large-Kernel-with-Dynamic-Feature-Fusion-for-Volumetric-Medical-Image-Segmentation"><a href="#D-Net-Dynamic-Large-Kernel-with-Dynamic-Feature-Fusion-for-Volumetric-Medical-Image-Segmentation" class="headerlink" title="D-Net: Dynamic Large Kernel with Dynamic Feature Fusion for Volumetric Medical Image Segmentation"></a>D-Net: Dynamic Large Kernel with Dynamic Feature Fusion for Volumetric Medical Image Segmentation</h2><p><strong>论文：《D-Net: Dynamic Large Kernel with Dynamic Feature Fusion for Volumetric Medical Image Segmentation》（arXiv 2024）</strong></p><p>主要贡献：</p><ul><li>提出用于通用特征提取的大核模块（DLK)，其采用多个大卷积核来捕获多尺度特征，然后利用动态选择机制，根据全局上下文信息自适应地吐出最重要的空间特征</li><li>提出动态特征融合模块，实现自适应的特征融合（DFF)，其根据全局信息融合多尺度局部特征</li><li>提出D-Net用于3d医学图像分割，其将DLK和DFF模块结合到了分层Vit模块中，实现了更高的分割准确性</li></ul><p>使用固定大小的核的卷积在自适应地捕获多尺度特征方面存在不足，因而作者提出了动态大核（DLK)与动态特征融合模块（DFF)</p><p>(顺序地聚合大核卷积来扩大感受野)</p><p><strong>Dynamic Large Kernel (DLK)：</strong></p><p><img src="https://typoraimg.wangak.cc/2023/img/202404141315413.png" alt="image-20240413124105291" style="zoom:67%;"></p><p>作者采用了级联大核卷积的方法，其卷积核尺寸和膨胀率逐渐增大。这样设计使得有效感受野逐渐增大，从而有效地捕获更广泛的信息，在更深、更大的感受野内提取的特征对输出的贡献更显著，使得DLK能够捕获更细致和更具信息量的特征。</p><ul><li>作者使用了两个具有大核的深度卷积进行级联：</li></ul><p><img src="https://typoraimg.wangak.cc/2023/img/202404141315469.png" alt="image-20240413124653183" style="zoom: 67%;"></p><p><em>注1：5，7是核大小，1，3是膨胀率</em></p><p><em>注2：通过级联大核卷积，DLK具有与23 × 23 × 23核大小的卷积相同的有效感受野</em></p><ul><li>随后，对两个级联的大核卷积的输出<script type="math/tex">X_1^l、X_2^l</script>应用平均池化(AVP)和最大池化（MAP)来建模局部特征之间的全局空间关系。</li></ul><p><img src="/posts/ceff47a.htm/Users\wangak\AppData\Roaming\Typora\typora-user-images\image-20240413125400295.png" alt="image-20240413125400295" style="zoom:67%;"></p><ul><li>动态选择机制：通过一个7×7×7的卷积层来实现不同空间描述符<script type="math/tex">(w_{avg},w_{map})</script>之间的信息交互,并使用Sigmoid激活函数来获得动态选择值<script type="math/tex">w_1，w_2</script></li></ul><p><img src="https://typoraimg.wangak.cc/2023/img/202404141315234.png" alt="image-20240413125635669" style="zoom:67%;"></p><p>​    <em>注：不同大核卷积的特征通过利用这些选择值来自适应地进行选择</em></p><ul><li><p>DLK最后的输出为：</p><p><img src="https://typoraimg.wangak.cc/2023/img/202404141315364.png" alt="image-20240413125821821" style="zoom:67%;"></p></li><li><p>DLK block的设计如下：</p></li></ul><p><img src="https://typoraimg.wangak.cc/2023/img/202404141315908.png" alt="image-20240413130239187" style="zoom:67%;"></p><p><strong>Dynamic Feature Fusion (DFF)：</strong></p><p><img src="https://typoraimg.wangak.cc/2023/img/202404141315381.png" alt="image-20240413130621926" style="zoom:67%;"></p><ul><li><p>将特征映射<script type="math/tex">F_1^l、F_2^l</script>沿通道方向进行连接，然后通过级联平均池化、卷积和Sigmoid来得到全局通道特征的重要性描述<script type="math/tex">w_{ch}</script></p><p><img src="https://typoraimg.wangak.cc/2023/img/202404141315609.png" alt="image-20240413131334972" style="zoom:67%;"></p></li><li><p>根据<script type="math/tex">w_{ch}</script>进行特征映射，然后利用1×1×1卷积进行映射(保留重要特征)</p></li></ul><p><img src="https://typoraimg.wangak.cc/2023/img/202404141315858.png" alt="image-20240413132012723" style="zoom:67%;"></p><ul><li>通过1×1×1卷积层、Sigmoid激活对特征映射<script type="math/tex">F_1^l、F_2^l</script>进行处理来捕获全局空间信息<script type="math/tex">w_{sp}</script></li></ul><p><img src="https://typoraimg.wangak.cc/2023/img/202404141315379.png" alt="image-20240413132340534" style="zoom:67%;"></p><p><strong>D-Net:</strong></p><p><img src="https://typoraimg.wangak.cc/2023/img/202404141315703.png" alt="image-20240413132535496" style="zoom:67%;"></p><p><strong>实验部分：</strong></p><p><strong>多器官分割任务：</strong></p><p><img src="https://typoraimg.wangak.cc/2023/img/202404141315736.png" alt="image-20240413132922149" style="zoom:67%;"></p>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;D-Net-Dynamic-Large-Kernel-with-Dynamic-Feature-Fusion-for-Volumetric-Medical-Image-Segmentation&quot;&gt;&lt;a href=&quot;#D-Net-Dynamic-Large-Kern</summary>
      
    
    
    
    <category term="计算机视觉" scheme="https://wangak.cc/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/"/>
    
    
    <category term="计算机视觉" scheme="https://wangak.cc/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/"/>
    
    <category term="图像分割" scheme="https://wangak.cc/tags/%E5%9B%BE%E5%83%8F%E5%88%86%E5%89%B2/"/>
    
    <category term="特征融合" scheme="https://wangak.cc/tags/%E7%89%B9%E5%BE%81%E8%9E%8D%E5%90%88/"/>
    
    <category term="卷积block" scheme="https://wangak.cc/tags/%E5%8D%B7%E7%A7%AFblock/"/>
    
  </entry>
  
  <entry>
    <title>BiFormer</title>
    <link href="https://wangak.cc/posts/f56fa5b4.html"/>
    <id>https://wangak.cc/posts/f56fa5b4.html</id>
    <published>2024-04-12T16:00:00.000Z</published>
    <updated>2024-04-13T00:00:00.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="BiFormer-Vision-Transformer-with-Bi-Level-Routing-Attention"><a href="#BiFormer-Vision-Transformer-with-Bi-Level-Routing-Attention" class="headerlink" title="BiFormer: Vision Transformer with Bi-Level Routing Attention"></a>BiFormer: Vision Transformer with Bi-Level Routing Attention</h2><p><strong>论文：《BiFormer: Vision Transformer with Bi-Level Routing Attention》（CVPR 2023)</strong></p><p><strong>文章贡献：</strong></p><ul><li>作者提出了一种新颖的双层路由机制，将其应用于传统的注意力机制中。</li><li>基于双层路由注意力机制，作者提出了一种名为BiFormer的通用视觉Transformer模型。</li></ul><p>1.注意力：</p><p><img src="https://typoraimg.wangak.cc/2023/img/202404131212635.png" alt="image-20240406101554182" style="zoom:67%;"></p><p>注：<script type="math/tex">\sqrt{C}</script>​是缩放因子用以避免梯度消失</p><p>多头注意力：对输入沿着通道维度分成h个块（头部），每个块使用一组独立的权重</p><p><img src="https://typoraimg.wangak.cc/2023/img/202404131212205.png" alt="image-20240406101842223" style="zoom: 67%;"></p><p><em>注1：<script type="math/tex">W_0</script>​是一个额外的线性变换用来组合所有的头</em></p><p><em>注2：MHSA的复杂度是<script type="math/tex">O(N^2)</script>,因为有N个查询，每个查询涉及N个键值对</em></p><p>2.双级路由注意力（BRA）</p><p>作者探索了一种动态的、查询感知的稀疏注意机制，在粗粒度的区域级别上过滤掉大多数不相关的键-值对，使得只有少部分的路由区域保留下来，在这些路由区域的并集上应用细粒度的令牌-令牌注意力。</p><p>算法步骤：</p><ul><li>区域划分和输入投影：对于输入<script type="math/tex">X\in{H×W×C}</script>,首先将其划分为<script type="math/tex">S×S</script>非重叠区域，每个区域包含<script type="math/tex">\large H×W\over{S^2}</script>个特征向量（通过对X进行reshape操作实现），然后进行线性投影得到Q、K、V:</li></ul><p><img src="https://typoraimg.wangak.cc/2023/img/202404131212896.png" alt="image-20240406103338375" style="zoom:67%;"></p><ul><li>带有向图的区域到区域路由:得到区域级的<script type="math/tex">Q^r、K^r\in{R^{S×S×C}}</script>（通过对每个区域内的Q、K取平均值得到）,然后得到区域到区域亲和图<script type="math/tex">A^r</script></li></ul><p><img src="https://typoraimg.wangak.cc/2023/img/202404131212827.png" alt="image-20240406104429624" style="zoom: 67%;"></p><p>注：<script type="math/tex">A^r</script>反应了两个区域在语义上的关联程度</p><ul><li>对<script type="math/tex">A^r</script>逐行进行top-k操作得到每个区域最相关的 k 个区域的索引，以此来修剪关联图</li></ul><p><img src="https://typoraimg.wangak.cc/2023/img/202404131212647.png" alt="image-20240406104825390" style="zoom:67%;"></p><p><em>注：<script type="math/tex">I^r</script>的第i行包含了第i个区域最相关的k个区域的索引</em></p><p>3.Token-to-token注意</p><p>得到了区域到区域路由索引矩阵<script type="math/tex">I^r</script>后，即可进行Token-to-token关注，对于每个区域i中的Q,根据 k 个路由区域中的所有键值对，收集K、V:</p><p><img src="https://typoraimg.wangak.cc/2023/img/202404131212214.png" alt="image-20240406105921662" style="zoom: 67%;"></p><p>然后进行注意力计算：</p><p><img src="https://typoraimg.wangak.cc/2023/img/202404131212315.png" alt="image-20240406110015427" style="zoom:67%;"></p><p><em>注：LCE是为了增强局部信息，其通过深度卷积实现</em></p><p>4.BRA的计算复杂度：包括三部分（线性投影、区域到区域路由和Token-to-token注意）</p><p>总计算量为:</p><p><img src="https://typoraimg.wangak.cc/2023/img/202404131212610.png" alt="image-20240406110454570" style="zoom:67%;"></p><p>作者所设计的网络模型如下：</p><p><img src="https://typoraimg.wangak.cc/2023/img/202404131212482.png" alt="image-20240406110612740" style="zoom:67%;"></p>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;BiFormer-Vision-Transformer-with-Bi-Level-Routing-Attention&quot;&gt;&lt;a href=&quot;#BiFormer-Vision-Transformer-with-Bi-Level-Routing-Attention&quot; </summary>
      
    
    
    
    <category term="计算机视觉" scheme="https://wangak.cc/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/"/>
    
    
    <category term="计算机视觉" scheme="https://wangak.cc/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/"/>
    
    <category term="图像分割" scheme="https://wangak.cc/tags/%E5%9B%BE%E5%83%8F%E5%88%86%E5%89%B2/"/>
    
  </entry>
  
  <entry>
    <title>EPT-Net</title>
    <link href="https://wangak.cc/posts/2dfc19b2.html"/>
    <id>https://wangak.cc/posts/2dfc19b2.html</id>
    <published>2024-04-01T16:00:00.000Z</published>
    <updated>2024-04-02T00:00:00.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="EPT-Net-Edge-Perception-Transformer-for-3D-Medical-Image-Segmentation"><a href="#EPT-Net-Edge-Perception-Transformer-for-3D-Medical-Image-Segmentation" class="headerlink" title="EPT-Net: Edge Perception Transformer for 3D Medical Image Segmentation"></a>EPT-Net: Edge Perception Transformer for 3D Medical Image Segmentation</h2><p><strong>论文：《EPT-Net: Edge Perception Transformer for 3D Medical Image Segmentation》（TMI 2023)</strong></p><p><strong>EPT-NET:</strong>在编码器中通过CNN提取网络的详细的底层特征，作者提出了双位置Transformer模块通过学习位置编码和像素空间位置编码的过程，增强了定位能力，解决了多器官之间定位不准确的问题，增强了网络对复杂器官形状的理解能力。</p><ul><li><p>提出了一种双位置嵌入Transformer，包括可学习位置嵌入和体素空间位置嵌入。利用该方法对位置编码进行优化，可以有效地捕捉医学图像中不同器官位置之间的内在相关性。</p></li><li><p>提出了一个边缘权重指导模块来学习浅特征中的边缘信息，它可以捕获相邻器官之间的微小粘附。这种设计是在不增加网络参数的情况下最小化边缘信息功能。</p></li></ul><p>卷积操作在局部像素周围进行计算，只能捕获局部特征，缺乏全局上下文信息的提取能力，可能会导致边缘信息的处理不足。</p><h3 id="网络结构"><a href="#网络结构" class="headerlink" title="网络结构"></a>网络结构</h3><p><img src="https://typoraimg.wangak.cc/2023/img/202404050928262.png" alt="image-20240326135759096" style="zoom:50%;"></p><p><strong>EPT-Net:</strong>基于U型网络，由DPT和EWG模块组成。</p><p><strong>双位置嵌入Transformer:</strong></p><p><img src="https://typoraimg.wangak.cc/2023/img/202404050928762.png" alt="image-20240326152549228" style="zoom:50%;"></p><ul><li><p><strong>Learnable Patch Embedding:</strong>使用位置嵌入，并且利用异步卷积，确保相邻的patch有特定的交互部分</p></li><li><p><strong>Voxel Spacial Positional Embedding:</strong>其通过一个3x3x3大小的深度卷积实现</p></li></ul><p><strong>Edge Weight Guidance Module(EWG):</strong></p><p><strong>浅层引导模块：</strong>用于提取低层特征中保留的边缘信息，其操作编码器的前两层，这两层通过一个3x3x3的卷积层调整到同一个分辨率进行级联，级联后的特征通过一个1x1x1的卷积层来得到浅层引导特征。</p><p><strong>加权注意力模块：</strong>通过特征像素之间的数学特性来评估每个特征点的优先级，通过测量目标特征和周围特征的<strong>线性可分性</strong>，<strong>优先级较高的特征点与周围的特征是线性不可分的。</strong></p><p>各特征点的优先级函数定义如下：<br>                                                  <img src="https://typoraimg.wangak.cc/2023/img/202404050929624.png" alt="image-20240326164429692" style="zoom: 67%;"></p><p><em>注1：<script type="math/tex">\hat{x_{i}}、\hat{y_{i}}</script>为特征点的局部信息，其为<script type="math/tex">x_i、y_i</script>通过线性变换得到，<script type="math/tex">ω_t</script>和<script type="math/tex">b_t</script>是变换的权值和偏置,<script type="math/tex">i</script>​为空间维度上的索引</em></p><p><em>注2：根据计算得到的特征点优先级，可以为每个特征点分配一个优先级系数。这个系数可以用来加权整个特征图，从而更好地捕获边缘信息。</em></p><p><script type="math/tex">ω_t</script>和<script type="math/tex">b_t</script>的定义如下：<br>                                                                          <img src="https://typoraimg.wangak.cc/2023/img/202404050929187.png" alt="image-20240330152301027" style="zoom: 67%;"></p><p><img src="https://typoraimg.wangak.cc/2023/img/202404050929943.png" alt="image-20240330152315586" style="zoom:67%;"></p><p>注：ξ是超参数，<script type="math/tex">m_i、v_i</script>是通道上除了<script type="math/tex">x_t</script>的所有特征点的均值和方差</p><p><script type="math/tex">\hat{p_t}</script>越小，表示特征点与周围的特征是线性不可分的，也就越重要，故定义优先级为<script type="math/tex">p=1/\hat{p_t}</script>​</p><p>所以最后的注意力计算为：</p><p><img src="https://typoraimg.wangak.cc/2023/img/202404050929675.png" alt="image-20240330154215190" style="zoom: 67%;"></p>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;EPT-Net-Edge-Perception-Transformer-for-3D-Medical-Image-Segmentation&quot;&gt;&lt;a href=&quot;#EPT-Net-Edge-Perception-Transformer-for-3D-Medical-</summary>
      
    
    
    
    <category term="计算机视觉" scheme="https://wangak.cc/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/"/>
    
    
    <category term="计算机视觉" scheme="https://wangak.cc/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/"/>
    
    <category term="图像分割" scheme="https://wangak.cc/tags/%E5%9B%BE%E5%83%8F%E5%88%86%E5%89%B2/"/>
    
  </entry>
  
  <entry>
    <title>3D Medical image segmentation using parallel transformers</title>
    <link href="https://wangak.cc/posts/c50c9556.html"/>
    <id>https://wangak.cc/posts/c50c9556.html</id>
    <published>2024-03-18T16:00:00.000Z</published>
    <updated>2024-03-20T12:01:42.978Z</updated>
    
    <content type="html"><![CDATA[<h2 id="3D-Medical-image-segmentation-using-parallel-transformers"><a href="#3D-Medical-image-segmentation-using-parallel-transformers" class="headerlink" title="3D Medical image segmentation using parallel transformers"></a>3D Medical image segmentation using parallel transformers</h2><p><strong>TransHRNet:</strong>为并行连接不同分辨率的流而设计</p><p>本文的贡献点：</p><ul><li>提出了一种基于Transfomer的新型深度神经网络（TransHRNet)，将不同分辨率的数据流并行连接，并融合不同分辨率的信息。</li><li>引入EffTrans模块来提高性能</li></ul><p><strong>网络结构：</strong></p><p><img src="https://typoraimg.wangak.cc/2023/img/202403202000079.png" alt="image-20240319122733944" style="zoom: 50%;"></p><p>​        TransHRNet首先使用3D CNN生成X的紧凑特征表示来捕获空间和深度信息，在此过程中不断扩大感受野，并以不同的尺度对特征进行编码，但在此过程中仍然未充分利用图像的信息。作者提出了一个特征增强模块（EffTrans)，利用Transformer编码器来学习全局空间中的长距离依赖关系，同时并行连接不同的分辨率流，在分辨率上重复执行信息交互。之后，解码器通过上采样和卷积操作产生最后的分割结果。</p><p><strong>将图像转换为序列：</strong></p><p>将特征张量的空间和深度维度合并成一个维度，从而得到一个关于图像X的patch嵌入的1D序列。</p><p><strong>位置编码：</strong>使用正余弦位置编码</p><p><img src="https://typoraimg.wangak.cc/2023/img/202403202000117.png" alt="image-20240319130344786" style="zoom: 67%;"></p><p>注：<script type="math/tex">\large \#∈{D, H,W},v =1/10000^{2k\over{C/{3}}},pos为当前编码在序列中的位置，k为位置编码维度的索引</script></p><p><strong>特征融合：</strong>特征增强模块重复融合多分辨率特征，以跨多分辨率交换信息</p><p>三种并行方式：</p><p><img src="https://typoraimg.wangak.cc/2023/img/202403202000398.png" alt="image-20240319132030789" style="zoom:50%;"></p><p>注：高分辨率（绿色）、中分辨率（蓝色）、低分辨率（淡粉色）</p><ul><li>高分辨率与其他分辨率通过上采样得到的结果进行融合，得到新的高分辨率特征</li><li><p>中分辨率、下采样的高分辨率和上采样的低分辨率进行融合，可以获得新的中分辨率特征。</p></li><li><p>低分辨率与其他分辨率通过下采样生成得到的结果进行融合，生成新的低分辨率特征。</p></li></ul><p><strong>Effective Transformer：</strong></p><p>首先使用DeLighT变换对输入进行降维，然后采用Spatial-Reduction Attention (SRA)层进一步降低学习高分辨率特征图的资源成本。最后，将维数从<script type="math/tex">d_m</script>降维到<script type="math/tex">d_m/4</script>，然后将维数从<script type="math/tex">d_m/4</script>展开到<script type="math/tex">d_m</script>，具体结构如下图：</p><p><img src="https://typoraimg.wangak.cc/2023/img/202403202001898.png" alt="image-20240319202446776" style="zoom:67%;"></p>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;3D-Medical-image-segmentation-using-parallel-transformers&quot;&gt;&lt;a href=&quot;#3D-Medical-image-segmentation-using-parallel-transformers&quot; clas</summary>
      
    
    
    
    <category term="计算机视觉" scheme="https://wangak.cc/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/"/>
    
    
    <category term="计算机视觉" scheme="https://wangak.cc/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/"/>
    
    <category term="图像分割" scheme="https://wangak.cc/tags/%E5%9B%BE%E5%83%8F%E5%88%86%E5%89%B2/"/>
    
  </entry>
  
  <entry>
    <title>DELIGHT DEEP AND LIGHT-WEIGHT TRANSFORMER</title>
    <link href="https://wangak.cc/posts/16e5c57e.html"/>
    <id>https://wangak.cc/posts/16e5c57e.html</id>
    <published>2024-03-18T16:00:00.000Z</published>
    <updated>2024-09-10T13:45:17.060Z</updated>
    
    <content type="html"><![CDATA[<h2 id="DELIGHT"><a href="#DELIGHT" class="headerlink" title="DELIGHT"></a>DELIGHT</h2><p><strong>论文：《DELIGHT DEEP AND LIGHT-WEIGHT TRANSFORMER》</strong></p><p>DeLighT比标准的基于Transfomer的模型具有更少的参数，在每个Transformer块中使用DeLighT转换</p><p>DeLighT架构促使在Transformer中用单头注意力和轻量级前馈层替代多头注意力和前馈层，从而减少了总网络参数和运算。</p><p>注：在多头注意力中，每个注意力头都有自己的参数矩阵，因此总参数量较大。而使用单头注意力，参数矩阵只需一个，可以减少参数量。多头注意力需要对每个头进行独立计算，然后将它们合并，而单头注意力只需要进行一次计算，因此在计算上更加高效。</p><h3 id="DeLighT-Transformer"><a href="#DeLighT-Transformer" class="headerlink" title="DeLighT Transformer"></a><strong>DeLighT Transformer</strong></h3><p><strong>DeLighT变换：</strong>将一个<script type="math/tex">d_m</script>维的输入向量映射到一个高维空间（Expansion），然后通过组线性变换（GLT)将其将维到一个<script type="math/tex">d_o</script>维的输出向量（Reduction)。</p><p><img src="https://typoraimg.wangak.cc/2023/img/202403191940652.png" alt="image-20240319143848797"></p><p><em>注：组线性变换（GLT)用于降低Transformer模型的计算复杂度，分成N个组，每个组共享权重矩阵，通过输入的特定部分来输出局部的特征表示，其比线性更高效</em></p><p>为了学习全局表示，DeLighT变换使用特征重排在组线性变换中共享信息。</p><p><strong>模型表达能力的增加：</strong>传统Transformer增加输入维度<script type="math/tex">d_m</script>以增加表达力，而为了增加DeLighT块的表达力，不是增加输入维度<script type="math/tex">d_m</script>，而是通过Expansion和Reduction阶段增加中间DeLighT变换的深度和宽度。这使得我们可以使用更小的维度来计算注意力，从而需要更少的操作。</p><p>注：DeLighT变换的配置参数：GLTs的层数N、宽度乘法器<script type="math/tex">w_m</script>、输入维度<script type="math/tex">d_m</script>、输出维度<script type="math/tex">d_o</script>和GLTs的最大组数<script type="math/tex">g_{max}</script></p><p><strong>DeLighT变换具体步骤如下：</strong></p><ul><li>Expansion：将<script type="math/tex">d_m</script>维输入投影到高维空间，<script type="math/tex">d_{max}=w_md_m</script></li></ul><p><img src="https://typoraimg.wangak.cc/2023/img/202403191940643.png" alt="image-20240319151835407" style="zoom:50%;"></p><p><img src="https://typoraimg.wangak.cc/2023/img/202403191941700.png" alt="image-20240319153553608" style="zoom:50%;"></p><p><em>注：</em></p><ul><li><em><script type="math/tex">l</script>为GLT的层数，<script type="math/tex">g^l</script>表示<script type="math/tex">l</script>层的组数，W、b为可学习的权值和偏置</em></li><li><em><script type="math/tex">F</script>函数将输入分为<script type="math/tex">g^l</script>组然后进行线性变换得到Y</em></li><li><em><script type="math/tex">H</script>函数首先对<script type="math/tex">Y^{l-1}</script>​进行重排，来增加不同组间的信息的交互，然后将其和X进行混合</em></li></ul><p><strong>下图显示了DeLighT转换中的扩展阶段：</strong></p><p><img src="https://typoraimg.wangak.cc/2023/img/202403191940523.png" alt="image-20240319153106416"></p><p><strong>DeLighT Block：</strong>一个包含n个输入token的序列，每个标记的维度为<script type="math/tex">d_m</script>。这些n个<script type="math/tex">d_m</script>维的输入首先被送入DeLighT变换，产生n个维度为<script type="math/tex">d_o</script>的输出，其中<script type="math/tex">d_o < d_m</script>。这些n个<script type="math/tex">d_o</script>维的输出同时经过三个线性层投影，产生<script type="math/tex">d_o</script>维的查询Q、键K和值V</p><p><img src="https://typoraimg.wangak.cc/2023/img/202403191941128.png" alt="image-20240319160401455" style="zoom:50%;"></p><p><em>注：该模块将计算注意力的成本降低了<script type="math/tex">d_m/d_o</script>，作者取<script type="math/tex">d_o=d_m/2</script>​</em></p><p><strong>Light-weight FFN:</strong>r为降维因子，第一层将输入维度<script type="math/tex">d_m</script>降维到<script type="math/tex">d_m/r</script>，第二层将<script type="math/tex">d_m/r</script>再扩展到<script type="math/tex">d_m</script>​，相比于Transformer减少了FFN的参数和计算量</p><h3 id="块的扩展"><a href="#块的扩展" class="headerlink" title="块的扩展"></a><strong>块的扩展</strong></h3><p>与Transformer block相比，DeLighT block块的深度更深（N+4)</p><p>提高模型性能的方法通常通过<strong>增加模型维度（宽度缩放）、堆叠更多的块（深度缩放）</strong>，然而这种方式在小数据集上不是很有效。</p><p>作者认为这是因为缩放模型的宽度和深度在块之间是均匀分配参数的，这会导致模型学习到冗余的参数，于是作者<strong>将模型缩放扩展到块的级别</strong></p><p><img src="https://typoraimg.wangak.cc/2023/img/202403191941032.png" alt="image-20240319192955781" style="zoom: 50%;"></p><p><strong>缩放DeLighT块:</strong>其深度和宽度分别由两个配置参数控制:GLT层数<script type="math/tex">N</script>和宽度乘法器<script type="math/tex">w_m</script>，通过引入了逐块缩放，创建了一个具有可变大小的DeLighT块的网络，在输入附近分配较浅和较窄的DeLighT块，在输出附近分配较深和较宽的DeLighT块。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;DELIGHT&quot;&gt;&lt;a href=&quot;#DELIGHT&quot; class=&quot;headerlink&quot; title=&quot;DELIGHT&quot;&gt;&lt;/a&gt;DELIGHT&lt;/h2&gt;&lt;p&gt;&lt;strong&gt;论文：《DELIGHT DEEP AND LIGHT-WEIGHT TRANSFOR</summary>
      
    
    
    
    <category term="计算机视觉" scheme="https://wangak.cc/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/"/>
    
    
    <category term="计算机视觉" scheme="https://wangak.cc/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/"/>
    
    <category term="图像分割" scheme="https://wangak.cc/tags/%E5%9B%BE%E5%83%8F%E5%88%86%E5%89%B2/"/>
    
    <category term="Transformer" scheme="https://wangak.cc/tags/Transformer/"/>
    
  </entry>
  
  <entry>
    <title>VSA</title>
    <link href="https://wangak.cc/posts/73df481.html"/>
    <id>https://wangak.cc/posts/73df481.html</id>
    <published>2024-03-16T16:00:00.000Z</published>
    <updated>2024-03-19T04:14:29.444Z</updated>
    
    <content type="html"><![CDATA[<h2 id="VSA-Learning-Varied-Size-Window-Attention-in-Vision-Transformers"><a href="#VSA-Learning-Varied-Size-Window-Attention-in-Vision-Transformers" class="headerlink" title="VSA: Learning Varied-Size Window Attention in Vision Transformers"></a>VSA: Learning Varied-Size Window Attention in Vision Transformers</h2><p><strong>论文：《VSA: Learning Varied-Size Window Attention in Vision Transformers》（ECCV 2022)</strong></p><p>对于Swin Transfomer中的窗口注意力机制，如果窗口大小为可变的矩形窗口，其大小和位置直接从数据中学习，则transfomer可以从不同的窗口中捕获丰富的上下文，并学习更强大的对象特征表示。</p><p>VSA使用窗口回归模块根据每个默认窗口中的token来预测目标窗口的大小和位置</p><p><img src="https://typoraimg.wangak.cc/2023/img/202403191210588.png" alt="image-20240314104423831" style="zoom: 67%;"></p><p>VSA将可学习的可变大小的窗口注意力引入到Transformer中</p><p><strong>Varied-size window attention (VSA)的注意力机制:</strong>VSA允许查询令牌（query tokens）关注远处的区域，并赋予网络确定目标窗口大小（即注意力区域）的灵活性</p><ul><li><p>VSA先将输入特征划分为几个窗口，这些窗口的大小基于预定义的w，这样的窗口被称为<strong>默认窗口</strong></p></li><li><p>然后从默认窗口获取查询特征：通过一个线性变换来实现</p><p><img src="https://typoraimg.wangak.cc/2023/img/202403191210967.png" alt="image-20240315133836606" style="zoom:67%;"></p></li><li><p><strong>VSR模块:</strong>用于估计每个默认窗口的目标窗口的大小和位置</p></li></ul><p>​        包括一个平均池化层（average pooling layer）、一个LeakyReLU激活层和一个步长为1的1×1卷积层，按顺序排列。池化层的核大小和步幅遵循默认窗口的大小。</p><p><img src="https://typoraimg.wangak.cc/2023/img/202403191210935.png" alt="image-20240315133540975"></p><p><em>注1：<script type="math/tex">S_w</script>表示目标窗口相对于默认窗口位置的水平和垂直方向上的缩放比例,<script type="math/tex">O_w</script>是目标窗口相对于默认窗口的水平和垂直方向上的偏移量。</em></p><p><em>注2：<script type="math/tex">S_w、O_w\in R^{2×N}</script>,N为注意力头的个数</em></p><p><strong>VSA的计算过程：</strong></p><ul><li><p>首先，从特征图X得到K、V:</p><p><img src="https://typoraimg.wangak.cc/2023/img/202403191210047.png" alt="image-20240315135546889" style="zoom:67%;"></p><p><em>注：<script type="math/tex">K、V\in R^{H×W×C}</script></em></p></li><li><p>VSA模块分别在K、V上从每个不同大小的窗口（目标窗口）均匀采样M个特征，M设为w*w来使其计算的复杂度和窗口注意力相当。</p></li><li><p>对于Q、K、W进行注意力计算</p><p><em>注：由于K、V是从不同的位置采样得到的，因此使用相对位置嵌入可能无法很好地描述空间关系，所以在MHSA层之前采用<strong>条件位置嵌入（CPE)《Conditional positional encodings for vision transformers.（2021）》</strong>将空间关系提供给模型。</em></p></li></ul><p><img src="https://typoraimg.wangak.cc/2023/img/202403191210438.png" alt="image-20240315140839238" style="zoom:67%;"></p><p>​    <em>注：<script type="math/tex">Z^{l-1}</script>为前一个Transformer的输出特征，CPE由一个深度卷积层实现，其核大小设为窗口大小</em></p><p>模型的网络结构如下：<br><img src="https://typoraimg.wangak.cc/2023/img/202403191210313.png" alt="image-20240315142709174" style="zoom:67%;"></p><h3 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h3><p>对于语义分割任务：VSA模块可以提高baseline的性能<br><img src="https://typoraimg.wangak.cc/2023/img/202403191210600.png" alt="image-20240315143148652"></p>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;VSA-Learning-Varied-Size-Window-Attention-in-Vision-Transformers&quot;&gt;&lt;a href=&quot;#VSA-Learning-Varied-Size-Window-Attention-in-Vision-Tran</summary>
      
    
    
    
    <category term="计算机视觉" scheme="https://wangak.cc/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/"/>
    
    
    <category term="计算机视觉" scheme="https://wangak.cc/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/"/>
    
    <category term="图像分割" scheme="https://wangak.cc/tags/%E5%9B%BE%E5%83%8F%E5%88%86%E5%89%B2/"/>
    
  </entry>
  
  <entry>
    <title>TransNeXt</title>
    <link href="https://wangak.cc/posts/965265e1.html"/>
    <id>https://wangak.cc/posts/965265e1.html</id>
    <published>2024-03-07T16:00:00.000Z</published>
    <updated>2024-03-08T06:36:44.519Z</updated>
    
    <content type="html"><![CDATA[<h2 id="TransNeXt"><a href="#TransNeXt" class="headerlink" title="TransNeXt"></a>TransNeXt</h2><p><strong>论文：《TransNeXt: Robust Foveal Visual Perception for Vision Transformers》（CVPR 2024）</strong></p><h3 id="网络结构"><a href="#网络结构" class="headerlink" title="网络结构"></a>网络结<strong>构</strong></h3><p><img src="https://typoraimg.wangak.cc/2023/img/202403081436489.png" alt="image-20240308133954690"></p><p>像素聚焦注意力机制：在每个查询附近具有细粒度感知，同时保持对全局的粗粒度感知（结合了滑动窗口注意力和集中注意力）</p><p><strong>像素聚焦注意力(pixel-focused attention (PFA) )</strong>具体过程如下：</p><p><img src="https://typoraimg.wangak.cc/2023/img/202403081436294.png" alt="image-20240307093447355" style="zoom: 67%;"></p><p><em>注1：定义输入特征图上以 <script type="math/tex">（i, j）</script>为中心的滑动窗口中的像素集合为<script type="math/tex">ρ(i, j）</script> 。对于固定的窗口大小 <script type="math/tex">k×k</script> ， <script type="math/tex">||ρ(i, j)|| = k^2</script>。同时, 作者定义从特征图池化得到的像素集合为的 σ(<script type="math/tex">X</script>) 。对于池化大小 <script type="math/tex">Hp×Wp</script>，<script type="math/tex">||σ(X) || = Hp×Wp</script>。</em></p><p><em>注2：PFA由滑动窗口注意力和池化窗口注意力两部分组成</em></p><p><strong>池化：</strong>平均池化操作会严重丢失信息，所以在池化之前使用单层神经网络进行投影和激活，提前压缩提取有用的信息，从而提高下采样后的信息压缩率</p><p>下采样算子（Activate and Pool）如下：</p><p><img src="https://typoraimg.wangak.cc/2023/img/202403081436127.png" alt="image-20240307094953654" style="zoom: 67%;"></p><p><strong>LKV通过添加一个可学习的查询嵌入（QE)，来聚合多样的注意力：</strong></p><p><img src="https://typoraimg.wangak.cc/2023/img/202403081436731.png" alt="image-20240308093422092" style="zoom:67%;"></p><p><strong>QLV破坏了键和值之间的一对一对应关系，使当前查询学习更多的隐式相对位置信息</strong></p><p><img src="https://typoraimg.wangak.cc/2023/img/202403081436695.png" alt="image-20240308094738641" style="zoom:67%;"></p><p><strong>长度缩放余弦注意力:</strong>可以有效增强视觉模型的训练稳定性</p><p><img src="https://typoraimg.wangak.cc/2023/img/202403081436647.png" alt="image-20240308100847389" style="zoom:67%;"></p><p><em>注1：注意力设计应表现出熵不变性，以促进对未知长度的更好泛化，<script type="math/tex">τ log N</script>是为了保持熵的不变性和忽略常数项。</em></p><p><em>注2:<script type="math/tex">τ</script>是可学习参数，N表示每个查询交互的有效键的计数（在本文中N为<script type="math/tex">N(i,j) = ∥ρ(i, j)∥+∥σ(X)∥− ∥µ(i, j)∥</script>),<script type="math/tex">\hat{Q}、\hat{K}</script>是L2正则化后的结果</em></p><p><strong>位置偏置：</strong></p><ul><li><strong>池化特征路径：</strong>log-CPB方法，即使用一个具有 ReLU 激活函数的 2 层 MLP 来计算从查询点到空间相对坐标之间的位置偏差</li></ul><p><em>注：池化使得特征的空间信息变得模糊或丢失，直接使用可学习的位置偏差可能无法有效地捕获到像素级别的位置偏差，所以使用对数间隔连续位置偏差（log-CPB）方法，通过 MLP 网络计算位置偏差，可以更好地捕获到细粒度的位置信息，从而提高模型对多尺度图像输入的泛化能力。</em></p><ul><li><strong>滑动窗口路径</strong>：在这条路径上，作者直接使用了一个可学习的位置偏差 B(i,j)∼ρ(i,j)。</li></ul><p><strong>聚合像素聚焦注意力(AA)：</strong></p><p><img src="https://typoraimg.wangak.cc/2023/img/202403081436129.png" alt="image-20240308103811904" style="zoom: 67%;"></p><p>注：∆(i,j) ~ σ(X)为Q(i,j)和Kσ(X)之间的空间相对坐标</p><p><strong>卷积GLU通道混合器：</strong></p><p><img src="https://typoraimg.wangak.cc/2023/img/202403081436975.png" alt="image-20240308110209837"></p><p><strong>SE 机制</strong>的全局平均池过于粗粒度</p><p><strong>卷积GLU通道混合器</strong>中的通道注意力是基于每个像素点的最近邻特征计算的（深度卷积），而且其可以提供一定的位置信息</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;TransNeXt&quot;&gt;&lt;a href=&quot;#TransNeXt&quot; class=&quot;headerlink&quot; title=&quot;TransNeXt&quot;&gt;&lt;/a&gt;TransNeXt&lt;/h2&gt;&lt;p&gt;&lt;strong&gt;论文：《TransNeXt: Robust Foveal Visua</summary>
      
    
    
    
    <category term="计算机视觉" scheme="https://wangak.cc/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/"/>
    
    
    <category term="计算机视觉" scheme="https://wangak.cc/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/"/>
    
    <category term="图像分割" scheme="https://wangak.cc/tags/%E5%9B%BE%E5%83%8F%E5%88%86%E5%89%B2/"/>
    
  </entry>
  
</feed>
