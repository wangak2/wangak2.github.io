<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>Unet</title>
      <link href="/2023/09/03/Unet/"/>
      <url>/2023/09/03/Unet/</url>
      
        <content type="html"><![CDATA[<h2 id="Unet"><a href="#Unet" class="headerlink" title="Unet"></a>Unet</h2><h3 id="1-网络结构"><a href="#1-网络结构" class="headerlink" title="1.网络结构"></a>1.网络结构</h3><p><img src="/2023/09/03/Unet/1.png" alt="1"></p><p>网络左边为contracting path(收缩路径)，右边为expansive path(扩张路径)。</p><p><strong><em>注：</em></strong></p><p><strong><em>1.Contracting path为常规的3×3卷积结构，与ReLU，还有2×2的max pooling。每次下采样，都将featuremap的channel变为之前的两倍。</em></strong></p><p><strong><em>2.Expansive path为上采样过程，每次都是22的上采样卷积过程，并且将相应的feature channel减少为之前的一半。</em></strong></p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>FCN</title>
      <link href="/2023/09/02/FCN/"/>
      <url>/2023/09/02/FCN/</url>
      
        <content type="html"><![CDATA[<h3 id="1-FCN网络"><a href="#1-FCN网络" class="headerlink" title="1.FCN网络"></a>1.FCN网络</h3><h4 id="1-1-核心思想"><a href="#1-1-核心思想" class="headerlink" title="1.1 核心思想"></a>1.1 核心思想</h4><ul><li>不含全连接层的全卷积网络，可适应任意尺寸输入；（可以为不同大小和分辨率的图像生成像素级别的预测）</li><li>反卷积层增大图像尺寸，输出精细结果；</li><li>结合不同深度层结果的跳级结构，确保鲁棒性和精确性。</li></ul><h4 id="1-2-网络结构"><a href="#1-2-网络结构" class="headerlink" title="1.2 网络结构"></a>1.2 网络结构</h4><p><img src="/2023/09/02/FCN/1.png" alt="1"></p><p><strong><em>注：</em></strong></p><p><em>1.<strong>全卷积部分</strong>为一些经典的CNN网络（如VGG，ResNet等），用于提取特征</em>。</p><p><em>2.<strong>反卷积部分</strong>则是通过上采样得到原尺寸的语义分割图像。</em></p><p><em>3.FCN的<strong>输入</strong>可以为任意尺寸的彩色图像，<strong>输出</strong>与输入尺寸相同，通道数为n（目标类别数）+1（背景）。</em></p><h3 id="2-实例"><a href="#2-实例" class="headerlink" title="2.实例"></a>2.实例</h3><h4 id="2-1-创建一个全卷积网络"><a href="#2-1-创建一个全卷积网络" class="headerlink" title="2.1 创建一个全卷积网络"></a>2.1 创建一个全卷积网络</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line">%matplotlib inline</span><br><span class="line">import torch</span><br><span class="line">import torchvision</span><br><span class="line">from torch import nn</span><br><span class="line">from torch.nn import functional as F</span><br><span class="line">from d2l import torch as d2l</span><br><span class="line"></span><br><span class="line">#加载预训练的ResNet-18模型来提取图像特征，并查看该模型的最后三个子模块的结构和参数</span><br><span class="line">pretrained_net = torchvision.models.resnet18(pretrained=True)</span><br><span class="line">list(pretrained_net.children())[-3:]</span><br><span class="line"></span><br><span class="line">out:</span><br><span class="line">[Sequential(</span><br><span class="line">   (0): BasicBlock(</span><br><span class="line">     (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)</span><br><span class="line">     (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)</span><br><span class="line">     (relu): ReLU(inplace=True)</span><br><span class="line">     (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)</span><br><span class="line">     (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)</span><br><span class="line">     (downsample): Sequential(</span><br><span class="line">       (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)</span><br><span class="line">       (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)</span><br><span class="line">     )</span><br><span class="line">   )</span><br><span class="line">   (1): BasicBlock(</span><br><span class="line">     (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)</span><br><span class="line">     (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)</span><br><span class="line">     (relu): ReLU(inplace=True)</span><br><span class="line">     (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)</span><br><span class="line">     (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)</span><br><span class="line">   )</span><br><span class="line"> ),</span><br><span class="line"> AdaptiveAvgPool2d(output_size=(1, 1)),</span><br><span class="line"> Linear(in_features=512, out_features=1000, bias=True)]</span><br></pre></td></tr></table></figure><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">#利用ResNet-18模型创建一个全卷积网络实例net(去除ResNet-18模型的池化层和全连接层)</span><br><span class="line">net = nn.Sequential(*list(pretrained_net.children())[:-2])</span><br><span class="line"></span><br><span class="line">X = torch.rand(size=(1, 3, 320, 480))</span><br><span class="line">net(X).shape #对张量X进行前向传播</span><br><span class="line"></span><br><span class="line">out:</span><br><span class="line">torch.Size([1, 512, 10, 15])</span><br></pre></td></tr></table></figure><h4 id="2-2-添加1x1的卷积层和转置卷积层"><a href="#2-2-添加1x1的卷积层和转置卷积层" class="headerlink" title="2.2 添加1x1的卷积层和转置卷积层"></a>2.2 添加1x1的卷积层和转置卷积层</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">#向网络net中添加final_conv和transpose_conv两个层，特征图的分辨率还原回输入图像的大小</span><br><span class="line">num_classes = 21</span><br><span class="line">net.add_module(&#x27;final_conv&#x27;, nn.Conv2d(512, num_classes, kernel_size=1))</span><br><span class="line">net.add_module(</span><br><span class="line">    &#x27;transpose_conv&#x27;,</span><br><span class="line">    nn.ConvTranspose2d(num_classes, num_classes, kernel_size=64, padding=16,</span><br><span class="line">                       stride=32))#kernel_size取stride的两倍，padding取kernel_size的1/4</span><br><span class="line">net(X).shape</span><br><span class="line"></span><br><span class="line">out:</span><br><span class="line">torch.Size([1, 21, 320, 480])</span><br></pre></td></tr></table></figure><h4 id="2-3-初始化卷积核"><a href="#2-3-初始化卷积核" class="headerlink" title="2.3 初始化卷积核"></a>2.3 初始化卷积核</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">#初始化转置卷积层</span><br><span class="line">def bilinear_kernel(in_channels, out_channels, kernel_size):</span><br><span class="line">    #找到卷积核的中心位置</span><br><span class="line">    factor = (kernel_size + 1) // 2    </span><br><span class="line">    if kernel_size % 2 == 1:</span><br><span class="line">        center = factor - 1</span><br><span class="line">    else:</span><br><span class="line">        center = factor - 0.5</span><br><span class="line">    og = (torch.arange(kernel_size).reshape(-1, 1),</span><br><span class="line">          torch.arange(kernel_size).reshape(1, -1))</span><br><span class="line">    #权重矩阵 filt,通过将偏移与 factor 相除并从1中减去得到的,确保了中心位置附近的权重最大</span><br><span class="line">    filt = (1 - torch.abs(og[0] - center) / factor) *\</span><br><span class="line">           (1 - torch.abs(og[1] - center) / factor)</span><br><span class="line">    weight = torch.zeros(</span><br><span class="line">        (in_channels, out_channels, kernel_size, kernel_size))</span><br><span class="line">    weight[range(in_channels), range(out_channels), :, :] = filt</span><br><span class="line">    return weight</span><br></pre></td></tr></table></figure><h4 id="2-4-双线性插值的上采样的实验"><a href="#2-4-双线性插值的上采样的实验" class="headerlink" title="2.4 双线性插值的上采样的实验"></a>2.4 双线性插值的上采样的实验</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">conv_trans = nn.ConvTranspose2d(3, 3, kernel_size=4, padding=1, stride=2,</span><br><span class="line">                                bias=False)#kernel_size取stride的两倍，padding取kernel_size的1/4</span><br><span class="line">conv_trans.weight.data.copy_(bilinear_kernel(3, 3, 4));</span><br><span class="line"></span><br><span class="line">img = torchvision.transforms.ToTensor()(d2l.Image.open(&#x27;../data/cat.jpg&#x27;))#加载一张图像并将其转换为 PyTorch 张量格式。</span><br><span class="line">X = img.unsqueeze(0)#unsqueeze(0)的作用是在索引0的位置插入一个新的维度，将原始的三维图像张量变成了四维</span><br><span class="line">Y = conv_trans(X)#对X进行转置卷积</span><br><span class="line">out_img = Y[0].permute(1, 2, 0).detach()#删除一个维度，并将其他的三个维度调整为高度（Height）、宽度（Width）和通道数（Channels）的顺序</span><br><span class="line">d2l.set_figsize()</span><br><span class="line">print(&#x27;input image shape:&#x27;, img.permute(1, 2, 0).shape)</span><br><span class="line">d2l.plt.imshow(img.permute(1, 2, 0))</span><br><span class="line">print(&#x27;output image shape:&#x27;, out_img.shape)</span><br><span class="line">d2l.plt.imshow(out_img);</span><br></pre></td></tr></table></figure><p><img src="/2023/09/02/FCN/2.png" alt="2"></p><h4 id="2-5-初始化转置卷积层"><a href="#2-5-初始化转置卷积层" class="headerlink" title="2.5 初始化转置卷积层"></a>2.5 初始化转置卷积层</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">W = bilinear_kernel(num_classes, num_classes, 64)</span><br><span class="line">net.transpose_conv.weight.data.copy_(W);</span><br></pre></td></tr></table></figure><h4 id="2-6-读取数据集"><a href="#2-6-读取数据集" class="headerlink" title="2.6 读取数据集"></a>2.6 读取数据集</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">batch_size, crop_size = 32, (320, 480)</span><br><span class="line">train_iter, test_iter = d2l.load_data_voc(batch_size, crop_size)</span><br></pre></td></tr></table></figure><h4 id="2-7-训练"><a href="#2-7-训练" class="headerlink" title="2.7 训练"></a>2.7 训练</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">def loss(inputs, targets):</span><br><span class="line">    return F.cross_entropy(inputs, targets, reduction=&#x27;none&#x27;).mean(1).mean(1)</span><br><span class="line"></span><br><span class="line">num_epochs, lr, wd, devices = 5, 0.001, 1e-3, d2l.try_all_gpus()</span><br><span class="line">trainer = torch.optim.SGD(net.parameters(), lr=lr, weight_decay=wd)</span><br><span class="line">d2l.train_ch13(net, train_iter, test_iter, loss, trainer, num_epochs, devices)</span><br></pre></td></tr></table></figure><p><img src="/2023/09/02/FCN/3.png" alt="3"></p><h4 id="2-8-可视化预测的类别"><a href="#2-8-可视化预测的类别" class="headerlink" title="2.8 可视化预测的类别"></a>2.8 可视化预测的类别</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">def predict(img):</span><br><span class="line">    X = test_iter.dataset.normalize_image(img).unsqueeze(0)</span><br><span class="line">    pred = net(X.to(devices[0])).argmax(dim=1)</span><br><span class="line">    return pred.reshape(pred.shape[1], pred.shape[2])</span><br><span class="line">    </span><br><span class="line">def label2image(pred):</span><br><span class="line">    colormap = torch.tensor(d2l.VOC_COLORMAP, device=devices[0])</span><br><span class="line">    X = pred.long()</span><br><span class="line">    return colormap[X, :]</span><br><span class="line"></span><br><span class="line">voc_dir = d2l.download_extract(&#x27;voc2012&#x27;, &#x27;VOCdevkit/VOC2012&#x27;)</span><br><span class="line">test_images, test_labels = d2l.read_voc_images(voc_dir, False)</span><br><span class="line">n, imgs = 4, []</span><br><span class="line">for i in range(n):</span><br><span class="line">    crop_rect = (0, 0, 320, 480)</span><br><span class="line">    X = torchvision.transforms.functional.crop(test_images[i], *crop_rect)</span><br><span class="line">    pred = label2image(predict(X))</span><br><span class="line">    imgs += [</span><br><span class="line">        X.permute(1, 2, 0),</span><br><span class="line">        pred.cpu(),</span><br><span class="line">        torchvision.transforms.functional.crop(test_labels[i],</span><br><span class="line">                                               *crop_rect).permute(1, 2, 0)]</span><br><span class="line">d2l.show_images(imgs[::3] + imgs[1::3] + imgs[2::3], 3, n, scale=2);</span><br></pre></td></tr></table></figure><p><img src="/2023/09/02/FCN/4.png" alt="4"></p>]]></content>
      
      
      <categories>
          
          <category> 计算机视觉 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> FCN </tag>
            
            <tag> 计算机视觉 </tag>
            
            <tag> 图像分割 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>图像分割基础</title>
      <link href="/2023/08/30/%E5%9B%BE%E5%83%8F%E5%88%86%E5%89%B2%E5%9F%BA%E7%A1%80/"/>
      <url>/2023/08/30/%E5%9B%BE%E5%83%8F%E5%88%86%E5%89%B2%E5%9F%BA%E7%A1%80/</url>
      
        <content type="html"><![CDATA[<h3 id="1-图像分割"><a href="#1-图像分割" class="headerlink" title="1.图像分割"></a>1.图像分割</h3><p><strong>语义分割：</strong>为每个像素都打上标签，只区分类别，但不区分类别中的具体单位。</p><p><strong>实例分割：</strong>不光要区别类别，还要区分类别中的每一个个体。</p><h3 id="2-Focal-loss"><a href="#2-Focal-loss" class="headerlink" title="2.Focal loss"></a>2.Focal loss</h3><p><strong>focal loss从样本难易分类角度出发，解决样本非平衡带来的模型训练问题。</strong></p><p><strong>focal loss的具体形式：</strong>$\large-\alpha(1-y<em>{pred})^{\gamma}y</em>{true}log(y<em>{pred})-(1-\alpha)y</em>{pred}^{\gamma}(1-y<em>{true})log(1-y</em>{pred})$</p><p><strong>注：</strong></p><p><em>1.$\large\gamma$通常设置为2，$\large(1-y_{pred})^{\gamma}$相当于样本的难易度权值,$\large\alpha$为正负样本的比例</em></p><p><em>2.为了防止难易样本的频繁变化，应当选取小的学习率。防止学习率过大，造成w变化较大从而引起 $\large y_{pred}$的巨大变化，造成难易样本的改变。</em></p><h3 id="3-转置卷积"><a href="#3-转置卷积" class="headerlink" title="3.转置卷积"></a>3.转置卷积</h3><p><strong>转置卷积：</strong>用来增大输入的高宽</p><h4 id="3-1-转置卷积的计算方式"><a href="#3-1-转置卷积的计算方式" class="headerlink" title="3.1 转置卷积的计算方式"></a>3.1 转置卷积的计算方式</h4><p><strong>计算方式一：</strong></p><p><img src="/2023/08/30/%E5%9B%BE%E5%83%8F%E5%88%86%E5%89%B2%E5%9F%BA%E7%A1%80/1.png" alt="1"></p><p><strong>计算方式二（填充为0，步幅为1)：</strong></p><p><img src="/2023/08/30/%E5%9B%BE%E5%83%8F%E5%88%86%E5%89%B2%E5%9F%BA%E7%A1%80/2.png" alt="2"></p><p><strong>一般情况：</strong></p><p><img src="/2023/08/30/%E5%9B%BE%E5%83%8F%E5%88%86%E5%89%B2%E5%9F%BA%E7%A1%80/3.png" alt="3"></p><p><strong>基本的转置卷积运算：</strong></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">def trans_conv(X, K):</span><br><span class="line">    h, w = K.shape</span><br><span class="line">    Y = torch.zeros((X.shape[0] + h - 1, X.shape[1] + w - 1))</span><br><span class="line">    for i in range(X.shape[0]):</span><br><span class="line">        for j in range(X.shape[1]):</span><br><span class="line">            Y[i:i + h, j:j + w] += X[i, j] * K</span><br><span class="line">    return Y</span><br></pre></td></tr></table></figure><p><strong>高级API:</strong></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">X = torch.tensor([[0.0, 1.0], [2.0, 3.0]])</span><br><span class="line">K = torch.tensor([[0.0, 1.0], [2.0, 3.0]])</span><br><span class="line">X, K = X.reshape(1, 1, 2, 2), K.reshape(1, 1, 2, 2)</span><br><span class="line">tconv = nn.ConvTranspose2d(1, 1, kernel_size=2, bias=False)</span><br><span class="line">tconv.weight.data = K</span><br><span class="line">tconv(X)</span><br><span class="line"></span><br><span class="line">out:</span><br><span class="line">tensor([[[[ 0.,  0.,  1.],</span><br><span class="line">          [ 0.,  4.,  6.],</span><br><span class="line">          [ 4., 12.,  9.]]]], grad_fn=&lt;ConvolutionBackward0&gt;)</span><br></pre></td></tr></table></figure><p><strong>填充：</strong></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">tconv = nn.ConvTranspose2d(1, 1, kernel_size=2, padding=1, bias=False)#padding=1将输出的行列减小1</span><br><span class="line">tconv.weight.data = K</span><br><span class="line">tconv(X)</span><br><span class="line"></span><br><span class="line">out:</span><br><span class="line">tensor([[[[4.]]]], grad_fn=&lt;ConvolutionBackward0&gt;)</span><br></pre></td></tr></table></figure><p><strong>步幅:</strong></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">tconv = nn.ConvTranspose2d(1, 1, kernel_size=2, stride=2, bias=False)#增大stride会将输出变大</span><br><span class="line">tconv.weight.data = K</span><br><span class="line">tconv(X)</span><br><span class="line"></span><br><span class="line">out:</span><br><span class="line">tensor([[[[0., 0., 0., 1.],</span><br><span class="line">          [0., 0., 2., 3.],</span><br><span class="line">          [0., 2., 0., 3.],</span><br><span class="line">          [4., 6., 6., 9.]]]], grad_fn=&lt;ConvolutionBackward0&gt;)</span><br></pre></td></tr></table></figure><p><strong>多通道:</strong></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">X = torch.rand(size=(1, 10, 16, 16))</span><br><span class="line">conv = nn.Conv2d(10, 20, kernel_size=5, padding=2, stride=3)</span><br><span class="line">tconv = nn.ConvTranspose2d(20, 10, kernel_size=5, padding=2, stride=3)</span><br><span class="line">tconv(conv(X)).shape == X.shape</span><br><span class="line"></span><br><span class="line">out:</span><br><span class="line">True</span><br></pre></td></tr></table></figure><h4 id="3-2-转置卷积的棋盘效应"><a href="#3-2-转置卷积的棋盘效应" class="headerlink" title="3.2 转置卷积的棋盘效应"></a>3.2 转置卷积的棋盘效应</h4><p><strong>棋盘效应：</strong>在转置卷积操作中，生成的输出特征图中出现的不规则、重叠和错位的图案，类似于棋盘格子</p><p><strong>棋盘效应的主要原因：</strong>kernel size不能够被stride整除（<strong>不均匀重叠</strong>）</p><p><strong>解决方法：</strong></p><p><strong>方法一：</strong>使kernel size能够被stride整除（无法完全避免该问题）</p><p><strong>方法二：</strong>插值法+卷积</p><h3 id="4-双线性插值"><a href="#4-双线性插值" class="headerlink" title="4.双线性插值"></a>4.双线性插值</h3><p><strong>双线性插值：</strong>通过待求像素点在源图像中4个最近邻像素值的加权和计算得到</p><p><img src="/2023/08/30/%E5%9B%BE%E5%83%8F%E5%88%86%E5%89%B2%E5%9F%BA%E7%A1%80/4.png" alt="4"></p><p><strong>优势：</strong>没有灰度不连续的缺点，结果基本令人满意</p><p><strong>劣势：</strong>双线性内插法的计算比最邻近点法复杂，计算量较大。它具有低通滤波性质，使高频分量受损，图像轮廓可能会有一点模糊。</p><h3 id="5-Batch-Normalization"><a href="#5-Batch-Normalization" class="headerlink" title="5.Batch Normalization"></a>5.Batch Normalization</h3><p><strong>优点：</strong><br>1.可以用更大学习率，加速模型收敛<br>2.可以不用精心设计权值初始化<br>3.可以不用dropout或较小的dropout<br>4.可以不用L2或者较小的weight decay<br>5.可以不用LRN(local response normalization)</p><p><img src="/2023/08/30/%E5%9B%BE%E5%83%8F%E5%88%86%E5%89%B2%E5%9F%BA%E7%A1%80/5.png" alt="5"></p><p><strong><em>注：</em></strong></p><p><strong><em>1.由于归一化后的xi基本会被限制在正态分布下，使得网络的表达能力下降。为解决该问题，我们引入两个新的参数：γ,β。 γ和β是在训练时网络自己学习得到的。</em></strong></p><p><em>2.Batch Normalization将数据移到激活函数中心区域，对于大多数的激活函数而言，这个区域的梯度都是最大的或者是有梯度的，因而这种方法是一种对抗梯度消失的有效手段。</em></p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title></title>
      <link href="/2023/08/11/c++%E5%9F%BA%E7%A1%80%EF%BC%882%EF%BC%89/"/>
      <url>/2023/08/11/c++%E5%9F%BA%E7%A1%80%EF%BC%882%EF%BC%89/</url>
      
        <content type="html"><![CDATA[<h3 id="1-复制函数"><a href="#1-复制函数" class="headerlink" title="1.复制函数"></a>1.复制函数</h3><h4 id="1-1-浅复制"><a href="#1-1-浅复制" class="headerlink" title="1.1 浅复制"></a>1.1 浅复制</h4><p><strong>浅复制：</strong>仅复制对象的成员变量的值，而不复制对象中的指针所指向的内容。</p><p><em>注：如果原始对象中包含指针，浅复制将导致多个对象共享同一内存块，从而可能引发潜在的问题。当原始对象的析构函数被调用时，如果没有适当地管理共享资源，可能会导致重复释放内存或内存泄漏等问题。</em></p><h4 id="1-2-深复制"><a href="#1-2-深复制" class="headerlink" title="1.2 深复制"></a>1.2 深复制</h4><p><strong>深复制：</strong>不仅复制对象的成员变量的值，还要递归地复制对象中的指针所指向的内容，创建一个全新的数据拷贝。</p><p><em>注：新对象与原始对象彼此独立，<strong>不共享内存块</strong>，但需要更多的计算和内存开销。</em></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line">#include &lt;string.h&gt; </span><br><span class="line">#include&lt;iostream&gt;</span><br><span class="line">using namespace std; </span><br><span class="line">class String&#123;</span><br><span class="line">  private:</span><br><span class="line">    char *data;</span><br><span class="line">  public:</span><br><span class="line">    String(const char *str)</span><br><span class="line">    &#123;</span><br><span class="line">      data=new char[strlen(str)]+1;</span><br><span class="line">      strcpy(data,str);</span><br><span class="line">    &#125;</span><br><span class="line">    // String(const String &amp;other)//浅复制</span><br><span class="line">    // &#123;</span><br><span class="line">    //   data=other.data;</span><br><span class="line">    // &#125;</span><br><span class="line">    String(const String &amp;other)//深复制</span><br><span class="line">    &#123;</span><br><span class="line">      data=new char[strlen(other.data)+1];</span><br><span class="line">      strcpy(data,other.data);</span><br><span class="line">    &#125;</span><br><span class="line">    ~String()</span><br><span class="line">    &#123;</span><br><span class="line">      delete []data;</span><br><span class="line">    &#125;</span><br><span class="line">    void printData()</span><br><span class="line">    &#123;</span><br><span class="line">      cout&lt;&lt;data&lt;&lt;endl;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br><span class="line">int main()</span><br><span class="line">&#123;</span><br><span class="line">  String Str0(&quot;Hello&quot;);</span><br><span class="line">  String Str1(Str0);</span><br><span class="line">  Str1.printData();</span><br><span class="line">  return 0;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>C++基础（1）</title>
      <link href="/2023/08/08/c++%E5%9F%BA%E7%A1%80%EF%BC%881%EF%BC%89/"/>
      <url>/2023/08/08/c++%E5%9F%BA%E7%A1%80%EF%BC%881%EF%BC%89/</url>
      
        <content type="html"><![CDATA[<h3 id="1-常量"><a href="#1-常量" class="headerlink" title="1.常量"></a>1.常量</h3><h4 id="1-1-使用-const-将变量声明为常量"><a href="#1-1-使用-const-将变量声明为常量" class="headerlink" title="1.1 使用 const 将变量声明为常量"></a>1.1 使用 const 将变量声明为常量</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">const double pi = 3.14;</span><br></pre></td></tr></table></figure><h4 id="1-2-使用-constexpr-定义常量表达式"><a href="#1-2-使用-constexpr-定义常量表达式" class="headerlink" title="1.2 使用 constexpr 定义常量表达式"></a>1.2 使用 constexpr 定义常量表达式</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">constexpr double GetPi() &#123;return 3.14;&#125;</span><br><span class="line">constexpr double TwicePi()&#123;return 2* GetPi();&#125;</span><br></pre></td></tr></table></figure><h4 id="1-3-使用关键字-enum-声明的枚举常量"><a href="#1-3-使用关键字-enum-声明的枚举常量" class="headerlink" title="1.3 使用关键字 enum 声明的枚举常量"></a>1.3 使用关键字 enum 声明的枚举常量</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">#include&lt;bits/stdc++.h&gt;</span><br><span class="line">using namespace std;</span><br><span class="line">enum direct&#123;</span><br><span class="line">        North=5,</span><br><span class="line">        South,</span><br><span class="line">        East,</span><br><span class="line">        West</span><br><span class="line">    &#125;;</span><br><span class="line">int main()</span><br><span class="line">&#123;</span><br><span class="line">    direct f=North;</span><br><span class="line">    cout&lt;&lt;f&lt;&lt;endl;</span><br><span class="line">    cout&lt;&lt;South&lt;&lt;endl;</span><br><span class="line">    cout&lt;&lt;East&lt;&lt;endl;</span><br><span class="line">    cout&lt;&lt;West&lt;&lt;endl;</span><br><span class="line">    return 0;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">out:</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td></tr></table></figure><p><strong>注：</strong></p><p><em>1.使用枚举来指定变量的类型，这样声明的变量只能取指定的值</em>。</p><p><em>2.North的值设置为5，这自动将随后的常量分别设置为 6、7 和8。</em></p><h4 id="1-4-使用-define-定义常量"><a href="#1-4-使用-define-定义常量" class="headerlink" title="1.4 使用#define 定义常量"></a>1.4 使用#define 定义常量</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">#define pi 3.14</span><br></pre></td></tr></table></figure><p><strong>注：</strong></p><p><em>1.#define 是一个预处理器宏，让预处理器将随后出现的所有 pi 都替换为 3.14。</em></p><p><em>2.使用#define 定义常量的做法已被摒弃，因此不应采用这种做法。</em></p><h3 id="2-动态数组"><a href="#2-动态数组" class="headerlink" title="2.动态数组"></a>2.动态数组</h3><p><strong>std::vector:</strong>支持在运行阶段根据需要增大动态数组</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">#include&lt;bits/stdc++.h&gt;</span><br><span class="line">using namespace std;</span><br><span class="line">int main()</span><br><span class="line">&#123;</span><br><span class="line">    vector&lt;int&gt; a(3);</span><br><span class="line">    a[0]=1,a[1]=2,a[2]=3;</span><br><span class="line">    printf(&quot;%d\n&quot;,a.size());</span><br><span class="line">    a.push_back(4);</span><br><span class="line">    printf(&quot;%d&quot;,a.size());</span><br><span class="line">    return 0;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">out:</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td></tr></table></figure><h3 id="3-Lambda-表达式"><a href="#3-Lambda-表达式" class="headerlink" title="3.Lambda 表达式"></a>3.Lambda 表达式</h3><p><strong>[capture](parameters) -&gt; return_type { body }</strong></p><ul><li><code>capture</code>：捕获列表，用于指定在 lambda 表达式中可见的外部变量。可以通过值捕获或引用捕获来捕获变量。</li><li><code>parameters</code>：参数列表，与普通函数的参数列表一样，用于传递参数给 lambda 表达式。</li><li><code>return_type</code>：返回类型，指定 lambda 表达式的返回类型。可以省略，编译器会自动推断返回类型。</li><li><code>body</code>：函数体，包含 lambda 表达式的实际操作。</li></ul><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">#include&lt;bits/stdc++.h&gt;</span><br><span class="line">using namespace std;</span><br><span class="line">int main()</span><br><span class="line">&#123;</span><br><span class="line">#lambda表达式接受两个整数参数x和y，然后返回它们的和</span><br><span class="line">    auto add=[](int x,int y)-&gt;int&#123;</span><br><span class="line">        return x+y;</span><br><span class="line">    &#125;;</span><br><span class="line">    int ans=add(2,3);</span><br><span class="line">    printf(&quot;%d\n&quot;,ans);</span><br><span class="line">    int x=2,y=3;</span><br><span class="line">    #这个lambda表达式没有参数，并且在捕获列表中捕获了变量x和y，返回x*y的结果</span><br><span class="line">    auto mul=[x,y]()-&gt;int&#123;</span><br><span class="line">        return x*y;</span><br><span class="line">    &#125;;</span><br><span class="line">    int res=mul();</span><br><span class="line">    printf(&quot;%d&quot;,res);</span><br><span class="line">    return 0;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="4-动态内存分配"><a href="#4-动态内存分配" class="headerlink" title="4.动态内存分配"></a>4.动态内存分配</h3><p><strong>使用 new  动态地分配内存</strong></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Type* Pointer = new Type;</span><br><span class="line">Type* Pointer = new Type[numElements];</span><br></pre></td></tr></table></figure><p><strong>最终都需使用对应的 delete 进行释放:</strong></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">delete Pointer;</span><br><span class="line">delete[] Pointer;</span><br></pre></td></tr></table></figure><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">#include&lt;bits/stdc++.h&gt;</span><br><span class="line">using namespace std;</span><br><span class="line">int main()</span><br><span class="line">&#123;</span><br><span class="line">    int *x=new int;</span><br><span class="line">    scanf(&quot;%d&quot;,x);</span><br><span class="line">    printf(&quot;%d&quot;,*x);</span><br><span class="line">    delete x;</span><br><span class="line">    return 0;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h4 id="5-将关键字-const-用于指针"><a href="#5-将关键字-const-用于指针" class="headerlink" title="5.将关键字 const 用于指针"></a>5.将关键字 const 用于指针</h4><h4 id="5-1-地址不能修改-数据可修改"><a href="#5-1-地址不能修改-数据可修改" class="headerlink" title="5.1 地址不能修改,数据可修改"></a>5.1 地址不能修改,数据可修改</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">int* const pDaysInMonth = &amp;daysInMonth;</span><br></pre></td></tr></table></figure><h4 id="5-2-地址可修改-数据不能修改"><a href="#5-2-地址可修改-数据不能修改" class="headerlink" title="5.2 地址可修改,数据不能修改"></a>5.2 地址可修改,数据不能修改</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">const int* pointsToInt = &amp;hoursInDay;</span><br></pre></td></tr></table></figure><h4 id="5-3-地址和数据均不能修改"><a href="#5-3-地址和数据均不能修改" class="headerlink" title="5.3 地址和数据均不能修改"></a>5.3 地址和数据均不能修改</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">const int* const pHoursInDay = &amp;hoursInDay;</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> C++ </category>
          
      </categories>
      
      
        <tags>
            
            <tag> C++ </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>torchvision.transforms使用详解</title>
      <link href="/2023/08/03/torchvision-transforms%E4%BD%BF%E7%94%A8%E8%AF%A6%E8%A7%A3/"/>
      <url>/2023/08/03/torchvision-transforms%E4%BD%BF%E7%94%A8%E8%AF%A6%E8%A7%A3/</url>
      
        <content type="html"><![CDATA[<h2 id="torchvision-transforms"><a href="#torchvision-transforms" class="headerlink" title="torchvision.transforms"></a>torchvision.transforms</h2><h3 id="1-torchvision-transforms-Compose"><a href="#1-torchvision-transforms-Compose" class="headerlink" title="1. torchvision.transforms.Compose"></a>1. torchvision.transforms.Compose</h3><p><strong>torchvision.transforms.Compose：</strong>是一个用于组合多个图像预处理操作的类，将多个预处理操作串联在一起，以便在数据加载时对图像进行连续的处理。</p><p><strong>torchvision.transforms.Compose(transforms)</strong> 的参数是一个列表，其中包含要进行的图像预处理操作。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">import torchvision.transforms as transforms</span><br><span class="line"></span><br><span class="line"># 定义图像预处理操作</span><br><span class="line">transformations = transforms.Compose([</span><br><span class="line">    transforms.Resize(256),              # 调整图像大小为256x256</span><br><span class="line">    transforms.RandomCrop(224),          # 随机裁剪为224x224</span><br><span class="line">    transforms.RandomHorizontalFlip(),   # 随机水平翻转</span><br><span class="line">    transforms.ToTensor(),               # 将图像转换为Tensor格式</span><br><span class="line">    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # 归一化</span><br><span class="line">])</span><br><span class="line"></span><br><span class="line"># 使用组合的预处理操作对图像进行处理</span><br><span class="line">img = Image.open(&#x27;example.jpg&#x27;)  # 假设有一张名为 &#x27;example.jpg&#x27; 的图像</span><br><span class="line">processed_img = transformations(img)</span><br></pre></td></tr></table></figure><h3 id="2-torchvision-transforms-RandomResizedCrop"><a href="#2-torchvision-transforms-RandomResizedCrop" class="headerlink" title="2.torchvision.transforms.RandomResizedCrop"></a>2.torchvision.transforms.RandomResizedCrop</h3><p><strong>torchvision.transforms.RandomResizedCrop：</strong>用于<strong>数据增广</strong>，以增加数据集的<strong>多样性</strong>和提高模型的<strong>泛化能力</strong>（随机裁剪图像，并将裁剪后的图像调整为指定的大小）</p><p><code>torchvision.transforms.RandomResizedCrop(size, scale=(0.08, 1.0), ratio=(3.0/4.0, 4.0/3.0), interpolation=2)</code> 的参数包括：</p><ul><li><p><code>size</code>：输出的裁剪后图像的大小，可以是一个整数或一个元组 (height, width)。</p></li><li><p><code>scale</code>：控制裁剪区域相对于原始图像大小的尺度范围，它是一个长度为2的元组 (min_scale, max_scale)。裁剪区域的大小在 [min_scale <em> 图像大小, max_scale </em> 图像大小] 之间随机选择。</p></li><li><p><code>ratio</code>：控制裁剪区域的宽高比范围，它是一个长度为2的元组 (min_ratio, max_ratio)。裁剪区域的宽高比在 [min_ratio, max_ratio] 之间随机选择。</p></li><li><p><code>interpolation</code>：插值方法，用于调整裁剪后图像的大小。默认值为2，表示使用双线性插值。</p><p><em>注：在图像处理中，将图像从一个尺寸调整为另一个尺寸时，通常需要使用插值方法来计算新尺寸的像素值。</em></p></li></ul><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">import torchvision.transforms as transforms</span><br><span class="line">from PIL import Image</span><br><span class="line">import matplotlib.pyplot as plt</span><br><span class="line"># 定义随机裁剪操作</span><br><span class="line">transformations = transforms.RandomResizedCrop(size=224, scale=(0.08, 1.0), ratio=(3.0/4.0, 4.0/3.0))</span><br><span class="line"></span><br><span class="line"># 使用随机裁剪操作对图像进行处理</span><br><span class="line">img = Image.open(&#x27;example.jpg&#x27;)  # 假设有一张名为 &#x27;example.jpg&#x27; 的图像</span><br><span class="line">processed_img = transformations(img)</span><br><span class="line">plt.imshow(processed_img)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><h3 id="3-torchvision-transforms-RandomHorizontalFlip"><a href="#3-torchvision-transforms-RandomHorizontalFlip" class="headerlink" title="3.torchvision.transforms.RandomHorizontalFlip()"></a>3.torchvision.transforms.RandomHorizontalFlip()</h3><p><strong>torchvision.transforms.RandomHorizontalFlip():</strong>对图像进行随机水平翻转，增加数据集的多样性和提高模型的鲁棒性。</p><p><code>torchvision.transforms.RandomHorizontalFlip(p=0.5)</code> 的参数 <code>p</code> （默认为0.5）控制水平翻转的概率。</p><p>当 <code>p=0.5</code> 时，有50%的概率对图像进行水平翻转。当 <code>p=0</code> 时，不进行翻转；当 <code>p=1</code> 时，100%进行翻转。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">import torchvision.transforms as transforms</span><br><span class="line"></span><br><span class="line"># 定义随机水平翻转操作</span><br><span class="line">transformations = transforms.RandomHorizontalFlip(p=0.5)</span><br><span class="line"></span><br><span class="line"># 使用随机水平翻转操作对图像进行处理</span><br><span class="line">img = Image.open(&#x27;example.jpg&#x27;)  # 假设有一张名为 &#x27;example.jpg&#x27; 的图像</span><br><span class="line">processed_img = transformations(img)</span><br></pre></td></tr></table></figure><h3 id="4-torchvision-transforms-ToTensor"><a href="#4-torchvision-transforms-ToTensor" class="headerlink" title="4.torchvision.transforms.ToTensor()"></a>4.torchvision.transforms.ToTensor()</h3><p><strong>torchvision.transforms.ToTensor():</strong>用于将 PIL 图像或 NumPy 数组转换为 PyTorch 张量（Tensor）格式，以便在深度学习模型中使用。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">import torchvision.transforms as transforms</span><br><span class="line"></span><br><span class="line"># 定义 ToTensor 操作</span><br><span class="line">transformations = transforms.ToTensor()</span><br><span class="line"></span><br><span class="line"># 使用 ToTensor 操作将 PIL 图像转换为张量</span><br><span class="line">img = Image.open(&#x27;example.jpg&#x27;)  # 假设有一张名为 &#x27;example.jpg&#x27; 的图像</span><br><span class="line">tensor_img = transformations(img)</span><br></pre></td></tr></table></figure><h3 id="5-torchvision-transforms-Normalize"><a href="#5-torchvision-transforms-Normalize" class="headerlink" title="5.torchvision.transforms.Normalize"></a>5.torchvision.transforms.Normalize</h3><p><strong>torchvision.transforms.Normalize:</strong>对图像进行归一化处理，以便模型在训练和推断过程中更好地处理数据</p><h4 id="5-1-计算数据集的均值和标准差"><a href="#5-1-计算数据集的均值和标准差" class="headerlink" title="5.1 计算数据集的均值和标准差"></a>5.1 计算数据集的均值和标准差</h4><p><strong>os.walk(dataset_path)：</strong>用于遍历文件夹的一个函数，它生成一个三元组的迭代器，每次迭代返回一个包含当前<strong>目录路径</strong>、当前目录下所有<strong>子目录名</strong>、当前目录下所有<strong>文件名</strong>的元组。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line">import numpy as np</span><br><span class="line">from PIL import Image</span><br><span class="line">import os</span><br><span class="line">import torchvision.transforms as transforms</span><br><span class="line">import torch</span><br><span class="line"></span><br><span class="line">def compute_mean_and_std(dataset_path):</span><br><span class="line">    # 初始化均值和标准差</span><br><span class="line">    mean = torch.zeros(3)</span><br><span class="line">    std = torch.zeros(3)</span><br><span class="line">    num_images = 0</span><br><span class="line"></span><br><span class="line">    # 遍历数据集</span><br><span class="line">    for root, _, files in os.walk(dataset_path):</span><br><span class="line">        for file in files:</span><br><span class="line">            if file.endswith(&quot;.jpg&quot;) or file.endswith(&quot;.png&quot;) or file.endswith(&quot;.jpeg&quot;):</span><br><span class="line">                img_path = os.path.join(root, file)</span><br><span class="line">                img = Image.open(img_path).convert(&quot;RGB&quot;)</span><br><span class="line">                transformations = transforms.ToTensor()</span><br><span class="line">                img = transformations(img)</span><br><span class="line"></span><br><span class="line">                # 计算当前图像的均值和标准差</span><br><span class="line">                mean += torch.mean(img, dim=(1, 2))</span><br><span class="line">                std += torch.std(img, dim=(1, 2))</span><br><span class="line">                num_images += 1</span><br><span class="line"></span><br><span class="line">    # 计算整个数据集的均值和标准差</span><br><span class="line">    mean /= num_images</span><br><span class="line">    std /= num_images</span><br><span class="line"></span><br><span class="line">    return mean.tolist(), std.tolist()</span><br><span class="line"></span><br><span class="line"># 数据集路径</span><br><span class="line">dataset_path = &quot;.\train&quot;</span><br><span class="line"></span><br><span class="line"># 计算均值和标准差</span><br><span class="line">mean, std = compute_mean_and_std(dataset_path)</span><br><span class="line">print(&quot;Mean:&quot;, mean)</span><br><span class="line">print(&quot;Std:&quot;, std)</span><br></pre></td></tr></table></figure><p><em>注：</em></p><p><em>1.表示图像的 Tensor，其维度为 (C, H, W)，其中 C 表示通道数，H 表示图像的高度，W 表示图像的宽度。<strong>dim=(1, 2)</strong> 表示在第 1 和第 2 维度上进行求均值的操作，<strong>torch.mean(img, dim=(1, 2))</strong>将对每个通道的高度和宽度上的所有像素值进行求均值。</em></p><p><em>2.<strong>PIL 图像（NumPy 数组表示）：</strong>在将图像转换为 NumPy 数组时，通常图像中的像素值被映射到 [0, 255] 范围内，方便对图像进行基本的像素级操作，如颜色调整、滤波等。</em></p><p><em>3.<strong>PyTorch 张量</strong>：PyTorch 张量在处理图像时，通常会进行数据归一化处理，图像的像素值会被映射到 [0, 1] 范围内，这可以有效地缩小不同通道之间数值的差异，避免数据在训练过程中产生较大的梯度，导致训练不稳定。（归一化方式是将像素值除以 255）</em></p><h4 id="5-2-torchvision-transforms-Normalize"><a href="#5-2-torchvision-transforms-Normalize" class="headerlink" title="5.2 torchvision.transforms.Normalize"></a>5.2 torchvision.transforms.Normalize</h4><p><code>torchvision.transforms.Normalize(mean, std)</code> 的参数包括：</p><ul><li><code>mean</code>：一个包含三个元素的列表或元组，表示每个通道的均值。对于 RGB 图像，通常是 [R 均值, G 均值, B 均值]。</li><li><code>std</code>：一个包含三个元素的列表或元组，表示每个通道的标准差。对于 RGB 图像，通常是 [R 标准差, G 标准差, B 标准差]</li></ul><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">import torchvision.transforms as transforms</span><br><span class="line"></span><br><span class="line"># 定义归一化操作</span><br><span class="line">mean = [0.4914, 0.4822, 0.4465]</span><br><span class="line">std = [0.2023, 0.1994, 0.2010]</span><br><span class="line">transformations = transforms.Normalize(mean=mean, std=std)</span><br><span class="line"></span><br><span class="line"># 假设 tensor_img 是一个张量，表示一张图像</span><br><span class="line">normalized_img = transformations(tensor_img)</span><br></pre></td></tr></table></figure><h3 id="6-torchvision-transforms-functional-crop"><a href="#6-torchvision-transforms-functional-crop" class="headerlink" title="6.torchvision.transforms.functional.crop"></a>6.torchvision.transforms.functional.crop</h3><p><strong>torchvision.transforms.functional.crop：</strong>从输入图像中裁剪出指定区域的子图像</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torchvision.transforms.functional.crop(img, top, left, height, width)</span><br></pre></td></tr></table></figure><ul><li><code>img</code>：输入的图像，通常是一个 PIL 图像或一个张量。</li><li><code>top</code>：裁剪区域的顶部边界（以像素为单位）。</li><li><code>left</code>：裁剪区域的左边界（以像素为单位）。</li><li><code>height</code>：裁剪区域的高度（以像素为单位）。</li><li><code>width</code>：裁剪区域的宽度（以像素为单位）。</li></ul><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">crop_rect = (0, 0, 320, 480)</span><br><span class="line">X = torchvision.transforms.functional.crop(test_images, *crop_rect)</span><br><span class="line">#crop_rect 是一个包含四个值的元组 (top, left, height, width)，它定义了要从图像中裁剪的区域的位置和大小</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> 计算机视觉 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 计算机视觉 </tag>
            
            <tag> torchvision </tag>
            
            <tag> 图像预处理 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>线性回归</title>
      <link href="/2023/07/29/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/"/>
      <url>/2023/07/29/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/</url>
      
        <content type="html"><![CDATA[<h2 id="线性回归"><a href="#线性回归" class="headerlink" title="线性回归"></a>线性回归</h2><h3 id="1-线性回归的从零开始实现"><a href="#1-线性回归的从零开始实现" class="headerlink" title="1.线性回归的从零开始实现"></a>1.线性回归的从零开始实现</h3><h4 id="1-1-构造数据集"><a href="#1-1-构造数据集" class="headerlink" title="1.1 构造数据集"></a>1.1 构造数据集</h4><p>以参数w=[2,−3.4]⊤、b=4.2和噪声项ϵ生成数据集。</p><p>标签：y=Xw+b+ϵ</p><p><strong>torch.normal()：</strong>生成一个服从正态分布的随机张量。</p><p><strong>torch.matmul：</strong>计算两个张量矩阵相乘（可以用利用广播机制进行不同维度的相乘操作）。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">def synthetic_data(w, b, num_examples):  </span><br><span class="line">    &quot;&quot;&quot;生成 y = Xw + b + 噪声。&quot;&quot;&quot;</span><br><span class="line">    X = torch.normal(0, 1, (num_examples, len(w)))</span><br><span class="line">    #torch.normal()生成一个服从正态分布（均值为0，标准差为1）的随机张量X，大小为(num_examples, len(w))</span><br><span class="line">    #其中num_examples是样本数量，len(w)是特征数量。</span><br><span class="line">    y = torch.matmul(X, w) + b</span><br><span class="line">    #函数通过矩阵乘法计算目标变量y。y是由X和w之间的线性关系构造的，并且加上了偏差项b。</span><br><span class="line">    y += torch.normal(0, 0.01, y.shape)</span><br><span class="line">    #函数为目标变量y添加了一个服从正态分布（均值为0，标准差为0.01）的随机噪声，以模拟真实数据中的噪声。</span><br><span class="line">    return X, y.reshape((-1, 1))</span><br><span class="line"></span><br><span class="line">true_w = torch.tensor([2, -3.4])</span><br><span class="line">true_b = 4.2</span><br><span class="line">features, labels = synthetic_data(true_w, true_b, 1000)</span><br></pre></td></tr></table></figure><h4 id="1-2-观察特征的相关性"><a href="#1-2-观察特征的相关性" class="headerlink" title="1.2 观察特征的相关性"></a>1.2 观察特征的相关性</h4><p>将features张量中第二列的数据作为x轴，labels张量的数据作为y轴，绘制出散点图，散点图的每个点的大小为1。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">d2l.plt.scatter(features[:,1].detach().numpy(),</span><br><span class="line">                labels.detach().numpy(), 1);#detach()函数用于从计算图中分离出这部分数据，以防止在绘制图形时产生梯度计算</span><br></pre></td></tr></table></figure><p><img src="/2023/07/29/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/img.png" alt="img"></p><h4 id="1-3-构造一个PyTorch数据迭代器"><a href="#1-3-构造一个PyTorch数据迭代器" class="headerlink" title="1.3 构造一个PyTorch数据迭代器"></a>1.3 构造一个PyTorch数据迭代器</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line">def data_iter(batch_size, features, labels):#接收批量大小、特征矩阵和标签向量作为输入</span><br><span class="line">    num_examples = len(features)#计算了特征数据的样本数量。</span><br><span class="line">    indices = list(range(num_examples))#创建一个索引列表。</span><br><span class="line">    random.shuffle(indices)# 将样本索引列表打乱，目的是随机化样本的顺序，这样在每个迭代周期中会使用不同的样本顺序，有助于训练模型的泛化能力。</span><br><span class="line">    for i in range(0, num_examples, batch_size):#循环开始，每次增加batch_size个步长进行迭代，直到遍历完所有样本。</span><br><span class="line">        batch_indices = torch.tensor(indices[i:min(i +</span><br><span class="line">                                                   batch_size, num_examples)])</span><br><span class="line">                #从打乱后的索引列表中选择一个批次的索引。</span><br><span class="line">                #注意，最后一个批次可能不足batch_size，因此使用min(i + batch_size, num_examples)来确保不超出数据集的边界。</span><br><span class="line">        yield features[batch_indices], labels[batch_indices]</span><br><span class="line">  </span><br><span class="line">batch_size = 10</span><br><span class="line">for X, y in data_iter(batch_size, features, labels):</span><br><span class="line">    print(X, &#x27;\n&#x27;, y)</span><br><span class="line">    break</span><br><span class="line"></span><br><span class="line">out:</span><br><span class="line">tensor([[ 0.9374, -0.1930],</span><br><span class="line">        [ 0.1325, -1.0253],</span><br><span class="line">        [ 0.9573,  0.4727],</span><br><span class="line">        [-0.3795,  0.4407],</span><br><span class="line">        [-0.4063,  1.0375],</span><br><span class="line">        [ 0.7656, -1.9743],</span><br><span class="line">        [ 1.4891,  1.4386],</span><br><span class="line">        [-0.9692, -0.4879],</span><br><span class="line">        [ 2.5848,  1.3103],</span><br><span class="line">        [-0.5822,  0.5608]]) </span><br><span class="line"> tensor([[ 6.7309],</span><br><span class="line">        [ 7.9584],</span><br><span class="line">        [ 4.4986],</span><br><span class="line">        [ 1.9576],</span><br><span class="line">        [-0.1413],</span><br><span class="line">        [12.4321],</span><br><span class="line">        [ 2.2723],</span><br><span class="line">        [ 3.9170],</span><br><span class="line">        [ 4.9298],</span><br><span class="line">        [ 1.1197]])</span><br></pre></td></tr></table></figure><h4 id="1-4-初始化模型参数"><a href="#1-4-初始化模型参数" class="headerlink" title="1.4 初始化模型参数"></a>1.4 初始化模型参数</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">w = torch.normal(0, 0.01, size=(2, 1), requires_grad=True)</span><br><span class="line">b = torch.zeros(1, requires_grad=True)</span><br><span class="line">#requires_grad=True:表示这个张量是可训练的，当执行反向传播时，PyTorch会自动计算关于w的梯度，并在优化算法中更新这个张量。</span><br></pre></td></tr></table></figure><h4 id="1-5-定义模型"><a href="#1-5-定义模型" class="headerlink" title="1.5 定义模型"></a>1.5 定义模型</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">def linreg(X, w, b):  </span><br><span class="line">    &quot;&quot;&quot;线性回归模型。&quot;&quot;&quot;</span><br><span class="line">    return torch.matmul(X, w) + b</span><br></pre></td></tr></table></figure><h4 id="1-6-定义损失函数"><a href="#1-6-定义损失函数" class="headerlink" title="1.6 定义损失函数"></a>1.6 定义损失函数</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">def squared_loss(y_hat, y):  #y_hat：预测值</span><br><span class="line">    &quot;&quot;&quot;均方损失。&quot;&quot;&quot;</span><br><span class="line">    return (y_hat - y.reshape(y_hat.shape))**2 / 2</span><br></pre></td></tr></table></figure><h4 id="1-7-定义优化算法"><a href="#1-7-定义优化算法" class="headerlink" title="1.7 定义优化算法"></a>1.7 定义优化算法</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">def sgd(params, lr, batch_size):  #params（模型的可训练参数）、lr（学习率）和batch_size（批大小）</span><br><span class="line">    &quot;&quot;&quot;小批量随机梯度下降。&quot;&quot;&quot;</span><br><span class="line">    with torch.no_grad():#上下文管理器，用于包裹在其内部的操作，告诉PyTorch在这个上下文中不要计算梯度</span><br><span class="line">        for param in params:</span><br><span class="line">            param -= lr * param.grad / batch_size#梯度乘以学习率，再除以批大小来更新参数</span><br><span class="line">            param.grad.zero_()#将参数的梯度清零，在PyTorch中，梯度是累积的，需要手动将梯度置零</span><br></pre></td></tr></table></figure><h4 id="1-8-训练过程"><a href="#1-8-训练过程" class="headerlink" title="1.8 训练过程"></a>1.8 训练过程</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">lr = 0.03</span><br><span class="line">num_epochs = 3</span><br><span class="line">net = linreg</span><br><span class="line">loss = squared_loss</span><br><span class="line"></span><br><span class="line">for epoch in range(num_epochs):</span><br><span class="line">    for X, y in data_iter(batch_size, features, labels):</span><br><span class="line">        l = loss(net(X, w, b), y)</span><br><span class="line">        l.sum().backward()# l是向量，用sum转换成标量，再对损失进行反向传播，计算模型参数 w 和 b 的梯度。</span><br><span class="line">        sgd([w, b], lr, batch_size)</span><br><span class="line">    with torch.no_grad():</span><br><span class="line">        train_l = loss(net(features, w, b), labels)</span><br><span class="line">        print(f&#x27;epoch &#123;epoch + 1&#125;, loss &#123;float(train_l.mean()):f&#125;&#x27;)</span><br></pre></td></tr></table></figure><h4 id="1-9-评估训练效果"><a href="#1-9-评估训练效果" class="headerlink" title="1.9 评估训练效果"></a>1.9 评估训练效果</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">print(f&#x27;w的估计误差: &#123;true_w - w.reshape(true_w.shape)&#125;&#x27;)</span><br><span class="line">print(f&#x27;b的估计误差: &#123;true_b - b&#125;&#x27;)</span><br></pre></td></tr></table></figure><h3 id="2-线性回归的简洁实现"><a href="#2-线性回归的简洁实现" class="headerlink" title="2.线性回归的简洁实现"></a>2.线性回归的简洁实现</h3><h4 id="2-1-构造数据集"><a href="#2-1-构造数据集" class="headerlink" title="2.1 构造数据集"></a>2.1 构造数据集</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">import numpy as np</span><br><span class="line">import torch</span><br><span class="line">from torch.utils import data</span><br><span class="line">from d2l import torch as d2l</span><br><span class="line"></span><br><span class="line">true_w = torch.tensor([2, -3.4])</span><br><span class="line">true_b = 4.2</span><br><span class="line">features, labels = d2l.synthetic_data(true_w, true_b, 1000)</span><br></pre></td></tr></table></figure><h4 id="2-2-构造一个PyTorch数据迭代器"><a href="#2-2-构造一个PyTorch数据迭代器" class="headerlink" title="2.2 构造一个PyTorch数据迭代器"></a>2.2 构造一个PyTorch数据迭代器</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">def load_array(data_arrays, batch_size, is_train=True):  </span><br><span class="line">    &quot;&quot;&quot;构造一个PyTorch数据迭代器。&quot;&quot;&quot;</span><br><span class="line">    dataset = data.TensorDataset(*data_arrays)#用于将特征数据和标签数据打包成一个数据集对象</span><br><span class="line">    return data.DataLoader(dataset, batch_size, shuffle=is_train)#从数据集中按照批大小 batch_size 加载数据</span><br><span class="line">#shuffle=is_train 表示在训练模式下（is_train=True）将数据打乱顺序</span><br><span class="line"></span><br><span class="line">batch_size = 10</span><br><span class="line">data_iter = load_array((features, labels), batch_size)</span><br><span class="line"></span><br><span class="line">next(iter(data_iter))</span><br></pre></td></tr></table></figure><h4 id="2-3-构建模型"><a href="#2-3-构建模型" class="headerlink" title="2.3 构建模型"></a>2.3 构建模型</h4><p><strong>nn.Sequential：</strong>PyTorch中的一个容器类，它按照顺序组合各种神经网络的层。</p><p><strong>nn.Linear(2, 1)：</strong><code>nn.Linear</code>是一个全连接层（线性层），它接收两个参数：输入特征的数量（输入维度）和输出特征的数量（输出维度）。因此该层将实现一个将输入维度为2的数据映射为输出维度为1的线性变换（线性回归模型）。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">from torch import nn</span><br><span class="line"></span><br><span class="line">net = nn.Sequential(nn.Linear(2, 1))</span><br><span class="line">#初始化模型参数</span><br><span class="line">net[0].weight.data.normal_(0, 0.01)#net[0]表示模型中的第一个层</span><br><span class="line">net[0].bias.data.fill_(0)</span><br><span class="line"></span><br><span class="line">loss = nn.MSELoss()#定义损失函数</span><br><span class="line">trainer = torch.optim.SGD(net.parameters(), lr=0.03)#实例化SGD实例</span><br></pre></td></tr></table></figure><h4 id="2-4-训练过程"><a href="#2-4-训练过程" class="headerlink" title="2.4 训练过程"></a>2.4 训练过程</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">num_epochs = 3</span><br><span class="line">for epoch in range(num_epochs):</span><br><span class="line">    for X, y in data_iter:</span><br><span class="line">        l = loss(net(X), y)</span><br><span class="line">        trainer.zero_grad()</span><br><span class="line">        l.backward() #pytorch自动对l求sum，无需再求sum</span><br><span class="line">        trainer.step()</span><br><span class="line">    l = loss(net(features), labels)</span><br><span class="line">    print(f&#x27;epoch &#123;epoch + 1&#125;, loss &#123;l:f&#125;&#x27;)</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> 深度学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> python </tag>
            
            <tag> 深度学习 </tag>
            
            <tag> 线性回归 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>数据预处理</title>
      <link href="/2023/07/28/%E6%95%B0%E6%8D%AE%E9%A2%84%E5%A4%84%E7%90%86/"/>
      <url>/2023/07/28/%E6%95%B0%E6%8D%AE%E9%A2%84%E5%A4%84%E7%90%86/</url>
      
        <content type="html"><![CDATA[<h2 id="数据预处理"><a href="#数据预处理" class="headerlink" title="数据预处理"></a>数据预处理</h2><h3 id="1-创建csv文件"><a href="#1-创建csv文件" class="headerlink" title="1.创建csv文件"></a>1.创建csv文件</h3><h4 id="1-1-创建目录"><a href="#1-1-创建目录" class="headerlink" title="1.1 创建目录"></a>1.1 创建目录</h4><p><strong>os.makedirs():</strong> 这是一个用于递归创建目录的函数。它接受一个路径作为输入，并创建路径中所有缺失的目录</p><p><strong>exist_ok=True:</strong> 这是os.makedirs()函数的一个可选参数。当设置为True时，如果目标目录已经存在，函数不会引发错误，而是默默地继续执行。</p><p><strong>os.path.join()：</strong>函数来连接两个路径部分：”..”(表示父目录) 和 “data”</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">os.makedirs(os.path.join(&#x27;..&#x27;, &#x27;data&#x27;), exist_ok=True)</span><br><span class="line">#创建一个名为&quot;data&quot;的目录，该目录位于当前工作目录的父目录中</span><br><span class="line">data_file = os.path.join(&#x27;..&#x27;, &#x27;data&#x27;, &#x27;house_tiny.csv&#x27;)</span><br><span class="line">#创建一个名为data_file的变量，指向当前工作目录的上一级目录中的&quot;data&quot;目录下的&quot;house_tiny.csv&quot;文件</span><br></pre></td></tr></table></figure><p><em>注：如果目录不存在，它将被创建。这样，代码在运行时，如果”data”目录已经存在，它不会抛出异常，而是继续执行。</em></p><h4 id="1-2-创建新文件"><a href="#1-2-创建新文件" class="headerlink" title="1.2 创建新文件"></a>1.2 创建新文件</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">with open(data_file, &#x27;w&#x27;) as f:#打开一个文件只用于写入。如果该文件已存在则将其覆盖。如果该文件不存在，创建新文件。</span><br><span class="line">    f.write(&#x27;NumRooms,Alley,Price\n&#x27;)</span><br><span class="line">    f.write(&#x27;NA,Pave,127500\n&#x27;)</span><br><span class="line">    f.write(&#x27;2,NA,106000\n&#x27;)</span><br><span class="line">    f.write(&#x27;4,NA,178100\n&#x27;)</span><br><span class="line">    f.write(&#x27;NA,NA,140000\n&#x27;)</span><br></pre></td></tr></table></figure><h3 id="2-数据预处理"><a href="#2-数据预处理" class="headerlink" title="2.数据预处理"></a>2.数据预处理</h3><h4 id="2-1-读取文件"><a href="#2-1-读取文件" class="headerlink" title="2.1 读取文件"></a>2.1 读取文件</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">import pandas as pd</span><br><span class="line">data = pd.read_csv(data_file)</span><br><span class="line">print(data)</span><br></pre></td></tr></table></figure><h4 id="2-2-数据切片"><a href="#2-2-数据切片" class="headerlink" title="2.2 数据切片"></a>2.2 数据切片</h4><p><strong>iloc方法</strong>用于基于整数位置对数据进行索引和切片</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">inputs, outputs = data.iloc[:,0:2], data.iloc[:,2]#iloc方法用于基于整数位置对数据进行索引和切片</span><br><span class="line">print(inputs)</span><br><span class="line">print(outputs)</span><br><span class="line"></span><br><span class="line">out:</span><br><span class="line"> NumRooms Alley</span><br><span class="line">0 NaN Pave</span><br><span class="line">1 2.0 NaN</span><br><span class="line">2 4.0 NaN</span><br><span class="line">3 NaN NaN</span><br><span class="line"></span><br><span class="line">0    127500</span><br><span class="line">1    106000</span><br><span class="line">2    178100</span><br><span class="line">3    140000</span><br><span class="line">Name: Price, dtype: int64</span><br></pre></td></tr></table></figure><h4 id="2-3-缺失值填充"><a href="#2-3-缺失值填充" class="headerlink" title="2.3 缺失值填充"></a>2.3 缺失值填充</h4><p><strong>fillna() 方法</strong>用指定的值（在这里是 inputs.mean() 即每列的均值）替换DataFrame中的缺失值。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">#对筛选出的数值类型列使用 mean() 方法计算各自列的均值进行填充</span><br><span class="line">inputs = inputs.fillna(inputs.select_dtypes(include=&#x27;number&#x27;).mean())</span><br><span class="line"></span><br><span class="line">out:</span><br><span class="line">   NumRooms Alley</span><br><span class="line">0       3.0  Pave</span><br><span class="line">1       2.0   NaN</span><br><span class="line">2       4.0   NaN</span><br><span class="line">3       3.0   NaN</span><br></pre></td></tr></table></figure><p><strong>pd.get_dummies()函数</strong>对inputs DataFrame 进行独热编码（One-Hot Encoding）处理。</p><p><strong>dummy_na=True:</strong> 这是pd.get_dummies()函数的一个可选参数。当设置为True时，它会为原始数据中的缺失值创建一个额外的二进制列。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">inputs = pd.get_dummies(inputs, dummy_na=True)</span><br><span class="line"></span><br><span class="line">out:</span><br><span class="line">   NumRooms  Alley_Pave  Alley_nan</span><br><span class="line">0       3.0        True      False</span><br><span class="line">1       2.0       False       True</span><br><span class="line">2       4.0       False       True</span><br><span class="line">3       3.0       False       True</span><br></pre></td></tr></table></figure><p><em>注：独热编码的过程会将inputs DataFrame  中的分类变量转换成二进制的向量表示。对于每个分类变量，它会为每个类别创建一个新的列，其中类别出现的位置为1，其余位置为0。如果某一行数据的分类变量具有某个类别，则该类别对应的列为1，其他类别对应的列都为0。如果原始数据中有缺失值，独热编码会在对应的缺失值列中标记为1。</em></p><h4 id="2-4-类型转换"><a href="#2-4-类型转换" class="headerlink" title="2.4 类型转换"></a>2.4 类型转换</h4><p>inputs和outputs中的所有条目都是数值类型，可以转换为张量格式</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">import torch</span><br><span class="line">inputs = inputs.astype(&#x27;float64&#x27;)</span><br><span class="line">x, y = torch.tensor(inputs.values), torch.tensor(outputs.values)</span><br><span class="line">x, y</span><br><span class="line"></span><br><span class="line">out:</span><br><span class="line">(tensor([[3., 1., 0.],</span><br><span class="line">         [2., 0., 1.],</span><br><span class="line">         [4., 0., 1.],</span><br><span class="line">         [3., 0., 1.]], dtype=torch.float64),</span><br><span class="line"> tensor([127500, 106000, 178100, 140000]))</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> 深度学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> python </tag>
            
            <tag> 数据预处理 </tag>
            
            <tag> Pytorch </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>神经网络(1)</title>
      <link href="/2023/07/16/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
      <url>/2023/07/16/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/</url>
      
        <content type="html"><![CDATA[<h2 id="神经网络"><a href="#神经网络" class="headerlink" title="神经网络"></a>神经网络</h2><h3 id="1-激活函数"><a href="#1-激活函数" class="headerlink" title="1. 激活函数"></a>1. 激活函数</h3><p>激活函数：作用在于决定如何来激活输入信号的总和。</p><p>如，感知机的数学形式：<img src="/2023/07/16/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/img2.png" alt="img2"></p><p>其亦可表达为：</p><p><img src="/2023/07/16/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/img3.png" alt="img3"></p><p><img src="/2023/07/16/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/img4.png" alt="img4"></p><p>h(x)函数会将输入信号的总和转换为输出信号，即为激活函数。</p><h4 id="1-1-阶跃函数"><a href="#1-1-阶跃函数" class="headerlink" title="1.1 阶跃函数"></a>1.1 阶跃函数</h4><p><strong>激活函数：</strong>以阈值为界，一旦输入超过阈值，就切换输出。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">#当输入超过0时，输出1，否则输出0的阶跃函数</span><br><span class="line">def step_function(x):</span><br><span class="line">  y = x &gt; 0</span><br><span class="line">  return y.astype(np.int)</span><br><span class="line">  #用astype()方法转换NumPy数组的类型</span><br></pre></td></tr></table></figure><p><strong>阶跃函数的图形</strong></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">import numpy as np</span><br><span class="line">import matplotlib.pylab as plt</span><br><span class="line">def step_function(x):</span><br><span class="line"> return np.array(x &gt; 0, dtype=np.int)</span><br><span class="line">x = np.arange(-5.0, 5.0, 0.1)</span><br><span class="line">y = step_function(x)</span><br><span class="line">plt.plot(x, y)</span><br><span class="line">plt.ylim(-0.1, 1.1) # 指定y轴的范围</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="/2023/07/16/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/img5.png" alt="img5" style="zoom:50%;"></p><h4 id="1-2-sigmoid函数"><a href="#1-2-sigmoid函数" class="headerlink" title="1.2 sigmoid函数"></a>1.2 sigmoid函数</h4><p><strong>sigmoid函数:</strong> $h(x)={1 \over 1+e^{-x} }$</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">def sigmoid(x):</span><br><span class="line"> return 1 / (1 + np.exp(-x))</span><br></pre></td></tr></table></figure><p><strong>sigmoid函数的图形</strong></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">x = np.arange(-5.0, 5.0, 0.1)</span><br><span class="line">y = sigmoid(x)</span><br><span class="line">plt.plot(x, y)</span><br><span class="line">plt.ylim(-0.1, 1.1) # 指定y轴的范围</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p>​                                                                <img src="/2023/07/16/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/img6.png" alt="img6" style="zoom: 50%;"></p><p><em>注：神经网络的激活函数必须使用非线性函数。使用线性函数时，加深神经网络的层数就没有意义了，因为不管如何加深层数，总是存在与之等效的“无隐藏层的神经网络”。</em></p><h4 id="1-3-tanh函数"><a href="#1-3-tanh函数" class="headerlink" title="1.3 tanh函数"></a>1.3 tanh函数</h4><p><strong>tanh函数：</strong>$\LARGE {e^{x}-e^{-x}\over e^{x}+e^{-x} }$</p><p><img src="/2023/07/16/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/img11.png" alt="img11" style="zoom: 50%;"></p><h4 id="1-4-ReLU函数"><a href="#1-4-ReLU函数" class="headerlink" title="1.4 ReLU函数"></a>1.4 ReLU函数</h4><p><strong>ReLU函数</strong>：<img src="/2023/07/16/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/img7.png" alt="img7"></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">def relu(x):</span><br><span class="line"> return np.maximum(0, x)</span><br></pre></td></tr></table></figure><p><strong>ReLU函数的图形</strong></p><p><img src="/2023/07/16/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/img8.png" alt="img8" style="zoom: 67%;"></p><p><strong>Leaky ReLU函数:</strong>max(0.1x,x)</p><h3 id="2-恒等函数和-softmax函数"><a href="#2-恒等函数和-softmax函数" class="headerlink" title="2 恒等函数和 softmax函数"></a>2 恒等函数和 softmax函数</h3><p><strong>恒等函数：</strong>将输入按原样输出，对于输入的信息，不加以任何改动地直接输出。</p><p><strong>softmax函数：</strong><script type="math/tex">\large y_k={e^{a_k} \over \sum\limits_{i=1}^ne^{a_i}}</script></p><p>为了防止溢出，可对softmax函数进行如下改进：</p><p><img src="/2023/07/16/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/img10.png" alt="img10"></p><p><em>注：其中，$C^{，}$通常使用输入信号中的最大值，来防止溢出。</em></p><p><img src="/2023/07/16/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/img9.png" alt="img9"></p><p><strong><em>注：</em></strong></p><p><em>1.softmax函数的输出通过箭头与所有的输入信号相连，输出层的各个神经元都受到所有输入信号的影响。</em></p><p><em>2.softmax函数将输出的给类别的得分转换成了概率</em></p><h3 id="3-交叉熵损失"><a href="#3-交叉熵损失" class="headerlink" title="3. 交叉熵损失"></a>3. 交叉熵损失</h3><p><strong>交叉熵损失：</strong>用于比较分类器预测分布与真实分布的距离。</p><p><strong>熵：</strong>H(p)=$-\sum p(x)log(p(x))$</p><p><strong>交叉熵：</strong>H(p,q)=$-\sum p(x)log(q(x))$</p><p><strong>相对熵(KL散度)：</strong>KL(p||q)=$-\sum p(x)log({q(x)\over p(x)})$</p><p><strong>三者之间的关系：</strong>H(p,q)=KL(p||q)+H(p)</p><p>p(x):真实分布概率        q(x):分类器预测分布概率</p><p><strong>注：</strong></p><p><strong>1.相对熵(KL散度)用于度量两个分布间的不相似性。</strong></p><p><strong>2.真实分布为one-hot形式时，H(p)=0,用H(p,q)代替KL(p||q),且交叉熵损失可化简为L=$-log(q_j)$,其中j为真实的类别。</strong></p><h3 id="3-参数优化"><a href="#3-参数优化" class="headerlink" title="3. 参数优化"></a>3. 参数优化</h3><p>参数优化：利用损失函数的输出值作为反馈信号来调整分类器的参数。</p><h4 id="3-1-梯度下降算法"><a href="#3-1-梯度下降算法" class="headerlink" title="3.1 梯度下降算法"></a>3.1 梯度下降算法</h4><p>原理：对于最小化优化问题，只需要将参数沿着梯度相反的方向前进一个步长，就可以实现目标函数的下降。</p><p>方向：负梯度方向</p><p>移动：步长(学习率)</p><p><strong>随机梯度下降算法：</strong>每次随机选择一个样本，计算损失并更新梯度。</p><p>缺点：单个样本的训练易受噪声的影响，不是每次迭代都向着整体最优化的方向。</p><p><strong>小批量随机梯度下降：</strong>每次选择m个样本，计算损失并更新梯度。</p><h4 id="3-2-梯度消失与梯度爆炸"><a href="#3-2-梯度消失与梯度爆炸" class="headerlink" title="3.2 梯度消失与梯度爆炸"></a>3.2 梯度消失与梯度爆炸</h4><p><strong>梯度消失：</strong>由于链式法则的乘法性质导致梯度趋向于0。</p><p>注：tanh,sigmoid局部梯度特性不利于网络梯度流的反向传播，尽量选择RelU或Leaky RelU。</p><p><strong>梯度爆炸：</strong>断崖处梯度乘以学习率后是一个非常大得值，从而“飞”出了合理区域，最终导致算法不收敛;</p><p>注：通过限制步长的大小(梯度裁剪)可以避免梯度爆炸。</p><h4 id="3-3-梯度下降算法的问题及改进"><a href="#3-3-梯度下降算法的问题及改进" class="headerlink" title="3.3 梯度下降算法的问题及改进"></a>3.3 梯度下降算法的问题及改进</h4><p><strong>梯度下降算法的问题：</strong>在一个方向上变化迅速而在另一个方向上的变化缓慢，并且通过增大步长不能提高算法的收敛速度。</p><p><strong>动量法：</strong>利用累加历史梯度信息更新梯度。</p><p>步骤：</p><p>1.采样</p><p>2.计算梯度：g</p><p>3.速度更新：v=uv+g(u:动量系数，u=0时为梯度下降算法)</p><p>4.更新权值：w=w-$\varepsilon$v($\varepsilon$:学习率）</p><p><em>注：梯度下降算法在局部最小点与鞍点处的梯度为0，无法通过，而动量法由于动量的存在可以通过，可以找到更优的解）</em></p><p><strong>自适应梯度算法(AdaGrad)：</strong>减小震荡方向的步长，增大平坦方向的步长。</p><p>步骤：</p><p>1.采样</p><p>2.计算梯度：g</p><p>3.累计平方梯度：r=r+g*g (r : 累计变量)</p><p>4.更新权值：w=w-${\large \varepsilon \over \sqrt{r}+\delta}*g$($\delta$:小常数，通常取$10^{-5}$,用于防止除0出错）</p><p><em>注：AdaGrad的一个限制是，随着累计变量的不断增大，导致每个参数的步长(学习率)非常小，这可能会大大减慢搜索进度，并且可能意味着无法找到最优值。</em></p><p><strong>RMSProp：</strong>累计平方梯度：r=$\rho$r+(1-$\rho$)g*g ($\rho$:衰减系数，通常取0.999)</p><p><strong>ADAM:</strong>同时使用动量与自适应梯度的思想</p><p>步骤：</p><p>1.采样</p><p>2.计算梯度：g</p><p>3.累计梯度：v=uv+(1-u)*g(u:动量系数,通常取0.9)</p><p>4.累计平方梯度：r=$\rho$r+(1-$\rho$)g*g </p><p>5.修正偏差：$\large \tilde{v}={v\over 1-u^t}$  ,$\large \tilde{r}={r\over 1-\rho^t}$ (t:迭代系数，该步骤极大缓解了算法初期的冷启动的问题)</p><p>6.更新权值：w=w-${\large \varepsilon \over \sqrt{\tilde{r}}+\delta}*\tilde{v}$</p><p><em>注：动量法等同于修改学习的“方向”，自适应梯度等同于修改“步长”，ADAM等同于修改“步长”和“方向”。</em></p>]]></content>
      
      
      <categories>
          
          <category> 深度学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> python </tag>
            
            <tag> 神经网络 </tag>
            
            <tag> 激活函数 </tag>
            
            <tag> 深度学习 </tag>
            
            <tag> 交叉熵 </tag>
            
            <tag> 参数优化 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>matplotlib</title>
      <link href="/2023/07/15/matplotlib/"/>
      <url>/2023/07/15/matplotlib/</url>
      
        <content type="html"><![CDATA[<h3 id="1-plot-函数"><a href="#1-plot-函数" class="headerlink" title="1.plot()函数"></a>1.plot()函数</h3><p><strong>plot()函数会根据列表中的数据尝试绘制出有意义的图形</strong></p><p>参数：<img src="/2023/07/15/matplotlib/img1.png" alt="img1"></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">x = np.arange(-6,6,0.1)</span><br><span class="line">#起点：-6，终点：6，步长：0.1</span><br><span class="line">y1 = np.sin(x)</span><br><span class="line">y2 = np.cos(x)</span><br><span class="line">plt.plot(x,y1)</span><br><span class="line">plt.plot(x,y2)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="/2023/07/15/matplotlib/img2.png" alt="img2"></p><p><strong>plt的title()方法：</strong>添加图标标题</p><p><strong>xlabel()、ylabel()方法：</strong>为每条轴设置标题、大小等参数</p><p><strong>tick_params()方法：</strong>指定刻度标记的大小</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">x = np.arange(-6,6,0.1)#起点：-6，终点：6，步长：0.1</span><br><span class="line">y1 = np.sin(x)</span><br><span class="line">y2 = np.cos(x)</span><br><span class="line">plt.plot(x,y1)</span><br><span class="line">plt.plot(x,y2)</span><br><span class="line">plt.title(&quot;Figure 1&quot;,fontsize = 18,color = &quot;red&quot;)</span><br><span class="line">plt.xlabel(&quot;x value&quot;,fontsize = 14)</span><br><span class="line">plt.ylabel(&quot;y value&quot;,fontsize = 14)</span><br><span class="line">plt.tick_params(axis=&quot;both&quot;,labelsize=10)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="/2023/07/15/matplotlib/img3.png" alt="img3"></p><h3 id="2-图像的显示和读取"><a href="#2-图像的显示和读取" class="headerlink" title="2.图像的显示和读取"></a>2.图像的显示和读取</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">img = plt.imread(&#x27;lena.png&#x27;)</span><br><span class="line">plt.imshow(img)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="/2023/07/15/matplotlib/img4.png" alt="img4"></p>]]></content>
      
      
      <categories>
          
          <category> 数据分析 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> python </tag>
            
            <tag> 数据分析 </tag>
            
            <tag> matplotlib </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>感知机</title>
      <link href="/2023/07/12/%E6%84%9F%E7%9F%A5%E6%9C%BA/"/>
      <url>/2023/07/12/%E6%84%9F%E7%9F%A5%E6%9C%BA/</url>
      
        <content type="html"><![CDATA[<h2 id="感知机"><a href="#感知机" class="headerlink" title="感知机"></a>感知机</h2><h3 id="感知机的概念"><a href="#感知机的概念" class="headerlink" title="感知机的概念"></a>感知机的概念</h3><p>​        感知机是一种线性分类模型，属于<strong>判别模型</strong>，其使用一个线性方程所对应的超平面，将特征空间分成两部分。</p><p><strong>例：有两个输入的感知机：</strong></p><p>x1、x2是输入信号，y是输出信号，w1、w2是权重。图中的○称为“神经元”或者“节点”。输入信号被送往神经元时，会被分别乘以固定的权重（w1x1、w2x2）。神经元会计算传送过来的信号的总和，当这个总和超过了某个界限值时，才会输出1，否则输出0（二分类）。这也称为“神经元被激活”。这里将这个界限值称为阈值，用符号θ表示。</p><p><img src="/2023/07/12/%E6%84%9F%E7%9F%A5%E6%9C%BA/img1.png" alt="img1"></p><p>用<strong>数学公式</strong>表示为：</p><p><img src="/2023/07/12/%E6%84%9F%E7%9F%A5%E6%9C%BA/img2.png" alt="img2"></p><p><em>注：感知机的多个输入信号都有各自固有的权重，权重越大，对应该权重的信号的重要性就越高</em></p><p><strong>单层感知机的局限性：</strong>单层感知机的局限性就在于它只能表示由一条直线分割的空间。（单层感知机无法分离非线性空间）</p><p><strong>多层感知机：</strong>多层感知机可以表示非线性空间<img src="/2023/07/12/%E6%84%9F%E7%9F%A5%E6%9C%BA/img3.png" alt="img3"></p><h3 id="感知机的损失函数"><a href="#感知机的损失函数" class="headerlink" title="感知机的损失函数"></a>感知机的损失函数</h3><p>感知机的损失函数：误分类点到超平面的距离，即$-{1 \over {||w||}}\sum \limits_{x_i\in M} y_i(w*x_i+b)$,其中M为误分类点的集合</p><p><em>注：对于误分类点，$y_i(wx_i+b)$&lt;0</em></p><h3 id="感知机学习算法的原始形式"><a href="#感知机学习算法的原始形式" class="headerlink" title="感知机学习算法的原始形式"></a>感知机学习算法的原始形式</h3><p>感知机学习算法是误分类驱动的，采用随机梯度下降法，不断地极小化目标函数。</p><p><strong>训练过程：</strong></p><p><img src="/2023/07/12/%E6%84%9F%E7%9F%A5%E6%9C%BA/学习步骤.png" alt="感知机算法学习步骤"></p><p><strong><em>注：</em></strong></p><p><em>1.当一个实例点被误分类时，调整w,b，使分离超平面向误分类点的一侧移动，以减少误分类点距离超平面的距离。</em></p><p><em>2.其训练过程等价于使用批量大小为1的梯度下降，并使用L(y,x,w)=max(0,-y(w,x))的损失函数。</em></p>]]></content>
      
      
      <categories>
          
          <category> 机器学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> python </tag>
            
            <tag> 感知机 </tag>
            
            <tag> 机器学习 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>正则化</title>
      <link href="/2023/07/12/%E6%AD%A3%E5%88%99%E5%8C%96-1/"/>
      <url>/2023/07/12/%E6%AD%A3%E5%88%99%E5%8C%96-1/</url>
      
        <content type="html"><![CDATA[<h3 id="正则化"><a href="#正则化" class="headerlink" title="正则化"></a>正则化</h3><h4 id="1-基本公式"><a href="#1-基本公式" class="headerlink" title="1.基本公式"></a>1.基本公式</h4><p><strong>经验风险最小化：</strong>$min{1\over N}\sum\limits_{i=1}^NL(y_i,f(x_i))$<br><strong>正则化项：</strong>$\lambda J(f)$</p><p><strong>L1正则化项：</strong>$||w_1||=|w1|+|w2|+…..+|w_n|$<br><strong>L2正则化项：</strong>$||w_2||=\sqrt [2]{w_1^2+w_2^2+…..+w_n^2}$</p><p><strong>结构风险最小化：</strong>$min{1\over N}\sum_{i=1}^{N}L(y_i,f(x_i)+\lambda J(f)$</p><p>注：其中λ 是正则化参数，用来控制正则化项在整体损失函数中的重要程度。较大的 λ 值会增加对模型复杂度的惩罚，从而更强调正则化的效果。</p><h4 id="2-基本概念"><a href="#2-基本概念" class="headerlink" title="2.基本概念"></a>2.基本概念</h4><p><strong>过拟合：</strong>模型过于复杂或过度拟合了训练数据的细节和噪声，导致对新数据的泛化能力不佳。</p><p><strong>欠拟合：</strong>无法在训练数据上达到较好的拟合效果，也不能很好地泛化到新数据。</p><p><strong>正则化：</strong>在经验风险的基础上增添了一个正则化项，用来控制模型的复杂度，可以防止过拟合。通过正则化可以选择出经验风险和模型复杂度同时较小的模型。</p><h4 id="3-常见问题："><a href="#3-常见问题：" class="headerlink" title="3.常见问题："></a>3.常见问题：</h4><h5 id="3-1为什么正则化可以防止过拟合？"><a href="#3-1为什么正则化可以防止过拟合？" class="headerlink" title="3.1为什么正则化可以防止过拟合？"></a>3.1为什么正则化可以防止过拟合？</h5><p>答：正则化等价于对模型的复杂度添加约束条件，其具有相同的解空间。</p><h5 id="3-2为什么L1正则化具有稀疏性？"><a href="#3-2为什么L1正则化具有稀疏性？" class="headerlink" title="3.2为什么L1正则化具有稀疏性？"></a>3.2为什么L1正则化具有稀疏性？</h5><p>答：在最小化损失函数时，L1正则化项会与损失函数一起构成优化问题。从解空间形状的角度分析，，另外，L1正则化在参数空间中形成了一个尖锐的角，使得优化算法更有可能选择参数为零的解（即L1正则化图像与损失函数的等值线的交点更可能位于坐标轴上）。</p><p><img src="/2023/07/12/%E6%AD%A3%E5%88%99%E5%8C%96-1/image-20230710170542545.png" alt="image-20230710170542545"></p><p>从贝叶斯的观点，所有的正则化都是来自于对参数分布的先验，Laplace先验会导出L1正则化，Gauss先验会导出L2正则化，而Laplace分布取零值的概率较大，所以参数取零值的概率较大，因而L1正则化具有稀疏性。</p><h5 id="3-3推导L1先验分布是Laplace分布"><a href="#3-3推导L1先验分布是Laplace分布" class="headerlink" title="3.3推导L1先验分布是Laplace分布"></a>3.3推导L1先验分布是Laplace分布</h5><p>后验概率：$P(\theta|D)={P(D|\theta)*P(\theta)} \over P(D) $</p><p>要使后验概率最大，即使$P(D|\theta)*P(\theta)$最大</p><p>取log加负号后求其最小值：</p><script type="math/tex; mode=display">-log(P(D|\theta))-log(P(\theta))</script><p>其等价于求$-\sum\limits_{i=1}^NlogP(x_i|\theta)-log(P(\theta))$的最小值</p><p>假设$P(\theta)$满足拉普拉斯分布：</p><script type="math/tex; mode=display">P(\theta)={1 \over {2\lambda}}e^{-|\theta|\over \lambda}</script><p>代入$P(\theta)$后，即求$-\sum\limits_{i=1}^NlogP(x_i|\theta)-log({1 \over {2\lambda}})+{1 \over \lambda}|\theta|$</p><p>其相当于L1正则</p>]]></content>
      
      
      <categories>
          
          <category> 机器学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 机器学习 </tag>
            
            <tag> 正则化 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>numpy基础</title>
      <link href="/2023/07/02/numpy%E5%9F%BA%E7%A1%80/"/>
      <url>/2023/07/02/numpy%E5%9F%BA%E7%A1%80/</url>
      
        <content type="html"><![CDATA[<h3 id="numpy"><a href="#numpy" class="headerlink" title="numpy"></a>numpy</h3><h4 id="1-np-random-randn函数"><a href="#1-np-random-randn函数" class="headerlink" title="1.np.random.randn函数"></a>1.np.random.randn函数</h4><p>numpy中rand与randn的区别:</p><p>(1).rand是随机生成值在0-1之间的函数。</p><p>(2).randn是随机生成均值为0，方差为1的正态分布上的数值。</p><p><strong>np.random.randn函数不同个数参数的输出</strong></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line">in: x = np.random.randn(3)</span><br><span class="line">    print(x)#输出1行3列的一维数组</span><br><span class="line">out:array([0.87154087, 0.78316833, 0.97669061])</span><br><span class="line"></span><br><span class="line">in: x = np.random.randn(4,3)</span><br><span class="line">    print(x)#输出4行3列的二维数组</span><br><span class="line">out:[[ 0.81536654 -0.06119516  0.23273782]</span><br><span class="line">     [-1.3096832   0.11305955 -0.43735283]</span><br><span class="line">     [-1.68680386 -0.08597495 -1.20287298]</span><br><span class="line">     [-0.30945828 -0.50162275  0.41018841]]</span><br><span class="line">     </span><br><span class="line">in: x = np.random.randn(4,3,2)</span><br><span class="line">    print(x)#输出4个小矩阵，每个小矩阵都是3行2列</span><br><span class="line">out:[[[-0.24571791 -0.51039714]</span><br><span class="line">      [ 0.88810165 -0.94045245]</span><br><span class="line">      [-0.37231775  1.33880112]]</span><br><span class="line"></span><br><span class="line">     [[ 0.09029384 -0.76451493]</span><br><span class="line">      [ 1.25793366  1.04770133]</span><br><span class="line">      [ 0.92711157  1.07891784]]</span><br><span class="line"></span><br><span class="line">     [[ 1.10773856 -0.0084584 ]</span><br><span class="line">      [-0.31300087 -0.80121054]</span><br><span class="line">      [ 0.15738774 -1.12602141]]</span><br><span class="line"></span><br><span class="line">     [[ 0.54273696  1.2454806 ]</span><br><span class="line">      [-0.80644804 -1.00282505]</span><br><span class="line">      [ 0.70669598 -0.45169629]]]</span><br><span class="line">   </span><br><span class="line">#其他情况以此类推</span><br></pre></td></tr></table></figure><h4 id="2-ndarray的创建"><a href="#2-ndarray的创建" class="headerlink" title="2.ndarray的创建"></a>2.ndarray的创建</h4><p>ndarray是一个通用的同构数据多维容器，其中的所有元素必须是相同类型的。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">in :data1 = [6, 7.5, 8, 0, 1]</span><br><span class="line">    arr1 = np.array(data1)</span><br><span class="line">    print(arr1)</span><br><span class="line">out:array([6. , 7.5, 8. , 0. , 1. ])#np.array会尝试为新建的这个数组推断出一个较为合适的数据类型。</span><br><span class="line"></span><br><span class="line">in :data2 = [[1, 2, 3, 4], [5, 6, 7, 8]]</span><br><span class="line">    arr2 = np.array(data2)</span><br><span class="line">    print(arr2)</span><br><span class="line">out:[[1 2 3 4]</span><br><span class="line">     [5 6 7 8]]</span><br><span class="line">in :arr1 = np.array([1, 2, 3], dtype=np.float64)#指定创建的array的元素类型</span><br><span class="line">    print(arr1)</span><br><span class="line">out:[1. 2. 3.]</span><br><span class="line"></span><br><span class="line">in :arr2.ndim  #维度大小</span><br><span class="line">out:2</span><br><span class="line"></span><br><span class="line">in :arr2.shape #数组大小</span><br><span class="line">out:(2, 4)</span><br><span class="line"></span><br><span class="line">in :arr2.dtype #元素类型</span><br><span class="line">out:dtype(&#x27;int32&#x27;) </span><br></pre></td></tr></table></figure><p>zeros和ones分别可以创建指定长度或形状的全0或全1数组。</p><p>empty可以创建一个没有任何具体值的数组，通常它返回的都是一些未初始化的垃圾值。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">in : np.zeros(10)</span><br><span class="line">out: array([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.])</span><br></pre></td></tr></table></figure><p>通过ndarray的astype方法可以明确地将一个数组从一个dtype转换成另一个dtype：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">in :arr = np.array([1, 2, 3, 4, 5])</span><br><span class="line">    float_arr = arr.astype(np.float64)</span><br><span class="line">    float_arr.dtype</span><br><span class="line">out:dtype(&#x27;float64&#x27;)</span><br></pre></td></tr></table></figure><h4 id="3-numpy数组的运算"><a href="#3-numpy数组的运算" class="headerlink" title="3.numpy数组的运算"></a>3.numpy数组的运算</h4><p>大小相等的数组之间的任何算术运算及数组与标量的算术运算都会将运算应用到元素级。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">In :arr = np.array([[1., 2., 3.], [4., 5., 6.]])</span><br><span class="line">    arr</span><br><span class="line">Out: </span><br><span class="line">array([[ 1.,  2.,  3.],</span><br><span class="line">       [ 4.,  5.,  6.]])</span><br><span class="line"></span><br><span class="line">In : arr * arr</span><br><span class="line">Out: </span><br><span class="line">array([[  1.,   4.,   9.],</span><br><span class="line">       [ 16.,  25.,  36.]])</span><br></pre></td></tr></table></figure><p><strong>矩阵的乘法</strong></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">in: A = np.array([[1,2], [3,4]])</span><br><span class="line">    B = np.array([[5,6], [7,8]])</span><br><span class="line">    np.dot(A, B)#进行矩阵A与矩阵B的乘法运算</span><br><span class="line">out:</span><br><span class="line">array([[19, 22],</span><br><span class="line">       [43, 50]])</span><br></pre></td></tr></table></figure><h4 id="4-ndarray的切片"><a href="#4-ndarray的切片" class="headerlink" title="4.ndarray的切片"></a>4.ndarray的切片</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">in :arr2d</span><br><span class="line">out:</span><br><span class="line">array([[1, 2, 3],</span><br><span class="line">       [4, 5, 6],</span><br><span class="line">       [7, 8, 9]])</span><br><span class="line">in :arr2d[:2, 1:]</span><br><span class="line">out:</span><br><span class="line">array([[2, 3],</span><br><span class="line">       [5, 6]])</span><br></pre></td></tr></table></figure><h4 id="5-广播"><a href="#5-广播" class="headerlink" title="5.广播"></a>5.广播</h4><p>NumPy中，形状不同的数组之间也可以进行运算。</p><p><img src="/2023/07/02/numpy%E5%9F%BA%E7%A1%80/img1.png" alt="img1"></p><p><img src="/2023/07/02/numpy%E5%9F%BA%E7%A1%80/img2.png" alt="img2"></p><h4 id="6-np-arange-用法"><a href="#6-np-arange-用法" class="headerlink" title="6.np.arange()用法"></a>6.np.arange()用法</h4><p><strong>一个参数时：</strong></p><h5 id="（1）参数值为终点"><a href="#（1）参数值为终点" class="headerlink" title="（1）参数值为终点"></a>（1）参数值为终点</h5><h5 id="（2）起点取默认值0"><a href="#（2）起点取默认值0" class="headerlink" title="（2）起点取默认值0"></a>（2）起点取默认值0</h5><h5 id="（3）步长取默认值1"><a href="#（3）步长取默认值1" class="headerlink" title="（3）步长取默认值1"></a>（3）步长取默认值1</h5><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">in ：</span><br><span class="line">import numpy </span><br><span class="line">print(numpy.arange(6)) #从0到5,不包括6</span><br><span class="line">out:</span><br><span class="line">[0 1 2 3 4 5]</span><br></pre></td></tr></table></figure><p><strong>两个参数时：</strong></p><h5 id="（1）第一个参数为起点"><a href="#（1）第一个参数为起点" class="headerlink" title="（1）第一个参数为起点"></a>（1）第一个参数为起点</h5><h5 id="（2）第二个参数为终点（不包括）"><a href="#（2）第二个参数为终点（不包括）" class="headerlink" title="（2）第二个参数为终点（不包括）"></a>（2）第二个参数为终点（不包括）</h5><h5 id="（3）步长取默认值1-1"><a href="#（3）步长取默认值1-1" class="headerlink" title="（3）步长取默认值1"></a>（3）步长取默认值1</h5><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">in: print(numpy.arange(6, 10)) #[6,9)</span><br><span class="line">out:[6 7 8 9]</span><br></pre></td></tr></table></figure><p><strong>三个参数时：</strong></p><h5 id="（1）第一个参数为起点-1"><a href="#（1）第一个参数为起点-1" class="headerlink" title="（1）第一个参数为起点"></a>（1）第一个参数为起点</h5><h5 id="（2）第二个参数为终点（不包括）-1"><a href="#（2）第二个参数为终点（不包括）-1" class="headerlink" title="（2）第二个参数为终点（不包括）"></a>（2）第二个参数为终点（不包括）</h5><h5 id="（3）第三个参数为步长"><a href="#（3）第三个参数为步长" class="headerlink" title="（3）第三个参数为步长"></a>（3）第三个参数为步长</h5><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">in: print(np.arange(6,20,1.5))  #步长支持小数</span><br><span class="line">out:</span><br><span class="line">[ 6.   7.5  9.  10.5 12.  13.5 15.  16.5 18.  19.5]</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> 数据分析 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> python </tag>
            
            <tag> numpy </tag>
            
            <tag> 数据分析 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>python基础（2）</title>
      <link href="/2023/07/01/python%E5%9F%BA%E7%A1%80%EF%BC%882%EF%BC%89/"/>
      <url>/2023/07/01/python%E5%9F%BA%E7%A1%80%EF%BC%882%EF%BC%89/</url>
      
        <content type="html"><![CDATA[<h2 id="python基础"><a href="#python基础" class="headerlink" title="python基础"></a>python基础</h2><h3 id="1-enumerate函数"><a href="#1-enumerate函数" class="headerlink" title="1.enumerate函数"></a>1.enumerate函数</h3><p>用于返回<code>(i, value)</code>元组序列，可以跟踪当前项的序号。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">for (i,value) in enumerate(tup):</span><br><span class="line">    print(str(i)+&quot;:&quot;+value)</span><br></pre></td></tr></table></figure><h3 id="2-sorted函数"><a href="#2-sorted函数" class="headerlink" title="2.sorted函数"></a>2.sorted函数</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">In : sorted([7, 1, 2, 6, 0, 3, 2])</span><br><span class="line">Out: [0, 1, 2, 2, 3, 6, 7]</span><br><span class="line"></span><br><span class="line">In : sorted(&#x27;horse race&#x27;)#对字符串进行排序</span><br><span class="line">Out: [&#x27; &#x27;, &#x27;a&#x27;, &#x27;c&#x27;, &#x27;e&#x27;, &#x27;e&#x27;, &#x27;h&#x27;, &#x27;o&#x27;, &#x27;r&#x27;, &#x27;r&#x27;, &#x27;s&#x27;]</span><br></pre></td></tr></table></figure><p>sorted()函数与sort()函数不同，会返回一个排序列表，而不改变原有序列</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">in :tup1=[2,6,8,1]</span><br><span class="line">    tup1.sort()</span><br><span class="line">    print(tup1)</span><br><span class="line">out:[1, 2, 6, 8]</span><br><span class="line"></span><br><span class="line">in :tup1=[2,6,8,1]</span><br><span class="line">    tup2=sorted(tup1)</span><br><span class="line">    print(tup2)</span><br><span class="line">    print(tup1)</span><br><span class="line">out:[1, 2, 6, 8]</span><br><span class="line">    [2, 6, 8, 1]</span><br></pre></td></tr></table></figure><p><strong>sort（）可以接受两个参数sort（key,reverse）</strong></p><p>key接受的函数返回值，表示此元素的权值，sort将按照权值大小进行排序</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">in: x=[8,9,0,7,4,5,1,2,3,6]</span><br><span class="line">    def size(a):</span><br><span class="line">        x=10-int(a)</span><br><span class="line">        return x</span><br><span class="line">    x.sort(key=size)</span><br><span class="line">    print(x)</span><br><span class="line">out:[9, 8, 7, 6, 5, 4, 3, 2, 1, 0]</span><br></pre></td></tr></table></figure><p>reverse接受的是一个bool类型的值 (Ture or False),表示是否颠倒排列顺序,一般默认的是False</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">in :x=[8,9,0,7,4,5,1,2,3,6]</span><br><span class="line">    x.sort(reverse=True)</span><br><span class="line">    print(x)</span><br><span class="line">out:[9, 8, 7, 6, 5, 4, 3, 2, 1, 0]</span><br></pre></td></tr></table></figure><p><strong>sort根据关键字排序</strong></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">in :l = [[1,2,3],[2,2,1],[2,1,3]]</span><br><span class="line">    l.sort(key = lambda x: (x[0],-x[1]))</span><br><span class="line">    print(l)</span><br><span class="line">out:[[1, 2, 3], [2, 2, 1], [2, 1, 3]]</span><br></pre></td></tr></table></figure><p><u>注：其中key表示按第几个关键字排序，lambda x:后()中的元素表示了关键字的优先级（由高至低），x[0]表示按第一个关键词的升序排列，-x[1]表示按第二个关键词的降序排列</u></p><h3 id="3-zip函数"><a href="#3-zip函数" class="headerlink" title="3.zip函数"></a>3.zip函数</h3><p><code>zip</code>可以将多个列表、元组或其它序列成对组合成一个元组列表：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">In :tup1=1,2,3</span><br><span class="line">    tup2=&#x27;a&#x27;,&#x27;b&#x27;,&#x27;c&#x27;</span><br><span class="line">    ziped=zip(tup1,tup2)</span><br><span class="line">    list(ziped)</span><br><span class="line">out:[(1, &#x27;a&#x27;), (2, &#x27;b&#x27;), (3, &#x27;c&#x27;)]</span><br></pre></td></tr></table></figure><p>处理任意多的序列，元素的个数取决于最短的序列</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">In :tup1=1,2,3</span><br><span class="line">    tup2=&#x27;a&#x27;,&#x27;b&#x27;,&#x27;c&#x27;</span><br><span class="line">    tup3=False,True</span><br><span class="line">    ziped=zip(tup1,tup2,tup3)</span><br><span class="line">    list(ziped)</span><br><span class="line">out:[(1, &#x27;a&#x27;, False), (2, &#x27;b&#x27;, True)]</span><br></pre></td></tr></table></figure><p><strong>（1）zip函数的常见用法：同时迭代多个序列</strong></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">in :tup1=1,2,3</span><br><span class="line">    tup2=&#x27;a&#x27;,&#x27;b&#x27;,&#x27;c&#x27;</span><br><span class="line">    for i,(value1,value2) in enumerate(zip(tup1,tup2)):</span><br><span class="line">         print(&#x27;&#123;0&#125;:&#123;1&#125;,&#123;2&#125;&#x27;.format(i,value1,value2))</span><br><span class="line">out:0:1,a</span><br><span class="line">    1:2,b</span><br><span class="line">    2:3,c</span><br></pre></td></tr></table></figure><p><strong>（2）zip函数的常见用法:解压序列</strong></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">in :pitchers = [(&#x27;Nolan&#x27;, &#x27;Ryan&#x27;), (&#x27;Roger&#x27;, &#x27;Clemens&#x27;),(&#x27;Schilling&#x27;, &#x27;Curt&#x27;)]</span><br><span class="line">    first_names, last_names = zip(*pitchers)</span><br><span class="line">    print(first_names)</span><br><span class="line">    print(last_names)</span><br><span class="line">out:(&#x27;Nolan&#x27;, &#x27;Roger&#x27;, &#x27;Schilling&#x27;)</span><br><span class="line">    (&#x27;Ryan&#x27;, &#x27;Clemens&#x27;, &#x27;Curt&#x27;)</span><br></pre></td></tr></table></figure><h3 id="4-reversed函数"><a href="#4-reversed函数" class="headerlink" title="4.reversed函数"></a>4.reversed函数</h3><p>用于从后向前迭代序列</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">in :tup = tuple(&#x27;string&#x27;)</span><br><span class="line">    for (i,value) in enumerate(reversed(tup)):#从后向前迭代元组</span><br><span class="line">        print(str(i)+&quot;:&quot;+value,end=&#x27; &#x27;)</span><br><span class="line">out:0:g 1:n 2:i 3:r 4:t 5:s </span><br></pre></td></tr></table></figure><h3 id="5-匿名（lambda）函数"><a href="#5-匿名（lambda）函数" class="headerlink" title="5.匿名（lambda）函数"></a>5.匿名（lambda）函数</h3><p>由单条语句组成，该语句的结果就是返回值。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">in :def apply_to_list(some_list, f):</span><br><span class="line">        return [f(x) for x in some_list]</span><br><span class="line"></span><br><span class="line">    ints = [4, 0, 1, 5, 6]</span><br><span class="line">    apply_to_list(ints, lambda x: x * 2)#&#x27;:&#x27;前为参数,&#x27;:&#x27;后为运算表达式</span><br><span class="line">out:[8, 0, 2, 10, 12]</span><br><span class="line"></span><br><span class="line">in :strings = [&#x27;foo&#x27;, &#x27;card&#x27;, &#x27;bar&#x27;, &#x27;aaaa&#x27;, &#x27;abab&#x27;]</span><br><span class="line">    strings.sort(key=lambda x: len(set(list(x))))#根据不同字母的数量，对strings排序</span><br><span class="line">    print(strings)</span><br><span class="line">out:[&#x27;aaaa&#x27;, &#x27;foo&#x27;, &#x27;abab&#x27;, &#x27;bar&#x27;, &#x27;card&#x27;]</span><br></pre></td></tr></table></figure><h3 id="6-迭代器"><a href="#6-迭代器" class="headerlink" title="6.迭代器"></a>6.迭代器</h3><p><strong>可迭代对象</strong>:可迭代的对象要有_iter_()方法</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">#常见的可迭代对象</span><br><span class="line">iterables=[</span><br><span class="line">    &quot;123&quot;,#字符串</span><br><span class="line">    [1,2,3],#列表</span><br><span class="line">    (1,2,3),#元组</span><br><span class="line">    &#123;1:&#x27;a&#x27;,2:&#x27;b&#x27;&#125;,#字典</span><br><span class="line">    &#123;1,2,3&#125;,#集合</span><br><span class="line">]</span><br><span class="line">for iterable in iterables:</span><br><span class="line">    print(type(iterable))</span><br><span class="line">    for x in iterable:</span><br><span class="line">        print(x,end=&#x27;,&#x27;)</span><br><span class="line">    print(&#x27;&#x27;)</span><br><span class="line"></span><br><span class="line">out:</span><br><span class="line">&lt;class &#x27;str&#x27;&gt;</span><br><span class="line">1,2,3,</span><br><span class="line">&lt;class &#x27;list&#x27;&gt;</span><br><span class="line">1,2,3,</span><br><span class="line">&lt;class &#x27;tuple&#x27;&gt;</span><br><span class="line">1,2,3,</span><br><span class="line">&lt;class &#x27;dict&#x27;&gt;</span><br><span class="line">1,2,</span><br><span class="line">&lt;class &#x27;set&#x27;&gt;</span><br><span class="line">1,2,3,</span><br></pre></td></tr></table></figure><p><strong>迭代器：</strong>有_next<em>()方法的可迭代对象，即迭代器既有_iter\</em>()方法，又有_next_()方法。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">#列表不是迭代器</span><br><span class="line">from collections.abc import Iterator</span><br><span class="line">lst=[1,2,3]</span><br><span class="line">isinstance(lst,Iterator)</span><br><span class="line"></span><br><span class="line">out:</span><br><span class="line">False</span><br></pre></td></tr></table></figure><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">#iter(lst)是迭代器</span><br><span class="line">x=iter(lst)</span><br><span class="line">isinstance(x,Iterator)</span><br><span class="line"></span><br><span class="line">out:</span><br><span class="line">True</span><br></pre></td></tr></table></figure><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">in: x.__next__()</span><br><span class="line">out:1</span><br><span class="line"></span><br><span class="line">in: x.__next__()</span><br><span class="line">out:2</span><br><span class="line"></span><br><span class="line">in: next(x)</span><br><span class="line">out:3</span><br></pre></td></tr></table></figure><p><strong>自定义迭代器：</strong>为类添加_iter_()方法和_next_()方法。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">#定义一个将字符串反序输出的迭代器</span><br><span class="line">class ReverseIterator:</span><br><span class="line">    def __init__(self, data):</span><br><span class="line">        self.data = data</span><br><span class="line">        self.index = len(data)</span><br><span class="line"></span><br><span class="line">    def __iter__(self):</span><br><span class="line">        return self</span><br><span class="line"></span><br><span class="line">    def __next__(self):</span><br><span class="line">        if self.index &lt;= 0:</span><br><span class="line">            raise StopIteration</span><br><span class="line">        self.index -= 1</span><br><span class="line">        return self.data[self.index]</span><br><span class="line"></span><br><span class="line">r=ReverseIterator(&#x27;abc&#x27;)</span><br><span class="line">for i in r:</span><br><span class="line">    print(i)</span><br><span class="line"></span><br><span class="line">out:</span><br><span class="line">c</span><br><span class="line">b</span><br><span class="line">a</span><br></pre></td></tr></table></figure><h3 id="7-生成器"><a href="#7-生成器" class="headerlink" title="7.生成器"></a>7.生成器</h3><p><strong>生成器</strong>：一种特殊的迭代器,通过函数的方式来定义，使用关键字<strong>yield</strong>来返回值，而不是使用<strong>return</strong>。当生成器函数被调用时，它并不会立即执行函数体，而是返回一个生成器对象，该对象可以用于逐步迭代产生值。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">def countdown(n):</span><br><span class="line">    while n &gt; 0:</span><br><span class="line">        yield n</span><br><span class="line">        n -= 1</span><br><span class="line"></span><br><span class="line"># 使用生成器进行迭代</span><br><span class="line">for num in countdown(5):</span><br><span class="line">    print(num, end=&quot; &quot;)</span><br><span class="line">   </span><br><span class="line">out:</span><br><span class="line">5 4 3 2 1 </span><br></pre></td></tr></table></figure><p><em>注：<strong>yield</strong>语句右边的对象作为next()的返回值，生成器在yield所在的位置暂停，当再次使用next()时继续从该位置继续运行。</em></p><h3 id="8-collections-Counter"><a href="#8-collections-Counter" class="headerlink" title="8.collections.Counter"></a>8.collections.Counter</h3><p><strong>collections.Counter:</strong>是Python标准库中的一个类，用于计数可哈希对象（通常是元素或元素组成的集合）的出现次数。</p><p><strong>collections.Counter是一个字典的子类，提供了方便的计数功能，可以用于各种计数和统计场景</strong></p><h4 id="8-1-创建计数器"><a href="#8-1-创建计数器" class="headerlink" title="8.1 创建计数器"></a>8.1 创建计数器</h4><p>可以通过传入可迭代对象（如列表、元组、字符串等）或字典来创建计数器。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">from collections import Counter</span><br><span class="line"></span><br><span class="line"># 通过列表创建计数器</span><br><span class="line">a= Counter([&#x27;apple&#x27;, &#x27;orange&#x27;, &#x27;apple&#x27;, &#x27;banana&#x27;, &#x27;apple&#x27;])</span><br><span class="line"></span><br><span class="line"># 通过字符串创建计数器</span><br><span class="line">b = Counter(&#x27;hello&#x27;)</span><br><span class="line"></span><br><span class="line"># 通过字典创建计数器</span><br><span class="line">c = Counter(&#123;&#x27;apple&#x27;: 3, &#x27;orange&#x27;: 2, &#x27;banana&#x27;: 1&#125;)</span><br><span class="line">a,b,c</span><br><span class="line"></span><br><span class="line">out:</span><br><span class="line">(Counter(&#123;&#x27;apple&#x27;: 3, &#x27;orange&#x27;: 1, &#x27;banana&#x27;: 1&#125;),</span><br><span class="line"> Counter(&#123;&#x27;h&#x27;: 1, &#x27;e&#x27;: 1, &#x27;l&#x27;: 2, &#x27;o&#x27;: 1&#125;),</span><br><span class="line"> Counter(&#123;&#x27;apple&#x27;: 3, &#x27;orange&#x27;: 2, &#x27;banana&#x27;: 1&#125;))</span><br></pre></td></tr></table></figure><h4 id="8-2-计数元素的出现次数"><a href="#8-2-计数元素的出现次数" class="headerlink" title="8.2 计数元素的出现次数"></a>8.2 计数元素的出现次数</h4><p>可以使用<code>elements()</code>方法获取计数器中所有元素的迭代器，也可以使用普通的字典语法或<code>get()</code>方法获取指定元素的出现次数。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">c = Counter(&#x27;abracadabra&#x27;)</span><br><span class="line">print(c)  # Counter(&#123;&#x27;a&#x27;: 5, &#x27;b&#x27;: 2, &#x27;r&#x27;: 2, &#x27;c&#x27;: 1, &#x27;d&#x27;: 1&#125;)</span><br><span class="line"></span><br><span class="line">print(c[&#x27;a&#x27;])  # 5</span><br><span class="line">print(c.get(&#x27;b&#x27;))  # 2</span><br><span class="line"></span><br><span class="line">elements = list(c.elements())  # [&#x27;a&#x27;, &#x27;a&#x27;, &#x27;a&#x27;, &#x27;a&#x27;, &#x27;a&#x27;, &#x27;b&#x27;, &#x27;b&#x27;, &#x27;r&#x27;, &#x27;r&#x27;, &#x27;c&#x27;, &#x27;d&#x27;]</span><br></pre></td></tr></table></figure><h4 id="8-3-获取最常见的元素"><a href="#8-3-获取最常见的元素" class="headerlink" title="8.3 获取最常见的元素"></a>8.3 获取最常见的元素</h4><p>可以使用<code>most_common()</code>方法获取计数器中出现频率最高的元素。该方法返回一个列表，其中每个元素是一个元组，包含元素和对应的计数。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">c = Counter(&#x27;abracadabra&#x27;)</span><br><span class="line">print(c.most_common(2))</span><br><span class="line"></span><br><span class="line">out:</span><br><span class="line">[(&#x27;a&#x27;, 5), (&#x27;b&#x27;, 2)]</span><br></pre></td></tr></table></figure><h4 id="8-4-更新计数器"><a href="#8-4-更新计数器" class="headerlink" title="8.4 更新计数器"></a>8.4 更新计数器</h4><p>可以使用<code>update()</code>方法将另一个计数器、可迭代对象或字典的元素合并到当前计数器中。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">c1 = Counter(&#x27;abracadabra&#x27;)</span><br><span class="line">c2 = Counter(&#x27;alakazam&#x27;)</span><br><span class="line"></span><br><span class="line">c1.update(c2)  # 合并c2中的计数到c1中</span><br><span class="line">print(c1)  # Counter(&#123;&#x27;a&#x27;: 8, &#x27;b&#x27;: 2, &#x27;r&#x27;: 2, &#x27;l&#x27;: 1, &#x27;k&#x27;: 1, &#x27;z&#x27;: 1, &#x27;m&#x27;: 1&#125;)</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> python </category>
          
      </categories>
      
      
        <tags>
            
            <tag> python </tag>
            
            <tag> 迭代器 </tag>
            
            <tag> 生成器 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>python基础（1）</title>
      <link href="/2023/06/26/python%E5%9F%BA%E7%A1%80%EF%BC%881%EF%BC%89/"/>
      <url>/2023/06/26/python%E5%9F%BA%E7%A1%80%EF%BC%881%EF%BC%89/</url>
      
        <content type="html"><![CDATA[<h3 id="python基础"><a href="#python基础" class="headerlink" title="python基础"></a>python基础</h3><h4 id="1-字符串输入：使用input-函数"><a href="#1-字符串输入：使用input-函数" class="headerlink" title="1.字符串输入：使用input()函数"></a>1.字符串输入：使用input()函数</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">x=input()</span><br><span class="line">y=input()</span><br><span class="line">print(x+y)</span><br><span class="line">#输入：5 4</span><br><span class="line"> 输出：54(&quot;+&quot;对于字符串进行拼接)</span><br><span class="line"></span><br><span class="line">name=input(&quot;请输入姓名：&quot;)#input()中添加文本对所要输入的信息进行提示</span><br><span class="line">print(name)</span><br></pre></td></tr></table></figure><h4 id="2-列表（list"><a href="#2-列表（list" class="headerlink" title="2.列表（list)"></a>2.列表（list)</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">List=[&#x27;A&#x27;,&#x27;B&#x27;,&#x27;C&#x27;]</span><br><span class="line">print(List)</span><br><span class="line">print(len(List))#获取列表的长度</span><br><span class="line">print(List[-1])#获取列表表尾的元素</span><br><span class="line">print(List[-2])#获取列表倒数第二个元素</span><br><span class="line"></span><br><span class="line">#列表的插入</span><br><span class="line">List.append(&#x27;D&#x27;)#列表尾部插入</span><br><span class="line">List.insert(2,&#x27;F&#x27;)#列表指定位置插入</span><br><span class="line">List.pop()#列表尾部元素删除</span><br><span class="line">List.pop(2)#删除列表指定索引处的元素</span><br><span class="line">print(List)</span><br><span class="line">List.remove(&#x27;A&#x27;)#一次remove()只删除一个，若有多个，要多次使用remove()函数</span><br><span class="line">print(List)</span><br></pre></td></tr></table></figure><h4 id="3-元组-tuple"><a href="#3-元组-tuple" class="headerlink" title="3.元组(tuple)"></a>3.元组(tuple)</h4><p>tuple一旦初始化就不能修改</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">tuple=(2,3,4)#定义元组并初始化</span><br><span class="line">print(tuple)</span><br><span class="line">tuple2=(2,)#定义单元组</span><br><span class="line">tuple3=(2)#由于出现歧义，tuple3为2这个数</span><br><span class="line">print(tuple2,tuple3)</span><br></pre></td></tr></table></figure><h4 id="4-for-…-in…-循环：可以用于遍历元组或列表"><a href="#4-for-…-in…-循环：可以用于遍历元组或列表" class="headerlink" title="4.for ….in….循环：可以用于遍历元组或列表"></a>4.for ….in….循环：可以用于遍历元组或列表</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">List=[&#x27;A&#x27;,&#x27;B&#x27;,&#x27;C&#x27;]</span><br><span class="line">for i in List:#遍历列表</span><br><span class="line">    print(i)</span><br><span class="line">print(list(range(5)))#range(5)生成一个从0开始小于5的整数序列</span><br><span class="line">                     #list()将这个整数序列转换成列表</span><br><span class="line">sum=0</span><br><span class="line">for i in range(101):#计算从0加到100的值</span><br><span class="line">    sum+=i</span><br><span class="line">print(sum)</span><br></pre></td></tr></table></figure><h4 id="5-字典-dict"><a href="#5-字典-dict" class="headerlink" title="5.字典(dict)"></a>5.字典(dict)</h4><p>使用键-值（key-value）存储，具有极快的查找速度</p><h5 id="字典的创建"><a href="#字典的创建" class="headerlink" title="字典的创建"></a>字典的创建</h5><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">Dict=&#123;&#x27;A&#x27;:1,&#x27;B&#x27;:2,&#x27;C&#x27;:3&#125;#创建字典</span><br><span class="line">Dict[&#x27;A&#x27;]=4#修改key对应的值</span><br><span class="line">print(&#x27;D&#x27; in Dict)#判断字典key是否存在</span><br><span class="line">Dict.pop(&#x27;A&#x27;)#按key删除 或可用：del Dict[&#x27;A&#x27;]</span><br><span class="line">print(Dict)#注：字典中的key是不可变的对象</span><br><span class="line">Dict[&#x27;D&#x27;]=4#向字典中添加键值对</span><br></pre></td></tr></table></figure><h5 id="字典的遍历"><a href="#字典的遍历" class="headerlink" title="字典的遍历"></a>字典的遍历</h5><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">for key,val in Dict.items():#以键值对的形式遍历字典</span><br><span class="line">    print(key,val)</span><br><span class="line">for key in Dict.keys():#遍历字典的键，其中.keys()可省略</span><br><span class="line">    print(key)</span><br><span class="line">for val in Dict.values():#遍历字典中的值</span><br><span class="line">    print(val)</span><br><span class="line">for key in sorted(Dict.keys()):#在遍历前对字典的键的列表排序</span><br><span class="line">    print(key)</span><br><span class="line">for val in set(Dict.values()):#在遍历前对字典的值的列表去重</span><br><span class="line">    print(val)</span><br></pre></td></tr></table></figure><p><strong>dictionary.get(key, default)</strong></p><ul><li><code>dictionary</code> 是要获取值的字典对象。</li><li><code>key</code> 是要查找的键。</li><li><code>default</code> 是一个可选参数，表示如果键在字典中不存在时，返回的默认值。如果不提供 <code>default</code> 参数，默认值为 <code>None</code>。</li></ul><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">fruits = &#123;&#x27;apple&#x27;: 3, &#x27;banana&#x27;: 2, &#x27;orange&#x27;: 5&#125;</span><br><span class="line"></span><br><span class="line"># 获取键 &#x27;apple&#x27; 对应的值</span><br><span class="line">print(fruits.get(&#x27;apple&#x27;))  # 输出: 3</span><br><span class="line"></span><br><span class="line"># 获取键 &#x27;grape&#x27; 对应的值，由于 &#x27;grape&#x27; 不在字典中，返回默认值 0</span><br><span class="line">print(fruits.get(&#x27;grape&#x27;, 0))  # 输出: 0</span><br><span class="line"></span><br><span class="line"># 不提供默认值，&#x27;grape&#x27; 不在字典中，返回 None</span><br><span class="line">print(fruits.get(&#x27;grape&#x27;))  # 输出: None</span><br><span class="line"></span><br><span class="line"># 可以省略默认值参数，将返回 None</span><br><span class="line">print(fruits.get(&#x27;kiwi&#x27;))  # 输出: None</span><br></pre></td></tr></table></figure><h4 id="6-集合-set"><a href="#6-集合-set" class="headerlink" title="6.集合(set)"></a>6.集合(set)</h4><p>重复的元素在set中会被自动过滤</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">s=set([1,2,2,3,3])#创建一个set，使用list作为输入集合</span><br><span class="line">print(s)#输出&#123;1, 2, 3&#125;，相同的元素被过滤</span><br><span class="line">s.add(4)#向集合中添加元素</span><br><span class="line">s.remove(1)</span><br><span class="line">print(s)#输出&#123;2，3，4&#125;</span><br><span class="line">s2=set([1,2,3])</span><br><span class="line">print(s&amp;s2)#集合取交集</span><br><span class="line">print(s|s2)#集合取并集</span><br></pre></td></tr></table></figure><h4 id="7-字符串（str"><a href="#7-字符串（str" class="headerlink" title="7.字符串（str)"></a>7.字符串（str)</h4><p>str是不可变对象</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">a=&#x27;abc&#x27;</span><br><span class="line">print(a.replace(&#x27;a&#x27;,&#x27;A&#x27;))#输出Abc</span><br><span class="line">print(a)#输出abc,不可变对象本身永远是不可变的</span><br></pre></td></tr></table></figure><h4 id="8-函数的默认参数"><a href="#8-函数的默认参数" class="headerlink" title="8.函数的默认参数"></a>8.函数的默认参数</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">def enroll(name, gender, age=6, city=&#x27;Beijing&#x27;):#把年龄和城市设为默认参数</span><br><span class="line">    print(&#x27;name:&#x27;, name)</span><br><span class="line">    print(&#x27;gender:&#x27;, gender)</span><br><span class="line">    print(&#x27;age:&#x27;, age)</span><br><span class="line">    print(&#x27;city:&#x27;, city)</span><br><span class="line"></span><br><span class="line">#只有与默认参数不符的学生才需要提供额外的信息</span><br><span class="line">enroll(&#x27;Bob&#x27;, &#x27;M&#x27;, 7)</span><br><span class="line">enroll(&#x27;Adam&#x27;, &#x27;M&#x27;, city=&#x27;Tianjin&#x27;)#不按顺序提供部分默认参数时，需要把参数名写上</span><br></pre></td></tr></table></figure><h4 id="9-函数的可变参数"><a href="#9-函数的可变参数" class="headerlink" title="9.函数的可变参数"></a>9.函数的可变参数</h4><p>可变参数就是函数传入的参数个数是可变的</p><p><u>注：可变参数通过创建一个元组将传入的参数封装到元组中，即使函数只收到一个值也是如此。</u></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">def calc(*numbers):#计算：a*a+b*b+c*c+……</span><br><span class="line">    sum = 0</span><br><span class="line">    for n in numbers:#numbers是元组</span><br><span class="line">        sum = sum + n * n</span><br><span class="line">    return sum</span><br><span class="line"></span><br><span class="line">print(calc(1, 2))</span><br><span class="line">nums = [1, 2, 3]</span><br><span class="line">print(calc(*nums))#在list或tuple前面加一个*号，把list或tuple的元素变成可变参数传进去</span><br></pre></td></tr></table></figure><h4 id="10-函数upper-、title-、lower"><a href="#10-函数upper-、title-、lower" class="headerlink" title="10.函数upper()、title()、lower()"></a>10.函数upper()、title()、lower()</h4><p>upper():全字母大写</p><p>title():首字母大写</p><p>lower():全字母小写</p><p><u>注：以上方法是非永久性改变，不会改变变量的值</u></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">cars=[&#x27;audi&#x27;,&#x27;bmw&#x27;,&#x27;subaru&#x27;,&#x27;toyota&#x27;]</span><br><span class="line">for car in cars:</span><br><span class="line">    if car==&#x27;bmw&#x27;:</span><br><span class="line">        print(car.upper())#car为bmw时以大写输出</span><br><span class="line">    else:</span><br><span class="line">        print(car.title())</span><br></pre></td></tr></table></figure><h4 id="11-函数的列表参数"><a href="#11-函数的列表参数" class="headerlink" title="11.函数的列表参数"></a>11.函数的列表参数</h4><p>列表作为参数传给函数，函数可以对列表做出永久性修改</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">def fun(List):</span><br><span class="line">    i=0</span><br><span class="line">    for x in List:</span><br><span class="line">        List[i]=x.lower()</span><br><span class="line">        i+=1</span><br><span class="line">List=[&#x27;A&#x27;,&#x27;B&#x27;,&#x27;C&#x27;]</span><br><span class="line">fun(List)</span><br><span class="line">print(List)#输出[&#x27;a&#x27;, &#x27;b&#x27;, &#x27;c&#x27;]</span><br></pre></td></tr></table></figure><p>传入列表的副本，原始列表不做修改</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">def fun(List):</span><br><span class="line">    i=0</span><br><span class="line">    for x in List:</span><br><span class="line">        List[i]=x.lower()</span><br><span class="line">        i+=1</span><br><span class="line">List=[&#x27;A&#x27;,&#x27;B&#x27;,&#x27;C&#x27;]</span><br><span class="line">fun(List[:])#切片表示法创建副本</span><br><span class="line">print(List)#输出[&#x27;A&#x27;, &#x27;B&#x27;, &#x27;C&#x27;]</span><br></pre></td></tr></table></figure><h4 id="12-列表生成式"><a href="#12-列表生成式" class="headerlink" title="12.列表生成式"></a>12.列表生成式</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">print([x * x for x in range(1, 11) if x % 2 == 0]) #筛选出仅偶数的平方</span><br><span class="line"></span><br><span class="line">print([m + n for m in &#x27;ABC&#x27; for n in &#x27;XYZ&#x27;])#使用两层循环，可以生成全排列</span><br><span class="line"></span><br><span class="line">import os</span><br><span class="line">print([d for d in os.listdir(&#x27;.&#x27;)])#列出当前目录下的所有文件和目录名</span><br><span class="line"></span><br><span class="line">print([x if x % 2 == 0 else -x for x in range(1, 5)])#生成表达式的if……else……，输出[-1, 2, -3, 4]</span><br></pre></td></tr></table></figure><h4 id="13-split-函数"><a href="#13-split-函数" class="headerlink" title="13.split()函数"></a>13.split()函数</h4><p><strong>split():</strong>通过指定分隔符对字符串进行切片，将字符串切片后的结果存放在列表中。</p><p><strong>str.split(str=””, num=string.count(str))</strong></p><ul><li>str — 分隔符，默认为空白格。</li><li>num — 分割次数。</li></ul><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">str = &quot;Line1-abcdef,\nLine2-abc,\nLine4-abcd&quot;</span><br><span class="line">str.split(&#x27;,&#x27;)</span><br><span class="line">out:</span><br><span class="line">[&#x27;Line1-abcdef&#x27;, &#x27;\nLine2-abc&#x27;, &#x27;\nLine4-abcd&#x27;]</span><br><span class="line"></span><br><span class="line">str = &quot;Line1-abcdef \nLine2-abc \nLine4-abcd&quot;</span><br><span class="line">str.split( )</span><br><span class="line">str.split(&#x27; &#x27;, 1 )#分割次数为1次</span><br><span class="line">out:</span><br><span class="line">[&#x27;Line1-abcdef&#x27;, &#x27;Line2-abc&#x27;, &#x27;Line4-abcd&#x27;]#以空白格分割，自动去除回车</span><br><span class="line">[&#x27;Line1-abcdef&#x27;, &#x27;\nLine2-abc \nLine4-abcd&#x27;]</span><br><span class="line"></span><br></pre></td></tr></table></figure><p> <strong>rstrip():</strong> 删除字符串末尾的指定字符,默认为空白符</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">#分割字符串列表</span><br><span class="line">str=[&#x27;1,a\n&#x27;,&#x27;2,b\n&#x27;,&#x27;3,c\n&#x27;]</span><br><span class="line">tokens=[l.rstrip().split(&#x27;,&#x27;) for l in str]</span><br><span class="line">tokens</span><br><span class="line"></span><br><span class="line">out:</span><br><span class="line">[[&#x27;1&#x27;, &#x27;a&#x27;], [&#x27;2&#x27;, &#x27;b&#x27;], [&#x27;3&#x27;, &#x27;c&#x27;]]</span><br></pre></td></tr></table></figure><h4 id="14-文件操作函数"><a href="#14-文件操作函数" class="headerlink" title="14.文件操作函数"></a>14.文件操作函数</h4><p><strong>os.makedirs():</strong> 这是一个用于递归创建目录的函数。它接受一个路径作为输入，并创建路径中所有缺失的目录</p><p><strong>exist_ok=True:</strong> 这是os.makedirs()函数的一个可选参数。当设置为True时，如果目标目录已经存在，函数不会引发错误，而是默默地继续执行。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">os.makedirs(os.path.join(&#x27;..&#x27;, &#x27;data&#x27;), exist_ok=True)#创建一个名为&quot;data&quot;的目录，该目录位于当前工作目录的父目录中</span><br></pre></td></tr></table></figure><p><strong>shutil.copy():</strong>用于将源文件的内容复制到目标文件或目录</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">shutil.copy(filename, target_dir)#将filename文件复制到target_dir目录中。</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> python </category>
          
      </categories>
      
      
        <tags>
            
            <tag> python </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
